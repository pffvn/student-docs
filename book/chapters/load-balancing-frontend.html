<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 19 - Load Balancing at the Frontend
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <section data-type="chapter" id="chapter_load-balance-frontend">
          <h1 class="heading">
            Load Balancing at the Frontend
          </h1>
          <p class="byline author">
            Written by Piotr Lewandowski<br>
            Edited by Sarah Chavis
          </p>
          <p>
            We serve many millions of requests every second and, as you may have already guessed,
            we use more than a single computer to handle this demand. But even if we <em>did</em>
            have a supercomputer that was somehow able to handle all these requests (imagine the
            network connectivity such a configuration would require!), we still wouldn’t employ a
            strategy that relied upon a single point of failure; when you’re dealing with
            large-scale systems, putting all your eggs in one basket is a recipe for disaster.
          </p>
          <p>
            This chapter focuses on high-level load balancing—how we balance user traffic
            <em>between</em> datacenters. The following chapter zooms in to explore how we
            implement load balancing <em>inside</em> a datacenter.
          </p>
          <section data-type="sect1" id="power-isnt-the-answer-MQsWTN">
            <h1 class="heading">
              Power Isn’t the Answer
            </h1>
            <p>
              <a data-primary="load balancing" data-secondary="frontend" data-tertiary=
              "optimal solutions for" data-type="indexterm" id="id-jyCxSoFET4"></a>For the sake of
              argument, let’s assume we have an unbelievably powerful machine and a network that
              never fails. Would <em>that</em> configuration be sufficient to meet Google’s needs?
              No. Even this configuration would still be limited by the physical constraints
              associated with our networking infrastructure. For example, the speed of light is a
              limiting factor on the communication speeds for fiber optic cable, which creates an
              upper bound on how quickly we can serve data based upon the distance it has to
              travel. Even in an ideal world, relying on an infrastructure with a single point of
              failure is a bad idea.
            </p>
            <p>
              In reality, Google has thousands of machines and even more users, many of whom issue
              multiple requests at a time. <em>Traffic load balancing</em> is how we decide which
              of the many, many machines in our datacenters will serve a particular request.
              Ideally, traffic is distributed across multiple network links, datacenters, and
              machines in an "optimal" fashion. But what does "optimal" mean in this context?
              There’s actually no single answer, because the optimal solution depends heavily on a
              variety of factors:
            </p>
            <ul>
              <li>
                <p>
                  The hierarchical level at which we evaluate the problem (global versus local)
                </p>
              </li>
              <li>
                <p>
                  The technical level at which we evaluate the problem (hardware versus software)
                </p>
              </li>
              <li>
                <p>
                  The nature of the traffic we’re dealing with
                </p>
              </li>
            </ul>
            <p>
              Let’s start by reviewing two common traffic scenarios: a basic search request and a
              video upload request. Users want to get their query results quickly, so the most
              important variable for the search request is latency. On the other hand, users expect
              video uploads to take a non-negligible amount of time, but also want such requests to
              succeed the first time, so the most important variable for the video upload is
              throughput. The differing needs of the two requests play a role in how we determine
              the optimal distribution for each request at the <em>global</em> level:
            </p>
            <ul>
              <li>
                <p>
                  The search request is sent to the nearest available datacenter—as measured in
                  round-trip time (RTT)—because we want to minimize the latency on the request.
                </p>
              </li>
              <li>
                <p>
                  The video upload stream is routed via a different path—perhaps to a link that is
                  currently underutilized—to maximize the throughput at the expense of latency.
                </p>
              </li>
            </ul>
            <p>
              But on the <em>local</em> level, inside a given datacenter, we often assume that all
              machines within the building are equally distant to the user and connected to the
              same network. Therefore, optimal distribution of load focuses on optimal resource
              utilization and protecting a single server from overloading.
            </p>
            <p>
              Of course, this example presents a vastly simplified picture. In reality, many more
              considerations factor into optimal load distribution: some requests may be directed
              to a datacenter that is slightly farther away in order to keep caches warm, or
              non-interactive traffic may be routed to a completely different region to avoid
              network congestion. Load balancing, especially for large systems, is anything but
              straightforward and static. At Google, we’ve approached the problem by load balancing
              at multiple levels, two of which are described in the following sections. For the
              sake of presenting a concrete discussion, we’ll consider HTTP requests sent over TCP.
              Load balancing of stateless services (like DNS over UDP) differs slightly, but most
              of the mechanisms described here should be applicable to stateless services as well.
            </p>
          </section>
          <section data-type="sect1" id="load-balancing-using-dns-7ks2cj">
            <h1 class="heading">
              Load Balancing Using DNS
            </h1>
            <p>
              <a data-primary="load balancing" data-secondary="frontend" data-tertiary="using DNS"
              data-type="indexterm" id="LBFdns19"></a> <a data-primary="DNS (Domain Name System)"
              data-secondary="load balancing using" data-type="indexterm" id="dns19"></a>Before a
              client can even send an HTTP request, it often has to look up an IP address using
              DNS. This provides the perfect opportunity to introduce our first layer of load
              balancing: <em>DNS load balancing</em>. The simplest solution is to return multiple
              <code>A</code> or <code>AAAA</code> records in the DNS reply and let the client pick
              an IP address arbitrarily. While conceptually simple and trivial to implement, this
              solution poses multiple challenges.
            </p>
            <p>
              The first problem is that it provides very little control over the client behavior:
              records are selected randomly, and each will attract a roughly equal amount of
              traffic. Can we mitigate this problem? In theory, we could use <code>SRV</code>
              records to specify record weights and priorities, but <code>SRV</code> records have
              not yet been adopted for HTTP.
            </p>
            <p>
              Another potential problem stems from the fact that usually the client cannot
              determine the closest address. We <em>can</em> mitigate this scenario by using an
              anycast address for authoritative nameservers and leverage the fact that DNS queries
              will flow to the closest address. In its reply, the server can return addresses
              routed to the closest datacenter. A further improvement builds a map of all networks
              and their approximate physical locations, and serves DNS replies based on that
              mapping. However, this solution comes at the cost of having a much more complex DNS
              server implementation and maintaining a pipeline that will keep the location mapping
              up to date.
            </p>
            <p>
              <a data-primary="recursive DNS servers" data-type="indexterm" id="id-g9CnSyhNcj"></a>
              <a data-primary="recursion" data-see="recursion" data-type="indexterm" id=
              "id-qXCAFVhGcE"></a>Of course, none of these solutions are trivial, due to a
              fundamental characteristic of DNS: end users rarely talk to authoritative nameservers
              directly. Instead, a recursive DNS server usually lies somewhere between end users
              and nameservers. This server proxies queries between a user and a server and often
              provides a caching layer. The DNS middleman has three very important implications on
              traffic management:
            </p>
            <ul>
              <li>
                <p>
                  Recursive resolution of IP addresses
                </p>
              </li>
              <li>
                <p>
                  Nondeterministic reply paths
                </p>
              </li>
              <li>
                <p>
                  Additional caching complications
                </p>
              </li>
            </ul>
            <p>
              <a data-primary="DNS (Domain Name System)" data-secondary="EDNS0 extension"
              data-type="indexterm" id="id-1nCxSrcMcv"></a>Recursive resolution of IP addresses is
              problematic, as the IP address seen by the authoritative nameserver does not belong
              to a user; instead, it’s the recursive resolver’s. This is a serious limitation,
              because it only allows reply optimization for the shortest distance between resolver
              and the nameserver. A possible solution is to use the EDNS0 extension proposed in
              <a data-type="xref" href="bibliography.html#Con15" target=
              "_blank">[Con15]</a>, which includes information about the client’s subnet in the DNS
              query sent by a recursive resolver. This way, an authoritative nameserver returns a
              response that is optimal from the user’s perspective, rather than the resolver’s
              perspective. While this is not yet the official standard, its obvious advantages have
              led the biggest DNS resolvers (such as OpenDNS and Google<sup><a data-type="noteref"
              href="load-balancing-frontend.html#id-Zo9unILcpcm" id="id-Zo9unILcpcm-marker">103</a></sup>) to support it
              already.
            </p>
            <p>
              <a data-primary="time-to-live (TTL)" data-type="indexterm" id="id-YQCDS7iKcq"></a>Not
              only is it difficult to find the optimal IP address to return to the nameserver for a
              given user’s request, but that nameserver may be responsible for serving thousands or
              millions of users, across regions varying from a single office to an entire
              continent. For instance, a large national ISP might run nameservers for its entire
              network from one datacenter, yet have network interconnects in each metropolitan
              area. The ISP’s nameservers would then return a response with the IP address best
              suited for their datacenter, despite there being better network paths for all users!
            </p>
            <p>
              Finally, recursive resolvers typically cache responses and forward those responses
              within limits indicated by the time-to-live (TTL) field in the DNS record. The end
              result is that estimating the impact of a given reply is difficult: a single
              authoritative reply may reach a single user or multiple thousands of users. We solve
              this problem in two ways:
            </p>
            <ul>
              <li>
                <p>
                  We analyze traffic changes and continuously update our list of known DNS
                  resolvers with the approximate size of the user base behind a given resolver,
                  which allows us to track the potential impact of any given resolver.
                </p>
              </li>
              <li>
                <p>
                  We estimate the geographical distribution of the users behind each tracked
                  resolver to increase the chance that we direct those users to the best location.
                </p>
              </li>
            </ul>
            <p>
              Estimating geographic distribution is particularly tricky if the user base is
              distributed across large regions. In such cases, we make trade-offs to select the
              best location and optimize the experience for the majority of users.
            </p>
            <p>
              But what does "best location" really mean in the context of DNS load balancing? The
              most obvious answer is the location closest to the user. However (as if determining
              users’ locations isn’t difficult in and of itself), there are additional criteria.
              The DNS load balancer needs to make sure that the datacenter it selects has enough
              capacity to serve requests from users that are likely to receive its reply. It also
              needs to know that the selected datacenter and its network connectivity are in good
              shape, because directing user requests to a datacenter that’s experiencing power or
              networking problems isn’t ideal. Fortunately, we can integrate the authoritative DNS
              server with our global control systems that track traffic, capacity, and the state of
              our infrastructure.
            </p>
            <p>
              The third implication of the DNS middleman is related to caching. Given that
              authoritative nameservers cannot flush resolvers’ caches, DNS records need a
              relatively low TTL. This effectively sets a lower bound on how quickly DNS changes
              can be propagated to users.<sup><a data-type="noteref" href="load-balancing-frontend.html#id-rq7uXS9Hzc7" id=
              "id-rq7uXS9Hzc7-marker">104</a></sup> Unfortunately, there is little we can do other
              than to keep this in mind as we make load balancing decisions.
            </p>
            <p>
              Despite all of these problems, DNS is still the simplest and most effective way to
              balance load before the user’s connection even starts. On the other hand, it should
              be clear that load balancing with DNS on its own is not sufficient. Keep in mind that
              all DNS replies served should fit within the 512-byte limit<sup><a data-type=
              "noteref" href="load-balancing-frontend.html#id-wbau7S1fmc8" id="id-wbau7S1fmc8-marker">105</a></sup> set by RFC
              1035 <a data-type="xref" href="bibliography.html#Moc87" target=
              "_blank">[Moc87]</a>. This limit sets an upper bound on the number of addresses we
              can squeeze into a single DNS reply, and that number is almost certainly less than
              our number of servers.
            </p>
            <p>
              To <em>really</em> solve the problem of frontend load balancing, this initial level
              of DNS load balancing should be followed by a level that takes advantage of virtual
              IP addresses. <a data-primary="" data-startref="LBFdns19" data-type="indexterm" id=
              "id-BnCNF4Sxca"></a> <a data-primary="" data-startref="dns19" data-type="indexterm"
              id="id-MnCVIMSjc4"></a>
            </p>
          </section>
          <section data-type="sect1" id="load-balancing-at-the-virtual-ip-address-Evsniz">
            <h1 class="heading">
              Load Balancing at the Virtual IP Address
            </h1>
            <p>
              <a data-primary="load balancing" data-secondary="frontend" data-tertiary=
              "virtual IP addresses (VIPs)" data-type="indexterm" id="id-pOCzSjFmiL"></a>
              <a data-primary="virtual IP addresses (VIPs)" data-type="indexterm" id=
              "id-oPCkFXFzie"></a>Virtual IP addresses (VIPs) are not assigned to any particular
              network interface. Instead, they are usually shared across many devices. However,
              from the user’s perspective, the VIP remains a single, regular IP address. In theory,
              this practice allows us to hide implementation details (such as the number of
              machines behind a particular VIP) and facilitates maintenance, because we can
              schedule upgrades or add more machines to the pool without the user knowing.
            </p>
            <p>
              <a data-primary="network load balancer" data-type="indexterm" id=
              "id-oPCAS7Izie"></a>In practice, the most important part of VIP implementation is a
              device called the <em>network load balancer</em>. The balancer receives packets and
              forwards them to one of the machines behind the VIP. These backends can then further
              process the request.
            </p>
            <p>
              There are several possible approaches the balancer can take in deciding which backend
              should receive the request. The first (and perhaps most intuitive) approach is to
              always prefer the least loaded backend. In theory, this approach should result in the
              best end-user experience because requests are always routed to the least busy
              machine. Unfortunately, this logic breaks down quickly in the case of stateful
              protocols, which must use the same backend for the duration of a request. This
              requirement means that the balancer must keep track of all connections sent through
              it in order to make sure that all subsequent packets are sent to the correct backend.
              The alternative is to use some parts of a packet to create a connection ID (possibly
              using a hash function and some information from the packet), and to use the
              connection ID to select a backend. For example, the connection ID could be expressed
              as:
            </p>
            <pre data-type="programlisting">
id(packet) mod N
</pre>
            <p>
              where <code>id</code> is a function that takes <code>packet</code> as an input and
              produces a connection ID, and <code>N</code> is the number of configured backends.
            </p>
            <p>
              This avoids storing state, and all packets belonging to a single connection are
              always forwarded to the same backend. Success? Not quite yet. What happens if one
              backend fails and needs to be removed from the backend list? Suddenly <code>N</code>
              becomes <code>N-1</code> and then, <code>id(packet) mod N</code> becomes
              <code>id(packet) mod N-1</code>. Almost every packet suddenly maps to a different
              backend! If backends don’t share any state between themselves, this remapping forces
              a reset of almost all of the existing connections. This scenario is definitely
              <em>not</em> the best user experience, even if such events are infrequent.
            </p>
            <p>
              <a data-primary="consistent hashing" data-type="indexterm" id=
              "id-vgCPSDijiE"></a>Fortunately, there <em>is</em> an alternate solution that doesn’t
              require keeping the state of every connection in memory, but won’t force all
              connections to reset when a single machine goes down: <em>consistent hashing</em>.
              Proposed in 1997, consistent hashing <a data-type="xref" href=
              "bibliography.html#Kar97" target="_blank">[Kar97]</a> describes a
              way to provide a mapping algorithm that remains relatively stable even when new
              backends are added to or removed from the list. This approach minimizes the
              disruption to existing connections when the pool of backends changes. As a result, we
              can usually use simple connection tracking, but fall back to consistent hashing when
              the system is under pressure (e.g., during an ongoing denial of service attack).
            </p>
            <p>
              <a data-primary="Network Address Translation" data-type="indexterm" id=
              "id-lrCPS1uDiN"></a>Returning to the larger question: how exactly should a network
              load balancer forward packets to a selected VIP backend? One solution is to perform a
              Network Address Translation. However, this requires keeping an entry of every single
              connection in the tracking table, which precludes having a completely stateless
              fallback mechanism.
            </p>
            <p>
              <a data-primary="Direct Server Response (DSR)" data-type="indexterm" id=
              "id-nxC1SBUKiq"></a>Another solution is to modify information on the data link layer
              (layer 2 of the OSI networking model). By changing the destination MAC address of a
              forwarded packet, the balancer can leave all the information in upper layers intact,
              so the backend receives the original source and destination IP addresses. The backend
              can then send a reply directly to the original sender—a technique known as <em>Direct
              Server Response</em> (DSR). If user requests are small and replies are large (e.g.,
              most HTTP requests), DSR provides tremendous savings, because only a small fraction
              of traffic need traverse the load balancer. Even better, DSR does not require us to
              keep state on the load balancer device. Unfortunately, using layer 2 for internal
              load balancing <em>does</em> incur serious disadvantages when deployed at scale: all
              machines (i.e., all load balancers and all their backends) must be able to reach each
              other at the data link layer. This isn’t an issue if this connectivity can be
              supported by the network and the number of machines doesn’t grow excessively, because
              all the machines need to reside in a single broadcast domain. As you may imagine,
              Google outgrew this solution quite some time ago, and had to find an alternate
              approach.
            </p>
            <p>
              <a data-primary="Generic Routing Encapsulation (GRE)" data-type="indexterm" id=
              "id-NnCnSgC7iJ"></a>Our current VIP load balancing solution <a data-type="xref" href=
              "bibliography.html#Eis16" target="_blank">[Eis16]</a> uses packet
              encapsulation. A network load balancer puts the forwarded packet into another IP
              packet with Generic Routing Encapsulation (GRE) <a data-type="xref" href=
              "bibliography.html#Han94" target="_blank">[Han94]</a>, and uses a
              backend’s address as the destination. A backend receiving the packet strips off the
              outer IP+GRE layer and processes the inner IP packet as if it were delivered directly
              to its network interface. The network load balancer and the backend no longer need to
              exist in the same broadcast domain; they can even be on separate continents as long
              as a route between the two exists.
            </p>
            <p>
              <a data-primary="load balancing" data-secondary="datacenter" data-tertiary=
              "packet encapsulation" data-type="indexterm" id="id-8nCmSqspiN"></a> <a data-primary=
              "packet encapsulation" data-type="indexterm" id="id-mwCPFpswiy"></a> <a data-primary=
              "encapsulation" data-type="indexterm" id="id-dkCaILseie"></a>Packet encapsulation is
              a powerful mechanism that provides great flexibility in the way our networks are
              designed and evolve. Unfortunately, encapsulation also comes with a price: inflated
              packet size. Encapsulation introduces overhead (24 bytes in the case of IPv4+GRE, to
              be precise), which can cause the packet to exceed the available Maximum Transmission
              Unit (MTU) size and require fragmentation.
            </p>
            <p>
              <a data-primary="Protocol Data Units" data-type="indexterm" id="id-mwCoSYHwiy"></a>
              <a data-primary="fragmentation" data-type="indexterm" id="id-dkCbFaHeie"></a>Once the
              packet reaches the datacenter, fragmentation can be avoided by using a larger MTU
              within the datacenter; however, this approach requires a network that supports large
              Protocol Data Units. As with many things at scale, load balancing sounds simple on
              the surface—load balance early and load balance often—but the difficulty is in the
              details, both for frontend load balancing and for handling packets once they reach
              the datacenter.
            </p>
          </section>
          <div class="footnotes" data-type="footnotes">
            <p data-type="footnote" id="id-Zo9unILcpcm">
              <sup><a href="load-balancing-frontend.html#id-Zo9unILcpcm-marker">103</a></sup>See <a href=
              "https://groups.google.com/forum/#!topic/public-dns-announce/67oxFjSLeUM" target=
              "_blank"><em class=
              "hyperlink">https://groups.google.com/forum/#!topic/public-dns-announce/67oxFjSLeUM</em></a>.
            </p>
            <p data-type="footnote" id="id-rq7uXS9Hzc7">
              <sup><a href="load-balancing-frontend.html#id-rq7uXS9Hzc7-marker">104</a></sup>Sadly, not all DNS resolvers
              respect the TTL value set by authoritative nameservers.
            </p>
            <p data-type="footnote" id="id-wbau7S1fmc8">
              <sup><a href="load-balancing-frontend.html#id-wbau7S1fmc8-marker">105</a></sup>Otherwise, users must establish a
              TCP connection just to get a list of IP addresses.
            </p>
          </div>
        </section>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="software-engineering-in-sre.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Chapter 18- Software Engineering in SRE
          </p></a>
        </div>
        <div class="next">
          <a href="load-balancing-datacenter.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 20- Load Balancing in the Datacenter
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
