<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 3 - Embracing Risk
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <section data-type="chapter" id="chapter_risk-management">
          <h1 class="heading">
            Embracing Risk
          </h1>
          <p class="byline author">
            Written by Marc Alvidrez<br>
            Edited by Kavita Guliani
          </p>
          <p>
            <a data-primary="reliability testing" data-secondary="reliability goals" data-type=
            "indexterm" id="id-LnC7Set2"></a><a data-primary="risk management" data-secondary=
            "balancing risk and innovation" data-type="indexterm" id="id-AnCDFmtB"></a>You might
            expect Google to try to build 100% reliable services—ones that never fail. It turns out
            that past a certain point, however, increasing reliability is worse for a service (and
            its users) rather than better! Extreme reliability comes at a cost: maximizing
            stability limits how fast new features can be developed and how quickly products can be
            delivered to users, and dramatically increases their cost, which in turn reduces the
            numbers of features a team can afford to offer. Further, users typically don’t notice
            the difference between high reliability and extreme reliability in a service, because
            the user experience is dominated by less reliable components like the cellular network
            or the device they are working with. Put simply, a user on a 99% reliable smartphone
            cannot tell the difference between 99.99% and 99.999% service reliability! With this in
            mind, rather than simply maximizing uptime, Site Reliability Engineering seeks to
            balance the risk of unavailability with the goals of rapid innovation and efficient
            service operations, so that users’ overall happiness—with features, service, and
            performance—is optimized.
          </p>
          <section data-type="sect1" id="managing-risk-OKsGh8">
            <h1 class="heading">
              Managing Risk
            </h1>
            <p>
              <a data-primary="risk management" data-secondary="costs of" data-type="indexterm" id=
              "id-W7CESLFDha"></a><a data-primary="costs" data-secondary=
              "of failing to embrace risk" data-secondary-sortas="failure to embrace risk"
              data-type="indexterm" id="id-e4CAFJFEh9"></a>Unreliable systems can quickly erode
              users’ confidence, so we want to reduce the chance of system failure. However,
              experience shows that as we build systems, cost does not increase linearly as
              reliability increments—an incremental improvement in reliability may cost 100x more
              than the previous increment. The costliness has two dimensions:
            </p>
            <dl>
              <dt class="subheaders">
                The cost of redundant machine/compute resources
              </dt>
              <dd>
                <p>
                  The cost associated with redundant equipment that, for example, allows us to take
                  systems offline for routine or unforeseen maintenance, or provides space for us
                  to store parity code blocks that provide a minimum data durability guarantee.
                </p>
              </dd>
              <dt class="subheaders">
                The opportunity cost
              </dt>
              <dd>
                <p>
                  The cost borne by an organization when it allocates engineering resources to
                  build systems or features that diminish risk instead of features that are
                  directly visible to or usable by end users. These engineers no longer work on new
                  features and products for end users.
                </p>
              </dd>
            </dl>
            <p>
              In SRE, we manage service reliability largely by managing risk. We conceptualize risk
              as a continuum. We give equal importance to figuring out how to engineer greater
              reliability into Google systems and identifying the appropriate level of tolerance
              for the services we run. Doing so allows us to perform a cost/benefit analysis to
              determine, for example, where on the (nonlinear) risk continuum we should place
              Search, Ads, Gmail, or Photos. Our goal is to explicitly align the risk taken by a
              given service with the risk the business is willing to bear. We strive to make a
              service reliable enough, but no <em>more</em> reliable than it needs to be. That is,
              when we set an availability target of 99.99%,we want to exceed it, but not by much:
              that would waste opportunities to add features to the system, clean up technical
              debt, or reduce its operational costs. In a sense, we view the availability target as
              both a minimum and a maximum. The key advantage of this framing is that it unlocks
              explicit, thoughtful risktaking.
            </p>
          </section>
          <section data-type="sect1" id="measuring-service-risk-XQsyTn">
            <h1 class="heading">
              Measuring Service Risk
            </h1>
            <p>
              <a data-primary="risk management" data-secondary="measuring service risk" data-type=
              "indexterm" id="id-e4CzSJFoT9"></a><a data-primary="unplanned downtime" data-type=
              "indexterm" id="id-GnCZFgFpT8"></a>As standard practice at Google, we are often best
              served by identifying an objective metric to represent the property of a system we
              want to optimize. By setting a target, we can assess our current performance and
              track improvements or degradations over time. For service risk, it is not immediately
              clear how to reduce all of the potential factors into a single metric. Service
              failures can have many potential effects, including user dissatisfaction, harm, or
              loss of trust; direct or indirect revenue loss; brand or reputational impact; and
              undesirable press coverage. Clearly, some of these factors are very hard to measure.
              To make this problem tractable and consistent across many types of systems we run, we
              focus on <em>unplanned downtime</em>.
            </p>
            <p>
              <a data-primary="service availability" data-secondary="time-based equation"
              data-type="indexterm" id="id-GnCJSjIpT8"></a><a data-primary=
              "time-based availability equation" data-type="indexterm" id="id-DnCaFbIGTy"></a>For
              most services, the most straightforward way of representing risk tolerance is in
              terms of the acceptable level of unplanned downtime. Unplanned downtime is captured
              by the desired level of <em>service availability</em>, usually expressed in terms of
              the number of "nines" we would like to provide: 99.9%, 99.99%, or 99.999%
              availability. Each additional nine corresponds to an order of magnitude improvement
              toward 100% availability. For serving systems, this metric is traditionally
              calculated based on the proportion of system uptime (see <a data-type="xref" href=
              "embracing-risk.html#risk-management_measuring-service-risk_time-availability-equation">Time-based
              availability</a>).
            </p>
            <div data-type="equation" id=
            "risk-management_measuring-service-risk_time-availability-equation">
              <h5 class="subheaders">
                Time-based availability
              </h5>
              <p>
                <img alt="" class="equation" src="../images/equation/eqn-4.png">
              </p>
            </div>
            <p>
              Using this formula over the period of a year, we can calculate the acceptable number
              of minutes of downtime to reach a given number of nines of availability. For example,
              a system with an availability target of 99.99% can be down for up to 52.56 minutes in
              a year and stay within its availability target; see <a data-type="xref" href=
              "availability-table.html#appendix_table-of-nines">Availability
              Table</a> for a table.
            </p>
            <p>
              <a data-primary="request success rate" data-type="indexterm" id=
              "id-4nCqSBTGTY"></a><a data-primary="user requests" data-secondary=
              "success rate metrics" data-type="indexterm" id="id-JnCzFpT2Tr"></a><a data-primary=
              "aggregate availability equation" data-type="indexterm" id="id-9nCrIGT7TP"></a>At
              Google, however, a time-based metric for availability is usually not meaningful
              because we are looking across globally distributed services. Our approach to fault
              isolation makes it very likely that we are serving at least a subset of traffic for a
              given service somewhere in the world at any given time (i.e., we are at least
              partially "up" at all times). Therefore, instead of using metrics around uptime, we
              define availability in terms of the <em>request success rate</em>. <a data-type=
              "xref" href=
              "embracing-risk.html#risk-management_measuring-service-risk_aggregate-availability-equation">Aggregate
              availability</a> shows how this yield-based metric is calculated over a rolling
              window (i.e., proportion of successful requests over a one-day window).
            </p>
            <div data-type="equation" id=
            "risk-management_measuring-service-risk_aggregate-availability-equation">
              <h5 class="subheaders">
                Aggregate availability
              </h5>
              <p data-type="tex"></p>
              <p>
                <img alt="" class="equation" src="../images/equation/eqn-5.png">
              </p>
            </div>
            <p>
              For example, a system that serves 2.5M requests in a day with a daily availability
              target of 99.99% can serve up to 250 errors and still hit its target for that given
              day.
            </p>
            <p>
              In a typical application, not all requests are equal: failing a new user sign-up
              request is different from failing a request polling for new email in the background.
              In many cases, however, availability calculated as the request success rate over all
              requests is a reasonable approximation of unplanned downtime, as viewed from the
              end-user perspective.
            </p>
            <p>
              Quantifying unplanned downtime as a request success rate also makes this availability
              metric more amenable for use in systems that do not typically serve end users
              directly. Most nonserving systems (e.g., batch, pipeline, storage, and transactional
              systems) have a well-defined notion of successful and unsuccessful units of work.
              Indeed, while the systems discussed in this chapter are primarily consumer and
              infrastructure serving systems, many of the same principles also apply to nonserving
              systems with minimal modification.
            </p>
            <p>
              For example, a batch process that extracts, transforms, and inserts the contents of
              one of our customer databases into a data warehouse to enable further analysis may be
              set to run periodically. Using a request success rate defined in terms of records
              successfully and unsuccessfully processed, we can calculate a useful availability
              metric despite the fact that the batch system does not run constantly.
            </p>
            <p>
              Most often, we set quarterly availability targets for a service and track our
              performance against those targets on a weekly, or even daily, basis. This strategy
              lets us manage the service to a high-level availability objective by looking for,
              tracking down, and fixing meaningful deviations as they inevitably arise. See
              <a data-type="xref" href="service-level-objectives.html">Service
              Level Objectives</a> for more details.
            </p>
          </section>
          <section data-type="sect1" id="risk-tolerance-of-services-jWsAcw">
            <h1 class="heading">
              Risk Tolerance of Services
            </h1>
            <p>
              <a data-primary="risk management" data-secondary="risk tolerance of services"
              data-type="indexterm" id="RMtoler3"></a>What does it mean to identify the risk
              tolerance of a service? In a formal environment or in the case of safety-critical
              systems, the risk tolerance of services is typically built directly into the basic
              product or service definition. At Google, services’ risk tolerance tends to be less
              clearly defined.
            </p>
            <p>
              To identify the risk tolerance of a service, SREs must work with the product owners
              to turn a set of business goals into explicit objectives to which we can engineer. In
              this case, the business goals we’re concerned about have a direct impact on the
              performance and reliability of the service offered. In practice, this translation is
              easier said than done. While consumer services often have clear product owners, it is
              unusual for infrastructure services (e.g., storage systems or a general-purpose HTTP
              caching layer) to have a similar structure of product ownership. We’ll discuss the
              consumer and infrastructure cases in turn.
            </p>
            <section data-type="sect2" id=
            "identifying-the-risk-tolerance-of-consumer-services-o8sytAcy">
              <h2 class="subheaders">
                Identifying the Risk Tolerance of Consumer Services
              </h2>
              <p>
                <a data-primary="consumer services, identifying risk tolerance of" data-type=
                "indexterm" id="CSrisk3"></a>Our consumer services often have a product team that
                acts as the business owner for an application. For example, Search, Google Maps,
                and Google Docs each have their own product managers. These product managers are
                charged with understanding the users and the business, and for shaping the product
                for success in the marketplace. When a product team exists, that team is usually
                the best resource to discuss the reliability requirements for a service. In the
                absence of a dedicated product team, the engineers building the system often play
                this role either knowingly or unknowingly.
              </p>
              <p>
                There are many factors to consider when assessing the risk tolerance of services,
                such as the following:
              </p>
              <ul>
                <li>What level of availability is required?
                </li>
                <li>Do different types of failures have different effects on the service?
                </li>
                <li>How can we use the service cost to help locate a service on the risk continuum?
                </li>
                <li>What other service metrics are important to take into account?
                </li>
              </ul>
              <section data-type="sect3" id="target-level-of-availability-Ymsgh1tKcW">
                <h3 class="subheaders">
                  Target level of availability
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary=
                  "target for consumer services" data-type="indexterm" id=
                  "id-zdCxSGFJhxtXce"></a>The target level of availability for a given Google
                  service usually depends on the function it provides and how the service is
                  positioned in the marketplace. The following list includes issues to consider:
                </p>
                <ul>
                  <li>What level of service will the users expect?
                  </li>
                  <li>Does this service tie directly to revenue (either our revenue, or our
                  customers’ revenue)?
                  </li>
                  <li>Is this a paid service, or is it free?
                  </li>
                  <li>If there are competitors in the marketplace, what level of service do those
                  competitors provide?
                  </li>
                  <li>Is this service targeted at consumers, or at enterprises?
                  </li>
                </ul>
                <p>
                  <a data-primary="Google Apps for Work" data-type="indexterm" id=
                  "id-VMCPSjt1hatzcY"></a>Consider the requirements of Google Apps for Work. The
                  majority of its users are enterprise users, some large and some small. These
                  enterprises depend on Google Apps for Work services (e.g., Gmail, Calendar,
                  Drive, Docs) to provide tools that enable their employees to perform their daily
                  work. Stated another way, an outage for a Google Apps for Work service is an
                  outage not only for Google, but also for all the enterprises that critically
                  depend on us. For a typical Google Apps for Work service, we might set an
                  external quarterly availability target of 99.9%, and back this target with a
                  stronger internal availability target and a contract that stipulates penalties if
                  we fail to deliver to the external target.
                </p>
                <p>
                  <a data-primary="YouTube" data-type="indexterm" id=
                  "id-rjCXSEhWhYtMcd"></a>YouTube provides a contrasting set of considerations.
                  When Google acquired YouTube, we had to decide on the appropriate availability
                  target for the website. In 2006, YouTube was focused on consumers and was in a
                  very different phase of its business lifecycle than Google was at the time. While
                  YouTube already had a great product, it was still changing and growing rapidly.
                  We set a lower availability target for YouTube than for our enterprise products
                  because rapid feature development was correspondingly more important.
                </p>
              </section>
              <section data-type="sect3" id="types-of-failures-vJsDTwtecQ">
                <h3 class="subheaders">
                  Types of failures
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary=
                  "types of consumer service failures" data-type="indexterm" id=
                  "id-yYCASpFrT4tzcX"></a>The expected shape of failures for a given service is
                  another important consideration. How resilient is our business to service
                  downtime? Which is worse for the service: a constant low rate of failures, or an
                  occasional full-site outage? Both types of failure may result in the same
                  absolute number of errors, but may have vastly different impacts on the business.
                </p>
                <p>
                  An illustrative example of the difference between full and partial outages
                  naturally arises in systems that serve private information. Consider a contact
                  management application, and the difference between intermittent failures that
                  cause profile pictures to fail to render, versus a failure case that results in a
                  user’s private contacts being shown to another user. The first case is clearly a
                  poor user experience, and SREs would work to remediate the problem quickly. In
                  the second case, however, the risk of exposing private data could easily
                  undermine basic user trust in a significant way. As a result, taking down the
                  service entirely would be appropriate during the debugging and potential clean-up
                  phase for the second case.
                </p>
                <p>
                  At the other end of services offered by Google, it is sometimes acceptable to
                  have regular outages during maintenance windows. A number of years ago, the Ads
                  Frontend used to be one such service. It is used by advertisers and website
                  publishers to set up, configure, run, and monitor their advertising campaigns.
                  Because most of this work takes place during normal business hours, we determined
                  that occasional, regular, scheduled outages in the form of maintenance windows
                  would be acceptable, and we counted these scheduled outages as planned downtime,
                  not unplanned downtime.
                </p>
              </section>
              <section data-type="sect3" id="cost-lqsxczt0c9">
                <h3 class="subheaders">
                  Cost
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary="cost factors" data-type=
                  "indexterm" id="id-VMCPSrFwcatzcY"></a><a data-primary="costs" data-secondary=
                  "availability targets and" data-type="indexterm" id="id-rjCpFYFzcYtMcd"></a>Cost
                  is often the key factor in determining the appropriate availability target for a
                  service. Ads is in a particularly good position to make this trade-off because
                  request successes and failures can be directly translated into revenue gained or
                  lost. In determining the availability target for each service, we ask questions
                  such as:
                </p>
                <ul>
                  <li>If we were to build and operate these systems at one more nine of
                  availability, what would our incremental increase in revenue be?
                  </li>
                  <li>Does this additional revenue offset the cost of reaching that level of
                  reliability?
                  </li>
                </ul>
                <p>
                  To make this trade-off equation more concrete, consider the following
                  cost/benefit for an example service where each request has equal value:
                </p>
                <ul class="simplelist">
                  <li>Proposed improvement in availability target: 99.9% → 99.99%
                  </li>
                  <li>Proposed increase in availability: 0.09%
                  </li>
                  <li>Service revenue: $1M
                  </li>
                  <li>Value of improved availability: $1M * 0.0009 = $900
                  </li>
                </ul>
                <p>
                  In this case, if the cost of improving availability by one nine is less than
                  $900, it is worth the investment. If the cost is greater than $900, the costs
                  will exceed the projected increase in revenue.
                </p>
                <p>
                  It may be harder to set these targets when we do not have a simple translation
                  function between reliability and revenue. One useful strategy may be to consider
                  the background error rate of ISPs on the Internet. If failures are being measured
                  from the end-user perspective and it is possible to drive the error rate for the
                  service below the background error rate, those errors will fall within the noise
                  for a given user’s Internet connection. While there are significant differences
                  between ISPs and protocols (e.g., TCP versus UDP, IPv4 versus IPv6), we’ve
                  measured the typical background error rate for ISPs as falling between 0.01% and
                  1%.
                </p>
              </section>
              <section data-type="sect3" id="other-service-metrics-nqsXipt8c1">
                <h3 class="subheaders">
                  Other service metrics
                </h3>
                <p>
                  Examining the risk tolerance of services in relation to metrics besides
                  availability is often fruitful. Understanding which metrics are important and
                  which metrics aren’t important provides us with degrees of freedom when
                  attempting to take thoughtful risks.
                </p>
                <p>
                  <a data-primary="service latency" data-secondary="looser approach to" data-type=
                  "indexterm" id="id-wqC7SDIziztGc4"></a>Service latency for our Ads systems
                  provides an illustrative example. When Google first launched Web Search, one of
                  the service’s key distinguishing features was speed. When we introduced AdWords,
                  which displays advertisements next to search results, a key requirement of the
                  system was that the ads should not slow down the search experience. This
                  requirement has driven the engineering goals in each generation of AdWords
                  systems and is treated as an invariant.
                </p>
                <p>
                  <a data-primary="AdSense" data-type="indexterm" id=
                  "id-x1C4S7tGintec7"></a>AdSense, Google’s ads system that serves contextual ads
                  in response to requests from JavaScript code that publishers insert into their
                  websites, has a very different latency goal. The latency goal for AdSense is to
                  avoid slowing down the rendering of the third-party page when inserting
                  contextual ads. The specific latency target, then, is dependent on the speed at
                  which a given publisher’s page renders. This means that AdSense ads can generally
                  be served hundreds of milliseconds slower than AdWords ads.
                </p>
                <p>
                  This looser serving latency requirement has allowed us to make many smart
                  trade-offs in provisioning (i.e., determining the quantity and locations of
                  serving resources we use), which save us substantial cost over naive
                  provisioning. In other words, given the relative insensitivity of the AdSense
                  service to moderate changes in latency performance, we are able to consolidate
                  serving into fewer geographical locations, reducing our operational
                  overhead.<a data-primary="" data-startref="CSrisk3" data-type="indexterm" id=
                  "id-PnCpSwhPiKt1cZ"></a>
                </p>
              </section>
            </section>
            <section data-type="sect2" id=
            "identifying-the-risk-tolerance-of-infrastructure-services-g0sxhdc2">
              <h2 class="subheaders">
                Identifying the Risk Tolerance of Infrastructure Services
              </h2>
              <p>
                <a data-primary="infrastructure services" data-secondary=
                "identifying risk tolerance of" data-type="indexterm" id="ISrisk3"></a>The
                requirements for building and running infrastructure components differ from the
                requirements for consumer products in a number of ways. A fundamental difference is
                that, by definition, infrastructure components have multiple clients, often with
                varying needs.
              </p>
              <section data-type="sect3" id="target-level-of-availability-1LsEIBhMcM">
                <h3 class="subheaders">
                  Target level of availability
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary=
                  "target for infrastructure service" data-type="indexterm" id=
                  "id-ZbCxSMFkIYhyce"></a><a data-primary="Bigtable" data-type="indexterm" id=
                  "id-zdCkFGFpI7hXce"></a>Consider Bigtable <a data-type="xref" href=
                  "bibliography.html#Cha06" target="_blank">[Cha06]</a>, a
                  massive-scale distributed storage system for structured data. Some consumer
                  services serve data directly from Bigtable in the path of a user request. Such
                  services need low latency and high reliability. Other teams use Bigtable as a
                  repository for data that they use to perform offline analysis (e.g., MapReduce)
                  on a regular basis. These teams tend to be more concerned about throughput than
                  reliability. Risk tolerance for these two use cases is quite distinct.
                </p>
                <p>
                  One approach to meeting the needs of both use cases is to engineer all
                  infrastructure services to be ultra-reliable. Given the fact that these
                  infrastructure services also tend to aggregate huge amounts of resources, such an
                  approach is usually far too expensive in practice. To understand the different
                  needs of the different types of users, you can look at the desired state of the
                  request queue for each type of Bigtable user.
                </p>
              </section>
              <section data-type="sect3" id="types-of-failures-YmsMtxhKcW">
                <h3 class="subheaders">
                  Types of failures
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary=
                  "types of infrastructure services failures" data-type="indexterm" id=
                  "id-zdCxSGFDt7hXce"></a>The low-latency user wants Bigtable’s request queues to
                  be (almost always) empty so that the system can process each outstanding request
                  immediately upon arrival. (Indeed, inefficient queuing is often a cause of high
                  tail latency.) The user concerned with offline analysis is more interested in
                  system throughput, so that user wants request queues to never be empty. To
                  optimize for throughput, the Bigtable system should never need to idle while
                  waiting for its next request.
                </p>
                <p>
                  As you can see, success and failure are antithetical for these sets of users.
                  Success for the low-latency user is failure for the user concerned with offline
                  analysis.
                </p>
              </section>
              <section data-type="sect3" id="cost-vJskhPhecQ">
                <h3 class="subheaders">
                  Cost
                </h3>
                <p>
                  <a data-primary="service availability" data-secondary="cost factors" data-type=
                  "indexterm" id="id-yYCASpFnhBhzcX"></a><a data-primary="costs" data-secondary=
                  "availability targets and" data-type="indexterm" id="id-VMCpFrF1hzhzcY"></a>One
                  way to satisfy these competing constraints in a cost-effective manner is to
                  partition the infrastructure and offer it at multiple independent levels of
                  service. In the Bigtable example, we can build two types of clusters: low-latency
                  clusters and throughput clusters. The low-latency clusters are designed to be
                  operated and used by services that need low latency and high reliability. To
                  ensure short queue lengths and satisfy more stringent client isolation
                  requirements, the Bigtable system can be provisioned with a substantial amount of
                  slack capacity for reduced contention and increased redundancy. The throughput
                  clusters, on the other hand, can be provisioned to run very hot and with less
                  redundancy, optimizing throughput over latency. In practice, we are able to
                  satisfy these relaxed needs at a much lower cost, perhaps as little as 10–50% of
                  the cost of a low-latency cluster. Given Bigtable’s massive scale, this cost
                  savings becomes significant very quickly.
                </p>
                <p>
                  <a data-primary="Spanner" data-type="indexterm" id="id-VMCPSoI1hzhzcY"></a>The
                  key strategy with regards to infrastructure is to deliver services with
                  explicitly delineated levels of service, thus enabling the clients to make the
                  right risk and cost trade-offs when building their systems. With explicitly
                  delineated levels of service, the infrastructure providers can effectively
                  externalize the difference in the cost it takes to provide service at a given
                  level to clients. Exposing cost in this way motivates the clients to choose the
                  level of service with the lowest cost that still meets their needs. For example,
                  Google+ can decide to put data critical to enforcing user privacy in a
                  high-availability, globally consistent datastore (e.g., a globally replicated
                  SQL-like system like Spanner <a data-type="xref" href=
                  "bibliography.html#Cor12" target="_blank">[Cor12]</a>), while
                  putting optional data (data that isn’t critical, but that enhances the user
                  experience) in a cheaper, less reliable, less fresh, and eventually consistent
                  datastore (e.g., a NoSQL store with best-effort replication like Bigtable).
                </p>
                <p>
                  Note that we can run multiple classes of services using identical hardware and
                  software. We can provide vastly different service guarantees by adjusting a
                  variety of service characteristics, such as the quantities of resources, the
                  degree of redundancy, the geographical provisioning constraints, and, critically,
                  the infrastructure software <span class="keep-together">configuration</span>.
                </p>
              </section>
              <section data-type="sect3" id="example-frontend-infrastructure-lqswTZh0c9">
                <h3 class="subheaders">
                  Example: Frontend infrastructure
                </h3>
                <p>
                  To demonstrate that these risk-tolerance assessment principles do not just apply
                  to storage infrastructure, let’s look at another large class of service: Google’s
                  frontend infrastructure. The frontend infrastructure consists of reverse proxy
                  and load balancing systems running close to the edge of our network. These are
                  the systems that, among other things, serve as one endpoint of the connections
                  from end users (e.g., terminate TCP from the user’s browser). Given their
                  critical role, we engineer these systems to deliver an extremely high level of
                  reliability. While consumer services can often limit the visibility of
                  unreliability in backends, these infrastructure systems are not so lucky. If a
                  request never makes it to the application service frontend server, it is lost.
                </p>
                <p>
                  We’ve explored the ways to identify the risk tolerance of both consumer and
                  infrastructure services. Now, we’ll discuss using that tolerance level to manage
                  unreliability via error budgets.<a data-primary="" data-startref="RMtoler3"
                  data-type="indexterm" id="id-rjCXSgIOTMhMcd"></a>
                </p>
              </section>
            </section>
          </section>
          <section data-type="sect1" id="xref_risk-management_unreliability-budgets">
            <h1 class="heading">
              Motivation for Error Budgets<sup><a data-type="noteref" href="embracing-risk.html#id-na2u1S2SKi1" id=
              "id-na2u1S2SKi1-marker">14</a></sup>
            </h1>
            <p class="byline author">
              Written by Mark Roth<br>
              Edited by Carmela Quinito
            </p>
            <p>
              <a data-primary="error budgets" data-secondary="motivation for" data-type="indexterm"
              id="id-4nCqSqt9iY"></a><a data-primary="reliability testing" data-secondary=
              "error budgets" data-type="indexterm" id="RTerror3"></a><a data-primary=
              "risk management" data-secondary="error budgets" data-type="indexterm" id=
              "RMerror3"></a>Other chapters in this book discuss how tensions can arise between
              product development teams and SRE teams, given that they are generally evaluated on
              different metrics. Product development performance is largely evaluated on product
              velocity, which creates an incentive to push new code as quickly as possible.
              Meanwhile, SRE performance is (unsurprisingly) evaluated based upon reliability of a
              service, which implies an incentive to push back against a high rate of change.
              Information asymmetry between the two teams further amplifies this inherent tension.
              The product developers have more visibility into the time and effort involved in
              writing and releasing their code, while the SREs have more visibility into the
              service’s reliability (and the state of production in general).
            </p>
            <p>
              These tensions often reflect themselves in different opinions about the level of
              effort that should be put into engineering practices. The following list presents
              some typical tensions:
            </p>
            <dl>
              <dt class="subheaders">
                Software fault tolerance
              </dt>
              <dd>
                <p>
                  <a data-primary="software fault tolerance" data-type="indexterm" id=
                  "id-ZbCxSDSXFdTjie"></a>How hardened do we make the software to unexpected
                  events? Too little, and we have a brittle, unusable product. Too much, and we
                  have a product no one wants to use (but that runs very stably).
                </p>
              </dd>
              <dt class="subheaders">
                Testing
              </dt>
              <dd>
                <p>
                  Again, not enough testing and you have embarrassing outages, privacy data leaks,
                  or a number of other press-worthy events. Too much testing, and you might lose
                  your market.
                </p>
              </dd>
              <dt class="subheaders">
                Push frequency
              </dt>
              <dd>
                <p>
                  <a data-primary="push frequency" data-type="indexterm" id=
                  "id-rjCXSOSOTVTAid"></a>Every push is risky. How much should we work on reducing
                  that risk, versus doing other work?
                </p>
              </dd>
              <dt class="subheaders">
                Canary duration and size
              </dt>
              <dd>
                <p>
                  <a data-primary="canarying" data-type="indexterm" id="id-x1C4SjSGimTvi7"></a>It’s
                  a best practice to test a new release on some small subset of a typical workload,
                  a practice often called <em>canarying</em>. How long do we wait, and how big is
                  the canary?
                </p>
              </dd>
            </dl>
            <p>
              Usually, preexisting teams have worked out some kind of informal balance between them
              as to where the risk/effort boundary lies. Unfortunately, one can rarely prove that
              this balance is optimal, rather than just a function of the negotiating skills of the
              engineers involved. Nor should such decisions be driven by politics, fear, or hope.
              (Indeed, Google SRE’s unofficial motto is "Hope is not a strategy.") Instead, our
              goal is to define an objective metric, agreed upon by both sides, that can be used to
              guide the negotiations in a reproducible way. The more data-based the decision can
              be, the better.
            </p>
            <section data-type="sect2" id="forming-your-error-budget-vJs8iDio">
              <h2 class="subheaders">
                Forming Your Error Budget
              </h2>
              <p>
                <a data-primary="error budgets" data-secondary="forming" data-type="indexterm" id=
                "id-yYCASpFKiOin"></a>In order to base these decisions on objective data, the two
                teams jointly define a quarterly error budget based on the service’s service level
                objective, or SLO (see <a data-type="xref" href=
                "service-level-objectives.html">Service Level Objectives</a>).
                The error budget provides a clear, objective metric that determines how unreliable
                the service is allowed to be within a single quarter. This metric removes the
                politics from negotiations between the SREs and the product developers when
                deciding how much risk to allow.
              </p>
              <p>
                Our practice is then as follows:
              </p>
              <ul>
                <li>Product Management defines an SLO, which sets an expectation of how much uptime
                the service should have per quarter.
                </li>
                <li>The actual uptime is measured by a neutral third party: our monitoring system.
                </li>
                <li>The difference between these two numbers is the "budget" of how much
                "unreliability" is remaining for the quarter.
                </li>
                <li>As long as the uptime measured is above the SLO—in other words, as long as
                there is error budget remaining—new releases can be pushed.
                </li>
              </ul>
              <p>
                For example, imagine that a service’s SLO is to successfully serve 99.999% of all
                queries per quarter. This means that the service’s error budget is a failure rate
                of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the
                expected queries for the quarter, the problem spends 20% of the service’s quarterly
                error budget.<a data-primary="" data-startref="RTerror3" data-type="indexterm" id=
                "id-wqC7Sehzigi9"></a>
              </p>
            </section>
            <section data-type="sect2" id="benefits-lqsduViV">
              <h2 class="subheaders">
                Benefits
              </h2>
              <p>
                <a data-primary="error budgets" data-secondary="benefits of" data-type="indexterm"
                id="id-VMCPSrFbuQid"></a>The main benefit of an error budget is that it provides a
                common incentive that allows both product development and SRE to focus on finding
                the right balance between innovation and reliability.
              </p>
              <p>
                Many products use this control loop to manage release velocity: as long as the
                system’s SLOs are met, releases can continue. If SLO violations occur frequently
                enough to expend the error budget, releases are temporarily halted while additional
                resources are invested in system testing and development to make the system more
                resilient, improve its performance, and so on. More subtle and effective approaches
                are available than this simple on/off technique:<sup><a data-type="noteref" href=
                "embracing-risk.html#id-bb2umSeIPu8iV" id="id-bb2umSeIPu8iV-marker">15</a></sup> for instance, slowing
                down releases or rolling them back when the SLO-violation error budget is close to
                being used up.
              </p>
              <p>
                For example, if product development wants to skimp on testing or increase push
                velocity and SRE is resistant, the error budget guides the decision. When the
                budget is large, the product developers can take more risks. When the budget is
                nearly drained, the product developers themselves will push for more testing or
                slower push velocity, as they don’t want to risk using up the budget and stall
                their launch. In effect, the product development team becomes self-policing. They
                know the budget and can manage their own risk. (Of course, this outcome relies on
                an SRE team having the authority to actually stop launches if the SLO is broken.)
              </p>
              <p>
                What happens if a network outage or datacenter failure reduces the measured SLO?
                Such events also eat into the error budget. As a result, the number of new pushes
                may be reduced for the remainder of the quarter. The entire team supports this
                reduction because everyone shares the responsibility for uptime.
              </p>
              <p>
                The budget also helps to highlight some of the costs of overly high reliability
                targets, in terms of both inflexibility and slow innovation. If the team is having
                trouble launching new features, they may elect to loosen the SLO (thus increasing
                the error budget) in order to increase innovation.<a data-primary="" data-startref=
                "RMerror3" data-type="indexterm" id="id-PnCpSZT8ujio"></a>
              </p>
              <aside class="highlight" data-type="sidebar" id="key-insights-EDSWceuYiM">
                <h5 class="heading">
                  Key Insights
                </h5>
                <ul>
                  <li>
                    <a data-primary="risk management" data-secondary="key insights" data-type=
                    "indexterm" id="id-XmC9SkSgS0FocPuniG"></a>Managing service reliability is
                    largely about managing risk, and managing risk can be costly.
                  </li>
                  <li>100% is probably never the right reliability target: not only is it
                  impossible to achieve, it’s typically more reliability than a service’s users
                  want or notice. Match the profile of the service to the risk the business is
                  willing to take.
                  </li>
                  <li>An error budget aligns incentives and emphasizes joint ownership between SRE
                  and product development. Error budgets make it easier to decide the rate of
                  releases and to effectively defuse discussions about outages with stakeholders,
                  and allows multiple teams to reach the same conclusion about production risk
                  without rancor.
                  </li>
                </ul>
              </aside>
            </section>
          </section>
          <div class="footnotes" data-type="footnotes">
            <p data-type="footnote" id="id-na2u1S2SKi1">
              <sup><a href="embracing-risk.html#id-na2u1S2SKi1-marker">14</a></sup>An early version of this section
              appeared as an article in <em>;login:</em> (August 2015, vol. 40, no. 4).
            </p>
            <p data-type="footnote" id="id-bb2umSeIPu8iV">
              <sup><a href="embracing-risk.html#id-bb2umSeIPu8iV-marker">15</a></sup>Known as "bang/bang" control—see
              <a href="https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control" target=
              "_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Bang–bang_control</em></a>.
            </p>
          </div>
        </section>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="part2.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Part II - Principles
          </p></a>
        </div>
        <div class="next">
          <a href="service-level-objectives.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 4- Service Level Objectives
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
