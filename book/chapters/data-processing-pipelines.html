<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 25 - Data Processing Pipelines
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <section data-type="chapter" id="chapter_continuous-pipelines">
          <h1 class="heading">
            Data Processing Pipelines
          </h1>
          <p class="byline author">
            Written by Dan Dennison<br>
            Edited by Tim Harvey
          </p>
          <p>
            This chapter focuses on the real-life challenges of managing data processing pipelines
            of depth and complexity. It considers the frequency continuum between periodic
            pipelines that run very infrequently through to continuous pipelines that never stop
            running, and discusses the discontinuities that can produce significant operational
            problems. A fresh take on the leader-follower model is presented as a more reliable and
            better-scaling alternative to the periodic pipeline for processing Big Data.
          </p>
          <section data-type="sect1" id="origin-of-the-pipeline-design-pattern-K7sBhp">
            <h1 class="heading">
              Origin of the Pipeline Design Pattern
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="origin of" data-type=
              "indexterm" id="id-qXCVS4FEho"></a><a data-primary="Big Data" data-type="indexterm"
              id="id-1nCkFqFAhD"></a><a data-primary="coroutines" data-type="indexterm" id=
              "id-YQCxIYFzhG"></a><a data-primary="DTSS communication files" data-type="indexterm"
              id="id-vgCrtMFXhq"></a><a data-primary="UNIX pipe" data-type="indexterm" id=
              "id-lrCzh8FEhW"></a><a data-primary="ETL pipelines" data-type="indexterm" id=
              "id-nxCeTLFnh9"></a>The classic approach to data processing is to write a program
              that reads in data, transforms it in some desired way, and outputs new data.
              Typically, the program is scheduled to run under the control of a periodic scheduling
              program such as cron. This design pattern is called a <em>data pipeline</em>. Data
              pipelines go as far back as co-routines <a data-type="xref" href=
              "bibliography.html#Con63" target="_blank">[Con63]</a>, the DTSS
              communication files <a data-type="xref" href=
              "bibliography.html#Bul80" target="_blank">[Bul80]</a>, the UNIX
              pipe <a data-type="xref" href="bibliography.html#McI86" target=
              "_blank">[McI86]</a>, and later, ETL pipelines,<sup><a data-type="noteref" href=
              "data-processing-pipelines.html#id-Pj0ubC7F7h4" id="id-Pj0ubC7F7h4-marker">116</a></sup> but such pipelines have
              gained increased attention with the rise of "Big Data," or "datasets that are so
              large and so complex that traditional data processing applications are
              inadequate."<sup><a data-type="noteref" href="data-processing-pipelines.html#id-OdauPsmFvhQ" id=
              "id-OdauPsmFvhQ-marker">117</a></sup>
            </p>
          </section>
          <section data-type="sect1" id=
          "initial-effect-of-big-data-on-the-simple-pipeline-pattern-a7swT4">
            <h1 class="heading">
              Initial Effect of Big Data on the Simple Pipeline Pattern
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="effect of big data on"
              data-type="indexterm" id="id-1nCxSqFBTD"></a><a data-primary="one-phase pipelines"
              data-type="indexterm" id="id-YQCeFYFNTG"></a><a data-primary=
              "data processing pipelines" data-secondary="simple vs. multiphase pipelines"
              data-type="indexterm" id="id-vgClIMFMTq"></a>Programs that perform periodic or
              continuous transformations on Big Data are usually referred to as "simple, one-phase
              pipelines."
            </p>
            <p>
              <a data-primary="multiphase pipelines" data-type="indexterm" id=
              "id-YQCDSKINTG"></a>Given the scale and processing complexity inherent to Big Data,
              programs are typically organized into a chained series, with the output of one
              program becoming the input to the next. There may be varied rationales for this
              arrangement, but it is typically designed for ease of reasoning about the system and
              not usually geared toward operational efficiency. Programs organized this way are
              called <em>multiphase pipelines</em>, because each program in the chain acts as a
              discrete data processing phase.
            </p>
            <p>
              <a data-primary="data processing pipelines" data-secondary="pipeline depth"
              data-type="indexterm" id="id-vgCPSwtMTq"></a>The number of programs chained together
              in series is a measurement known as the <em>depth</em> of a pipeline. Thus, a shallow
              pipeline may only have one program with a corresponding pipeline depth measurement of
              one, whereas a deep pipeline may have a pipeline depth in the tens or hundreds of
              programs.
            </p>
          </section>
          <section data-type="sect1" id="challenges-with-the-periodic-pipeline-pattern-00sGcX">
            <h1 class="heading">
              Challenges with the Periodic Pipeline Pattern
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary=
              "challenges to periodic pattern" data-type="indexterm" id=
              "id-YQCDSYFKcG"></a><a data-primary="periodic pipelines" data-type="indexterm" id=
              "id-vgCJFMFecq"></a>Periodic pipelines are generally stable when there are sufficient
              workers for the volume of data and execution demand is within computational capacity.
              In addition, instabilities such as processing bottlenecks are avoided when the number
              of chained jobs and the relative throughput between jobs remain uniform.
            </p>
            <p>
              <a data-primary="MapReduce" data-type="indexterm" id=
              "id-vgCPSoIecq"></a><a data-primary="Flume" data-type="indexterm" id=
              "id-lrClFGI0cW"></a>Periodic pipelines are useful and practical, and we run them on a
              regular basis at Google. They are written with frameworks like MapReduce
              <a data-type="xref" href="bibliography.html#Dea04" target=
              "_blank">[Dea04]</a> and Flume <a data-type="xref" href=
              "bibliography.html#Cha10" target="_blank">[Cha10]</a>, among
              others.
            </p>
            <p>
              However, the collective SRE experience has been that the periodic pipeline model is
              fragile. We discovered that when a periodic pipeline is first installed with worker
              sizing, periodicity, chunking technique, and other parameters carefully tuned,
              performance is initially reliable. However, organic growth and change inevitably
              begin to stress the system, and problems arise. Examples of such problems include
              jobs that exceed their run deadline, resource exhaustion, and hanging processing
              chunks that entail corresponding operational load.
            </p>
          </section>
          <section data-type="sect1" id="trouble-caused-by-uneven-work-distribution-LEs0iQ">
            <h1 class="heading">
              Trouble Caused By Uneven Work Distribution
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary=
              "challenges of uneven work distribution" data-type="indexterm" id=
              "id-vgCPSMFjiq"></a><a data-primary="“embarrassingly parallel” algorithms"
              data-primary-sortas="embarrassingly parallel algorithms" data-type="indexterm" id=
              "id-lrClF8FDiW"></a>The key breakthrough of Big Data is the widespread application of
              "embarrassingly parallel" <a data-type="xref" href=
              "bibliography.html#Mol86" target="_blank">[Mol86]</a> algorithms
              to cut a large workload into chunks small enough to fit onto individual machines.
              Sometimes chunks require an uneven amount of resources relative to one another, and
              it is seldom initially obvious why particular chunks require different amounts of
              resources. For example, in a workload that is partitioned by customer, data chunks
              for some customers may be much larger than others. Because the customer is the point
              of indivisibility, end-to-end runtime is thus capped to the runtime of the largest
              customer.
            </p>
            <p>
              <a data-primary="“hanging chunk” problem" data-primary-sortas="hanging chunk problem"
              data-type="indexterm" id="id-lrCPSGIDiW"></a>The "hanging chunk" problem can result
              when resources are assigned due to differences between machines in a cluster or
              overallocation to a job. This problem arises due to the difficulty of some real-time
              operations on streams such as sorting "steaming" data. The pattern of typical user
              code is to wait for the total computation to complete before progressing to the next
              pipeline stage, commonly because sorting may be involved, which requires all data to
              proceed. That can significantly delay pipeline completion time, because completion is
              blocked on the worst-case performance as dictated by the chunking methodology in use.
            </p>
            <p>
              If this problem is detected by engineers or cluster monitoring infrastructure, the
              response can make matters worse. For example, the "sensible" or "default" response to
              a hanging chunk is to immediately kill the job and then allow the job to restart,
              because the blockage may well be the result of nondeterministic factors. However,
              because pipeline implementations by design usually don’t include checkpointing, work
              on all chunks is restarted from the beginning, thereby wasting the time, CPU cycles,
              and human effort invested in the previous cycle.
            </p>
          </section>
          <section data-type="sect1" id=
          "drawbacks-of-periodic-pipelines-in-distributed-environments-AVsluv">
            <h1 class="heading">
              Drawbacks of Periodic Pipelines in Distributed Environments
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="drawbacks of periodic"
              data-type="indexterm" id="PDPdraw25"></a><a data-primary="clusters" data-secondary=
              "cluster management solution" data-type="indexterm" id=
              "id-nxCzFLFwu9"></a><a data-primary="Borg" data-type="indexterm" id=
              "id-NnCaInFMuk"></a>Big Data periodic pipelines are widely used at Google, and so
              Google’s cluster management solution includes an alternative scheduling mechanism for
              such pipelines. This mechanism is necessary because, unlike continuously running
              pipelines, periodic pipelines typically run as lower-priority batch jobs. A
              lower-priority designation works well in this case because batch work is not
              sensitive to latency in the same way that Internet-facing web services are. In
              addition, in order to control cost by maximizing machine workload, Borg (Google’s
              cluster management system, <a data-type="xref" href=
              "bibliography.html#Ver15" target="_blank">[Ver15]</a>) assigns
              batch work to available machines. This priority can result in degraded startup
              latency, so pipeline jobs can potentially experience open-ended startup delays.
            </p>
            <p>
              <a data-primary="batching" data-type="indexterm" id="id-nxC1SlIwu9"></a>Jobs invoked
              through this mechanism have a number of natural limitations, resulting in various
              distinct behaviors. For example, jobs scheduled in the gaps left by user-facing web
              service jobs might be impacted in terms of availability of low-latency resources,
              pricing, and stability of access to resources. Execution cost is inversely
              proportional to requested startup delay, and directly proportional to resources
              consumed. Although batch scheduling may work smoothly in practice, excessive use of
              the batch scheduler (<a data-type="xref" href=
              "distributed-periodic-scheduling.html">Distributed Periodic
              Scheduling with Cron</a>) places jobs at risk of preemptions (see section 2.5 of
              <a data-type="xref" href="bibliography.html#Ver15" target=
              "_blank">[Ver15]</a>) when cluster load is high because other users are starved of
              batch resources. In light of the risk trade-offs, running a well-tuned periodic
              pipeline successfully is a delicate balance between high resource cost and risk of
              preemptions.
            </p>
            <p>
              Delays of up to a few hours might well be acceptable for pipelines that run daily.
              However, as the scheduled execution frequency increases, the minimum time between
              executions can quickly reach the minimum average delay point, placing a lower bound
              on the latency that a periodic pipeline can expect to attain. Reducing the job
              execution interval below this effective lower bound simply results in undesirable
              behavior rather than increased progress. The specific failure mode depends on the
              batch scheduling policy in use. For example, each new run might stack up on the
              cluster scheduler because the previous run is not complete. Even worse, the currently
              executing and nearly finished run could be killed when the next execution is
              scheduled to begin, completely halting all progress in the name of increasing
              executions.
            </p>
            <p>
              Note where the downward-sloping idle interval line intersects the scheduling delay in
              <a data-type="xref" href=
              "data-processing-pipelines.html#fig_continuous-pipelines_periodic-pipeline-execution">Figure 25-1</a>. In this
              scenario, lowering the execution interval much below 40 minutes for this ~20-minute
              job results in potentially overlapping executions with undesired consequences.
            </p>
            <figure class="horizontal vertical" id=
            "fig_continuous-pipelines_periodic-pipeline-execution">
              <img alt="Periodic pipeline execution interval versus idle time (log scale)" src=
              "../images/srle-2501.jpg">
              <figcaption>
                <span class="label">Figure 25-1.</span> Periodic pipeline execution interval versus
                idle time (log scale)
              </figcaption>
            </figure>
            <p>
              The solution to this problem is to secure sufficient server capacity for proper
              operation. However, resource acquisition in a shared, distributed environment is
              subject to supply and demand. As expected, development teams tend to be reluctant to
              go through the processes of acquiring resources when the resources must be
              contributed to a common pool and shared. To resolve this, a distinction between batch
              scheduling resources versus production priority resources has to be made to
              rationalize resource acquisition costs.
            </p>
            <section class="pagebreak-before" data-type="sect2" id=
            "monitoring-problems-in-periodic-pipelines-JMs7ixud">
              <h2 class="subheaders">
                Monitoring Problems in Periodic Pipelines
              </h2>
              <p>
                <a data-primary="data processing pipelines" data-secondary="monitoring problems"
                data-type="indexterm" id="PDPmonitor25"></a>For pipelines of sufficient execution
                duration, having real-time information on runtime performance metrics can be as
                important, if not even more important, than knowing overall metrics. This is
                because real-time data is important to providing operational support, including
                emergency response. In practice, the standard monitoring model involves collecting
                metrics during job execution, and reporting metrics only upon completion. If the
                job fails during execution, no statistics are provided.
              </p>
              <p>
                Continuous pipelines do not share these problems because their tasks are constantly
                running and their telemetry is routinely designed so that real-time metrics are
                available. Periodic pipelines shouldn’t have inherent monitoring problems, but we
                have observed a strong association.
              </p>
            </section>
            <section data-type="sect2" id="thundering-herd-problems-9ps7uyuL">
              <h2 class="subheaders">
                "Thundering Herd" Problems
              </h2>
              <p>
                <a data-primary="“thundering herd” problems" data-primary-sortas=
                "thundering herd problems" data-type="indexterm" id="id-7nCJSgFluEuM"></a>Adding to
                execution and monitoring challenges is the "thundering herd" problem endemic to
                distributed systems, also discussed in <a data-type="xref" href=
                "distributed-periodic-scheduling.html">Distributed Periodic
                Scheduling with Cron</a>. Given a large enough periodic pipeline, for each cycle,
                potentially thousands of workers immediately start work. If there are too many
                workers or if the workers are misconfigured or invoked by faulty retry logic, the
                servers on which they run will be overwhelmed, as will the underlying shared
                cluster services, and any networking infrastructure that was being used will also
                be overwhelmed.
              </p>
              <p>
                Further worsening this situation, if retry logic is not implemented, correctness
                problems can result when work is dropped upon failure, and the job won’t be
                retried. If retry logic is present but it is naive or poorly implemented, retry
                upon failure can compound the problem.
              </p>
              <p>
                Human intervention can also contribute to this scenario. Engineers with limited
                experience managing pipelines tend to amplify this problem by adding more workers
                to their pipeline when the job fails to complete within a desired period of time.
              </p>
              <p>
                Regardless of the source of the "thundering herd" problem, nothing is harder on
                cluster infrastructure and the SREs responsible for a cluster’s various services
                than a buggy 10,000 worker pipeline job.
              </p>
            </section>
            <section data-type="sect2" id="moire-load-pattern-ZKsaUPuQ">
              <h2 class="subheaders">
                Moiré Load Pattern
              </h2>
              <p>
                <a data-primary="Moiré load pattern in pipelines" data-type="indexterm" id=
                "id-EnCMSMFoUZuD"></a>Sometimes the thundering herd problem may not be obvious to
                spot in isolation. A related problem we call "Moiré load pattern" occurs when two
                or more pipelines run simultaneously and their execution sequences occasionally
                overlap, causing them to simultaneously consume a common shared resource. This
                problem can occur even in continuous pipelines, although it is less common when
                load arrives more evenly.
              </p>
              <p>
                Moiré load patterns are most apparent in plots of pipeline usage of shared
                resources. For example, <a data-type="xref" href=
                "data-processing-pipelines.html#fig_continuous-pipelines_moire-load-pattern">Figure 25-2</a> identifies the
                resource usage of three periodic pipelines. In <a data-type="xref" href=
                "data-processing-pipelines.html#fig_continuous-pipelines_moire-load-pattern-shared">Figure 25-3</a>, which is a
                stacked version of the data of the previous graph, the peak impact causing on-call
                pain occurs when the aggregate load nears 1.2M.<a data-primary="" data-startref=
                "PDPdraw25" data-type="indexterm" id="id-KnC9I1I2U9uY"></a><a data-primary=""
                data-startref="PDPmonitor25" data-type="indexterm" id="id-aeCOt8IvU7uB"></a>
              </p>
              <figure class="horizontal vertical" id="fig_continuous-pipelines_moire-load-pattern">
                <img alt="Moiré load pattern in separate infrastructure" src=
                "../images/srle-2502.jpg">
                <figcaption>
                  <span class="label">Figure 25-2.</span> Moiré load pattern in separate
                  infrastructure
                </figcaption>
              </figure>
              <figure class="horizontal vertical" id=
              "fig_continuous-pipelines_moire-load-pattern-shared">
                <img alt="Moiré load pattern in shared infrastructure" src=
                "../images/srle-2503.jpg">
                <figcaption>
                  <span class="label">Figure 25-3.</span> Moiré load pattern in shared
                  infrastructure
                </figcaption>
              </figure>
            </section>
          </section>
          <section data-type="sect1" id="introduction-to-google-workflow-WEsWUz">
            <h1 class="heading">
              Introduction to Google Workflow
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="Workflow system"
              data-type="indexterm" id="id-nxC1SLFmU9"></a><a data-primary="Google Workflow system"
              data-secondary="development of" data-type="indexterm" id="id-NnC7FnFQUk"></a>When an
              inherently one-shot batch pipeline is overwhelmed by business demands for
              continuously updated results, the pipeline development team usually considers either
              refactoring the original design to satisfy current demands, or moving to a continuous
              pipeline model. Unfortunately, business demands usually occur at the least convenient
              time to refactor the pipeline system into an online continuous processing system.
              Newer and larger customers who are faced with forcing scaling issues typically also
              want to include new features, and expect that these requirements adhere to immovable
              deadlines. In anticipating this challenge, it’s important to ascertain several
              details at the outset of designing a system involving a proposed data pipeline. Be
              sure to scope expected growth trajectory,<sup><a data-type="noteref" href=
              "data-processing-pipelines.html#id-rq7ueIYFgUv" id="id-rq7ueIYFgUv-marker">118</a></sup> demand for design
              modifications, expected additional resources, and expected latency requirements from
              the business.
            </p>
            <p>
              Faced with these needs, Google developed a system in 2003 called "Workflow" that
              makes continuous processing available at scale. Workflow uses the leader-follower
              (workers) distributed systems design pattern <a data-type="xref" href=
              "bibliography.html#Sha00" target="_blank">[Sha00]</a> and the
              system prevalence design pattern.<sup><a data-type="noteref" href="data-processing-pipelines.html#id-rq7upFgIgUv"
              id="id-rq7upFgIgUv-marker">119</a></sup> This combination enables very large-scale
              transactional data pipelines, ensuring correctness with exactly-once semantics.
            </p>
            <section data-type="sect2" id="workflow-as-model-view-controller-pattern-DVsqtwUy">
              <h2 class="subheaders">
                Workflow as Model-View-Controller Pattern
              </h2>
              <p>
                <a data-primary="model-view-controller pattern" data-type="indexterm" id=
                "id-mwCoSyFotaUZ"></a><a data-primary="Google Workflow system" data-secondary=
                "as model-view-controller pattern" data-type="indexterm" id=
                "id-dkCbFyFbtWUM"></a>Because of how system prevalence works, it can be useful to
                think of Workflow as the distributed systems equivalent of the
                model-view-controller pattern known from user interface
                development.<sup><a data-type="noteref" href="data-processing-pipelines.html#id-Pj0ukI7FAtMU2" id=
                "id-Pj0ukI7FAtMU2-marker">120</a></sup> As shown in <a data-type="xref" href=
                "data-processing-pipelines.html#fig_continuous-pipelines_model-view-controller-ui">Figure 25-4</a>, this design
                pattern divides a given software application into three interconnected parts to
                separate internal representations of information from the ways that information is
                presented to or accepted from the user.<sup><a data-type="noteref" href=
                "data-processing-pipelines.html#id-Xe4uXhOFytbUM" id="id-Xe4uXhOFytbUM-marker">121</a></sup>
              </p>
              <figure class="horizontal vertical" id=
              "fig_continuous-pipelines_model-view-controller-ui">
                <img alt="The model-view-controller pattern used in user interface design." src=
                "../images/srle-2504.jpg">
                <figcaption>
                  <span class="label">Figure 25-4.</span> The model-view-controller pattern used in
                  user interface design
                </figcaption>
              </figure>
              <p>
                Adapting this pattern for Workflow, the <em>model</em> is held in a server called
                "Task Master." The Task Master uses the system prevalence pattern to hold all job
                states in memory for fast availability while synchronously journaling mutations to
                persistent disk. The <em>view</em> is the workers that continually update the
                system state transactionally with the master according to their perspective as a
                subcomponent of the pipeline. Although all pipeline data may be stored in the Task
                Master, the best performance is usually achieved when only pointers to work are
                stored in the Task Master, and the actual input and output data is stored in a
                common filesystem or other storage. Supporting this analogy, the workers are
                completely stateless and can be discarded at any time. A <em>controller</em> can
                optionally be added as a third system component to efficiently support a number of
                auxiliary system activities that affect the pipeline, such as runtime scaling of
                the pipeline, snapshotting, workcycle state control, rolling back pipeline state,
                or even performing global interdiction for business continuity. <a data-type="xref"
                href="data-processing-pipelines.html#fig_continuous-pipelines_model-view-controller-workflow">Figure 25-5</a>
                illustrates the design pattern.
              </p>
              <figure class="horizontal vertical" id=
              "fig_continuous-pipelines_model-view-controller-workflow">
                <img alt="The Model-View-Controller design pattern as adapted for Google Workflow."
                src="../images/srle-2505.jpg">
                <figcaption>
                  <span class="label">Figure 25-5.</span> The model-view-controller design pattern
                  as adapted for Google Workflow
                </figcaption>
              </figure>
            </section>
          </section>
          <section data-type="sect1" id="stages-of-execution-in-workflow-eKsgC7">
            <h1 class="heading">
              Stages of Execution in Workflow
            </h1>
            <p>
              <a data-primary="Google Workflow system" data-secondary="stages of execution in"
              data-type="indexterm" id="id-NnCnSnF0Ck"></a>We can increase pipeline depth to any
              level inside Workflow by subdividing processing into task groups held in the Task
              Master. Each task group holds the work corresponding to a pipeline stage that can
              perform arbitrary operations on some piece of data. It’s relatively straightforward
              to perform mapping, shuffling, sorting, splitting, merging, or any other operation in
              any stage.
            </p>
            <p>
              A stage usually has some worker type associated with it. There can be multiple
              concurrent instances of a given worker type, and workers can be self-scheduled in the
              sense that they can look for different types of work and choose which type to
              <span class="keep-together">perform</span>.
            </p>
            <p>
              The worker consumes work units from a previous stage and produces output units. The
              output can be an end point or input for some other processing stage. Within the
              system, it’s easy to guarantee that all work is executed, or at least reflected in
              permanent state, exactly once.
            </p>
            <section data-type="sect2" id="workflow-correctness-guarantees-4ksrh1Co">
              <h2 class="subheaders">
                Workflow Correctness Guarantees
              </h2>
              <p>
                <a data-primary="data processing pipelines" data-secondary="Workflow system"
                data-type="indexterm" id="id-BnClSjF4hxC4"></a><a data-primary=
                "correctness guarantees" data-type="indexterm" id=
                "id-MnCbF0F0hYCZ"></a><a data-primary="Google Workflow system" data-secondary=
                "correctness guarantees" data-type="indexterm" id="id-7nCEIgFQhGCM"></a>It’s not
                practical to store <em>every</em> detail of the pipeline’s state inside the Task
                Master, because the Task Master is limited by RAM size. However, a double
                correctness guarantee persists because the master holds a collection of pointers to
                uniquely named data, and each work unit has a uniquely held lease. Workers acquire
                work with a lease and may only commit work from tasks for which they currently
                possess a valid lease.
              </p>
              <p>
                To avoid the situation in which an orphaned worker may continue working on a work
                unit, thus destroying the work of the current worker, each output file opened by a
                worker has a unique name. In this way, even orphaned workers can continue writing
                independently of the master until they attempt to commit. Upon attempting a commit,
                they will be unable to do so because another worker holds the lease for that work
                unit. Furthermore, orphaned workers cannot destroy the work produced by a valid
                worker, because the unique filename scheme ensures that every worker is writing to
                a distinct file. In this way, the double correctness guarantee holds: the output
                files are always unique, and the pipeline state is always correct by virtue of
                tasks with leases.
              </p>
              <p>
                As if a double correctness guarantee isn’t enough, Workflow also versions all
                tasks. If the task updates or the task lease changes, each operation yields a new
                unique task replacing the previous one, with a new ID assigned to the task. Because
                all pipeline configuration in Workflow is stored inside the Task Master in the same
                form as the work units themselves, in order to commit work, a worker must own an
                active lease <em>and</em> reference the task ID number of the configuration it used
                to produce its result. If the configuration changed while the work unit was in
                flight, all workers of that type will be unable to commit despite owning current
                leases. Thus, all work performed after a configuration change is consistent with
                the new configuration, at the cost of work being thrown away by workers unfortunate
                enough to hold the old leases.
              </p>
              <p>
                These measures provide a triple correctness guarantee: configuration, lease
                ownership, and filename uniqueness. However, even this isn’t sufficient for all
                cases.
              </p>
              <p>
                For example, what if the Task Master’s network address changed, and a different
                Task Master replaced it at the same address? What if a memory corruption altered
                the IP address or port number, resulting in another Task Master on the other end?
                Even more commonly, what if someone (mis)configured their Task Master setup by
                inserting a load balancer in front of a set of independent Task Masters?
              </p>
              <p>
                Workflow embeds a server token, a unique identifier for this particular Task
                Master, in each task’s metadata to prevent a rogue or incorrectly configured Task
                Master from corrupting the pipeline. Both client and server check the token on each
                operation, avoiding a very subtle misconfiguration in which all operations run
                smoothly until a task identifier collision occurs.
              </p>
              <p>
                To summarize, the four Workflow correctness guarantees are:
              </p>
              <ul>
                <li>Worker output through configuration tasks creates barriers on which to
                predicate work.
                </li>
                <li>All work committed requires a currently valid lease held by the worker.
                </li>
                <li>Output files are uniquely named by the workers.
                </li>
                <li>The client and server validate the Task Master itself by checking a server
                token on every operation.
                </li>
              </ul>
              <p>
                At this point, it may occur to you that it would be simpler to forgo the
                specialized Task Master and use Spanner <a data-type="xref" href=
                "bibliography.html#Cor12" target="_blank">[Cor12]</a> or another
                database. However, Workflow is special because each task is unique and immutable.
                These twin properties prevent many potentially subtle issues with wide-scale work
                distribution from occurring.
              </p>
              <p>
                For example, the lease obtained by the worker is part of the task itself, requiring
                a brand new task even for lease changes. If a database is used directly and its
                transaction logs act like a "journal," each and every read must be part of a
                long-running transaction. This configuration is most certainly possible, but
                terribly inefficient.
              </p>
            </section>
          </section>
          <section data-type="sect1" id="ensuring-business-continuity-G1sasm">
            <h1 class="heading">
              Ensuring Business Continuity
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="business continuity and"
              data-type="indexterm" id="id-8nCmSnFAsP"></a><a data-primary="business continuity"
              data-type="indexterm" id="id-mwCPFyF1s7"></a><a data-primary="Google Workflow system"
              data-secondary="business continuity and" data-type="indexterm" id=
              "id-dkCaIyFgsz"></a>Big Data pipelines need to continue processing despite failures
              of all types, including fiber cuts, weather events, and cascading power grid
              failures. These types of failures can disable entire datacenters. In addition,
              pipelines that do not employ system prevalence to obtain strong guarantees about job
              completion are often disabled and enter an undefined state. This architecture gap
              makes for a brittle business continuity strategy, and entails costly mass duplication
              of effort to restore pipelines and data.
            </p>
            <p>
              <a data-primary="Spanner" data-type="indexterm" id="id-mwCoSnI1s7"></a>Workflow
              resolves this problem conclusively for continuous processing pipelines. To obtain
              global consistency, the Task Master stores journals on Spanner, using it as a
              globally available, globally consistent, but low-throughput filesystem. To determine
              which Task Master can write, each Task Master uses the distributed lock service
              called Chubby <a data-type="xref" href="bibliography.html#Bur06"
              target="_blank">[Bur06]</a> to elect the writer, and the result is persisted in
              Spanner. Finally, clients look up the current Task Master using internal naming
              services.
            </p>
            <p>
              Because Spanner does not make for a high-throughput filesystem, globally distributed
              Workflows employ two or more local Workflows running in distinct clusters, in
              addition to a notion of reference tasks stored in the global Workflow. As units of
              work (tasks) are consumed through a pipeline, equivalent reference tasks are inserted
              into the global Workflow by the binary labeled "stage 1" in <a data-type="xref" href=
              "data-processing-pipelines.html#fig_continuous-pipelines_distributed-data-flow">Figure 25-6</a>. As tasks finish,
              the reference tasks are transactionally removed from the global Workflow as depicted
              in "stage n" of <a data-type="xref" href=
              "data-processing-pipelines.html#fig_continuous-pipelines_distributed-data-flow">Figure 25-6</a>. If the tasks
              cannot be removed from the global Workflow, the local Workflow will block until the
              global Workflow becomes available again, ensuring transactional correctness.
            </p>
            <p>
              To automate failover, a helper binary labeled "stage 1" in <a data-type="xref" href=
              "data-processing-pipelines.html#fig_continuous-pipelines_distributed-data-flow">Figure 25-6</a> runs inside each
              local Workflow. The local Workflow is otherwise unaltered, as described by the "do
              work" box in the diagram. This helper binary acts as a "controller" in the MVC sense,
              and is responsible for creating reference tasks, as well as updating a special
              heartbeat task inside of the global Workflow. If the heartbeat task is not updated
              within the timeout period, the remote Workflow’s helper binary seizes the work in
              progress as documented by the reference tasks and the pipeline continues, unhindered
              by whatever the environment may do to the work.
            </p>
            <figure class="horizontal vertical" id=
            "fig_continuous-pipelines_distributed-data-flow">
              <img alt="An example of distributed data and process flow using Workflow pipelines."
              src="../images/srle-2506.jpg">
              <figcaption>
                <span class="label">Figure 25-6.</span> An example of distributed data and process
                flow using Workflow pipelines
              </figcaption>
            </figure>
          </section>
          <section data-type="sect1" id="summary-and-concluding-remarks-DVsZHg">
            <h1 class="heading">
              Summary and Concluding Remarks
            </h1>
            <p>
              <a data-primary="data processing pipelines" data-secondary="overview of" data-type=
              "indexterm" id="id-mwCoSyFgH7"></a>Periodic pipelines are valuable. However, if a
              data processing problem is continuous or will organically grow to become continuous,
              don’t use a periodic pipeline. Instead, use a technology with characteristics similar
              to Workflow.
            </p>
            <p>
              We have found that continuous data processing with strong guarantees, as provided by
              Workflow, performs and scales well on distributed cluster infrastructure, routinely
              produces results that users can rely upon, and is a stable and reliable system for
              the Site Reliability Engineering team to manage and maintain.
            </p>
          </section>
          <div class="footnotes" data-type="footnotes">
            <p data-type="footnote" id="id-Pj0ubC7F7h4">
              <sup><a href="data-processing-pipelines.html#id-Pj0ubC7F7h4-marker">116</a></sup>Wikipedia: Extract, transform,
              load, <a href="http://en.wikipedia.org/wiki/Extract,_transform,_load" target=
              "_blank"><em class=
              "hyperlink">http://en.wikipedia.org/wiki/Extract,_transform,_load</em></a>
            </p>
            <p data-type="footnote" id="id-OdauPsmFvhQ">
              <sup><a href="data-processing-pipelines.html#id-OdauPsmFvhQ-marker">117</a></sup>Wikipedia: Big data, <a href=
              "http://en.wikipedia.org/wiki/Big_data" target="_blank"><em class=
              "hyperlink">http://en.wikipedia.org/wiki/Big_data</em></a>
            </p>
            <p data-type="footnote" id="id-rq7ueIYFgUv">
              <sup><a href="data-processing-pipelines.html#id-rq7ueIYFgUv-marker">118</a></sup>Jeff Dean’s lecture on "Software
              Engineering Advice from Building Large-Scale Distributed Systems" is an excellent
              resource: <a data-type="xref" href="bibliography.html#Dea07"
              target="_blank">[Dea07]</a>.
            </p>
            <p data-type="footnote" id="id-rq7upFgIgUv">
              <sup><a href="data-processing-pipelines.html#id-rq7upFgIgUv-marker">119</a></sup>Wikipedia: System Prevalence,
              <a href="http://en.wikipedia.org/wiki/System_Prevalence"><em class=
              "hyperlink">http://en.wikipedia.org/wiki/System_Prevalence</em></a>
            </p>
            <p data-type="footnote" id="id-Pj0ukI7FAtMU2">
              <sup><a href="data-processing-pipelines.html#id-Pj0ukI7FAtMU2-marker">120</a></sup>The "model-view-controller"
              pattern is an analogy for distributed systems that was very loosely borrowed from
              Smalltalk, which was originally used to describe the design structure of graphical
              user interfaces <a data-type="xref" href="bibliography.html#Fow08"
              target="_blank">[Fow08]</a>.
            </p>
            <p data-type="footnote" id="id-Xe4uXhOFytbUM">
              <sup><a href="data-processing-pipelines.html#id-Xe4uXhOFytbUM-marker">121</a></sup>Wikipedia:
              Model-view-controller, <a href=
              "http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller" target=
              "_blank"><em class=
              "hyperlink">http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller</em></a>
            </p>
          </div>
        </section>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="distributed-periodic-scheduling.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Chapter 24- Distributed Periodic Scheduling with Cron
          </p></a>
        </div>
        <div class="next">
          <a href="data-integrity.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 26- Data Integrity: What You Read Is What You Wrote
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
