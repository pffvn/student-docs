<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 33 - Lessons Learned from Other Industries
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <section data-type="chapter" id="chapter_other-reliability-efforts">
          <h1 class="heading">
            Lessons Learned from Other Industries
          </h1>
          <p class="byline author">
            Written by Jennifer Petoff<br>
            Edited by Betsy Beyer
          </p>
          <p>
            <a data-primary="cross-industry lessons" data-secondary=
            "comparative questions presented" data-type="indexterm" id="id-mwCoSQtBe"></a>A deep
            dive into SRE culture and practices at Google naturally leads to the question of how
            other industries manage their businesses for reliability. Compiling this book on Google
            SRE created an opportunity to speak to a number of Google's engineers about their
            previous work experiences in a variety of other high-reliability fields in order to
            address the following comparative questions:
          </p>
          <ul>
            <li>Are the principles used in Site Reliability Engineering also important outside of
            Google, or do other industries tackle the requirements of high reliability in markedly
            different ways?
            </li>
            <li>If other industries also adhere to SRE principles, how are the principles
            manifested?
            </li>
            <li>What are the similarities and differences in the implementation of these principles
            across industries?
            </li>
            <li>What factors drive similarities and differences in implementation?
            </li>
            <li>What can Google and the tech industry learn from these comparisons?
            </li>
          </ul>
          <p>
            <a data-primary="cross-industry lessons" data-secondary="key themes addressed"
            data-type="indexterm" id="id-BnClSmTo7"></a>A number of principles fundamental to Site
            Reliability Engineering at Google are discussed throughout this text. To simplify our
            comparison of best practices in other industries, we distilled these concepts into four
            key themes:
          </p>
          <ul>
            <li>Preparedness and Disaster Testing
            </li>
            <li>Postmortem Culture
            </li>
          </ul>
          <ul class="pagebreak-before">
            <li>Automation and Reduced Operational Overhead
            </li>
            <li>Structured and Rational Decision Making
            </li>
          </ul>
          <p>
            This chapter introduces the industries that we profiled and the industry veterans we
            interviewed. We define key SRE themes, discuss how these themes are implemented at
            Google, and give examples of how these principles reveal themselves in other industries
            for comparative purposes. We conclude with some insights and discussion on the patterns
            and anti-patterns we discovered.
          </p>
          <section data-type="sect1" id="meet-our-industry-veterans-yDsLUp1">
            <h1 class="heading">
              Meet Our Industry Veterans
            </h1>
            <p>
              <strong>Peter Dahl</strong> <a data-primary="cross-industry lessons" data-secondary=
              "industry leaders contributing" data-type="indexterm" id="id-KnCdFeF2UGA"></a>is a
              Principal Engineer at Google. Previously, he worked as a defense contractor on
              several high-reliability systems including many airborne and wheeled vehicle GPS and
              inertial guidance systems. Consequences of a lapse in reliability in such systems
              include vehicle malfunction or loss, and the financial consequences associated with
              that failure.
            </p>
            <p>
              <strong>Mike Doherty</strong> is a Site Reliability Engineer at Google. He worked as
              a lifeguard and lifeguard trainer for a decade in Canada. Reliability is absolutely
              essential by nature in this field, because lives are on the line every day.
            </p>
            <p>
              <strong>Erik Gross</strong> is currently a software engineer at Google. Before
              joining the company, he spent seven years designing algorithms and code for the
              lasers and systems used to perform refractive eye surgery (e.g., LASIK). This is a
              high-stakes, high-reliability field, in which many lessons relevant to reliability in
              the face of government regulations and human risk were learned as the technology
              received FDA approval, gradually improved, and finally became ubiquitous.
            </p>
            <p>
              <strong>Gus Hartmann</strong> and <strong>Kevin Greer</strong> have experience in the
              telecommunications industry, including maintaining the E911 emergency response
              system.<sup><a data-type="noteref" href="lessons-learned.html#id-YAJuxIxhdU8D" id=
              "id-YAJuxIxhdU8D-marker">154</a></sup> Kevin is currently a software engineer on the
              Google Chrome team and Gus is a systems engineer for Google's Corporate Engineering
              team. User expectations of the telecom industry demand high reliability. Implications
              of a lapse of service range from user inconvenience due to a system outage to
              fatalities if E911 goes down.
            </p>
            <p>
              <strong>Ron Heiby</strong> is a Technical Program Manager for Site Reliability
              Engineering at Google. Ron has experience in development for cell phones, medical
              devices, and the automotive industry. In some cases he worked on interface components
              of these industries (for example, on a device to allow EKG readings<sup><a data-type=
              "noteref" href="lessons-learned.html#id-YAJueFGTdU8D" id="id-YAJueFGTdU8D-marker">155</a></sup> in
              ambulances to be transmitted over the digital wireless phone network). In these
              industries, the impact of a reliability issue can range from harm to the business
              incurred by equipment recalls to indirectly impacting life and health (e.g., people
              not getting the medical attention they need if the EKG cannot communicate with the
              hospital).
            </p>
            <p>
              <strong>Adrian Hilton</strong> is a Launch Coordination Engineer at Google.
              Previously, he worked on UK and USA military aircraft, naval avionics and aircraft
              stores management systems, and UK railway signaling systems. Reliability is critical
              in this space because impact of incidents ranges from multimillion-dollar loss of
              equipment to injuries and fatalities.
            </p>
            <p>
              <strong>Eddie Kennedy</strong> is a project manager for the Global Customer
              Experience team at Google and a mechanical engineer by training. Eddie spent six
              years working as a Six Sigma Black Belt process engineer in a manufacturing facility
              that makes synthetic diamonds. This industry is characterized by a relentless focus
              on safety, because the extremes of temperature and pressure demands of the process
              pose a high level of danger to workers on a daily basis.
            </p>
            <p>
              <strong>John Li</strong> is currently a Site Reliability Engineer at Google. John
              previously worked as a systems administrator and software developer at a proprietary
              trading company in the finance industry. Reliability issues in the financial sector
              are taken quite seriously because they can lead to serious fiscal consequences.
            </p>
            <p>
              <strong>Dan Sheridan</strong> is a Site Reliability Engineer at Google. Before
              joining the company, he worked as a safety consultant in the civil nuclear industry
              in the UK. Reliability is important in the nuclear industry because an incident can
              have serious repercussions: outages can incur millions a day in lost revenue, while
              risks to workers and those in the community are even more dire, dictating zero
              tolerance for failure. Nuclear infrastructure is designed with a series of failsafes
              that halt operations before an incident of any such magnitude is reached.
            </p>
            <p>
              <strong>Jeff Stevenson</strong> is currently a hardware operations manager at Google.
              He has past experience as a nuclear engineer in the US Navy on a submarine.
              Reliability stakes in the nuclear Navy are high—problems that arise in the case of
              incidents range from damaged equipment, to long-standing environmental impact, to
              potential loss of life.
            </p>
            <p>
              <strong>Matthew Toia</strong> is a Site Reliability Manager focused on storage
              systems. Prior to Google, he worked on software development and deployment of air
              traffic control software systems. Effects from incidents in this industry range from
              inconveniences to passengers and airlines (e.g., delayed flights, diverted planes) to
              potential loss of life in the event of a crash. Defense in depth is a key strategy to
              avoiding catastrophic <span class="keep-together">failures</span>.
            </p>
            <p>
              Now that you've met our experts and gained a high-level understanding of why
              reliability is important in their respective former fields, we'll delve into the four
              key themes of reliability.
            </p>
          </section>
          <section data-type="sect1" id="preparedness-and-disaster-testing-V7s9Cro">
            <h1 class="heading">
              Preparedness and Disaster Testing
            </h1>
            <p>
              <a data-primary="cross-industry lessons" data-secondary=
              "preparedness and disaster testing" data-type="indexterm" id=
              "CILprepared33"></a><a data-primary="emergency preparedness" data-secondary=
              "cross-industry lessons" data-type="indexterm" id=
              "id-aeCdFEFnCAd"></a><a data-primary="disaster testing" data-type="indexterm" id=
              "dtest33"></a><a data-primary="disaster testing" data-secondary=
              "Disaster and Recovery Testing (DiRT)" data-type="indexterm" id=
              "id-LnCvtGFNCbv"></a>"Hope is not a strategy." This rallying cry of the SRE team at
              Google sums up what we mean by preparedness and disaster testing. The SRE culture is
              forever vigilant and constantly questioning: What could go wrong? What action can we
              take to address those issues before they lead to an outage or data loss? Our annual
              Disaster and Recovery Testing (DiRT) drills seek to address these questions head-on
              <a data-type="xref" href="bibliography.html#Kri12" target=
              "_blank">[Kri12]</a>. In DiRT exercises, SREs push production systems to the limit
              and inflict actual outages in order to:
            </p>
            <ul>
              <li>Ensure that systems react the way we think they will
              </li>
              <li>Determine unexpected weaknesses
              </li>
              <li>Figure out ways to make the systems more robust in order to prevent uncontrolled
              outages
              </li>
            </ul>
            <p>
              Several strategies for testing disaster readiness and ensuring preparedness in other
              industries emerged from our conversations. Strategies included the following:
            </p>
            <ul>
              <li>Relentless organizational focus on safety
              </li>
              <li>Attention to detail
              </li>
              <li>Swing capacity
              </li>
              <li>Simulations and live drills
              </li>
              <li>Training and certification
              </li>
              <li>Obsessive focus on detailed requirements gathering and design
              </li>
              <li>Defense in depth
              </li>
            </ul>
            <section data-type="sect2" id="relentless-organizational-focus-on-safety-OKsKTeCD2">
              <h2 class="subheaders">
                Relentless Organizational Focus on Safety
              </h2>
              <p>
                This principle is particularly important in an industrial engineering context.
                According to Eddie Kennedy, who worked on a manufacturing floor where workers faced
                safety hazards, "every management meeting started with a discussion of safety." The
                manufacturing industry prepares itself for the unexpected by establishing highly
                defined processes that are strictly followed at every level of the organization. It
                is critical that all employees take safety seriously, and that workers feel
                empowered to speak up if and when anything seems amiss. In the case of nuclear
                power, military aircraft, and railway signaling industries, safety standards for
                software are well detailed (e.g., UK Defence Standard 00-56, IEC 61508, IEC513, US
                DO-178B/C, and DO-254) and levels of reliability for such systems are clearly
                identified (e.g., Safety Integrity Level (SIL) 1–4),<sup><a data-type="noteref"
                href="lessons-learned.html#id-vkKuPSMFMTpC2Y" id="id-vkKuPSMFMTpC2Y-marker">156</a></sup> with the aim
                of specifying acceptable approaches to delivering a product.
              </p>
            </section>
            <section data-type="sect2" id="attention-to-detail-XQslcdCY8">
              <h2 class="subheaders">
                Attention to Detail
              </h2>
              <p>
                From his time spent in the US Navy, Jeff Stevenson recalls an acute awareness of
                how a lack of diligence in executing small tasks (for example, lube oil
                maintenance) could lead to major submarine failure. A very small oversight or
                mistake can have big effects. Systems are highly interconnected, so an accident in
                one area can impact multiple related components. The nuclear Navy focuses on
                routine maintenance to ensure that small issues don't snowball.
              </p>
            </section>
            <section data-type="sect2" id="swing-capacity-jWsgi9CrP">
              <h2 class="subheaders">
                Swing Capacity
              </h2>
              <p>
                System utilization in the telecom industry can be highly unpredictable. Absolute
                capacity can be strained by unforeseeable events such as natural disasters, as well
                as large, predictable events like the Olympics. According to Gus Hartmann, the
                industry deals with these incidents by deploying swing capacity in the form of a
                SOW (switch on wheels), a mobile telco office. This excess capacity can be rolled
                out in an emergency or in anticipation of a known event that is likely to overload
                the system. Capacity issues veer into the unexpected in matters unrelated to
                absolute capacity, as well. For example, when a celebrity's private phone number
                was leaked in 2005 and thousands of fans simultaneously attempted to call her, the
                telecom system exhibited symptoms similar to a DDoS or massive routing error.
              </p>
            </section>
            <section data-type="sect2" id="simulations-and-live-drills-2ksBuzCJb">
              <h2 class="subheaders">
                Simulations and Live Drills
              </h2>
              <p>
                Google's Disaster Recovery tests have a lot in common with the simulations and live
                drills that are a key focus of many of the established industries we researched.
                The potential consequences of a system outage determine whether using a simulation
                or a live drill is appropriate. For example, Matthew Toia points out that the
                aviation industry can't perform a live test "in production" without putting
                equipment and passengers at risk. Instead, they employ extremely realistic
                simulators with live data feeds, in which the control rooms and equipment are
                modeled down to the tiniest details to ensure a realistic experience without
                putting real people at risk. Gus Hartmann reports that the telecom industry
                typically focuses on live drills centered on surviving hurricanes and other weather
                emergencies. Such modeling led to the creation of weatherproof facilities with
                generators inside the building capable of outlasting a storm.
              </p>
              <p>
                The US nuclear Navy uses a mixture of "what if" thought exercises and live drills.
                According to Jeff Stevenson, the live drills involve "actually breaking real stuff
                but with control parameters. Live drills are carried out religiously, every week,
                two to three days per week."For the nuclear Navy, thought exercises are useful, but
                not sufficient to prepare for actual incidents. Responses must be practiced so they
                are not <span class="keep-together">forgotten</span>.
              </p>
              <p>
                According to Mike Doherty, lifeguards face disaster testing exercises more akin to
                a "mystery shopper" experience. Typically, a facility manager works with a child or
                an incognito lifeguard in training to stage a mock drowning. These scenarios are
                conducted to be as realistic as possible so that lifeguards aren't able to
                differentiate between real and staged emergencies.
              </p>
            </section>
            <section data-type="sect2" id="training-and-certification-pWsdUpCMr">
              <h2 class="subheaders">
                Training and Certification
              </h2>
              <p>
                Our interviews suggest that training and certification are particularly important
                when lives are at stake. For example, Mike Doherty described how lifeguards
                complete a rigorous training certification, in addition to a periodic
                recertification process. Courses include fitness components (e.g., a lifeguard must
                be able to hold someone heavier than themselves with shoulders out of the water),
                technical components like first aid and CPR, and operational elements (e.g., if a
                lifeguard enters the water, how do other team members respond?). Every facility
                also has site-specific training, because lifeguarding in a pool is markedly
                different from lifeguarding on a lakeside beach or on the ocean.
              </p>
            </section>
            <section data-type="sect2" id=
            "focus-on-detailed-requirements-gathering-and-design-o8sMCyC9e">
              <h2 class="subheaders">
                Focus on Detailed Requirements Gathering and Design
              </h2>
              <p>
                Some of the engineers we interviewed discussed the importance of detailed
                requirements gathering and design docs. This practice was particularly important
                when working with medical devices. In many of these cases, actual use or
                maintenance of the equipment doesn't fall within the purview of product designers.
                Thus, usage and maintenance requirements must be gathered from other sources.
              </p>
              <p>
                For example, according to Erik Gross, laser eye surgery machines are designed to be
                as foolproof as possible. Thus, soliciting requirements from the surgeons who
                actually use these machines and the technicians responsible for maintaining them is
                particularly important. In another example, former defense contractor Peter Dahl
                described a very detailed design culture in which creating a new defense system
                commonly entailed an entire year of design, followed by just three weeks of writing
                the code to actualize the design. Both of these examples are markedly different
                from Google's launch and iterate culture, which promotes a much faster rate of
                change at a calculated risk. Other industries (e.g., the medical industry and the
                military, as previously discussed) have very different pressures, risk appetites,
                and requirements, and their processes are very much informed by these
                circumstances.
              </p>
            </section>
            <section data-type="sect2" id="defense-in-depth-and-breadth-g0svsxCD1">
              <h2 class="subheaders">
                Defense in Depth and Breadth
              </h2>
              <p>
                In the nuclear power industry, defense in depth is a key element to preparedness
                <a data-type="xref" href="bibliography.html#IAEA12" target=
                "_blank">[IAEA12]</a>. Nuclear reactors feature redundancy on all systems and
                implement a design methodology that mandates fallback systems behind primary
                systems in case of failure. The system is designed with multiple layers of
                protection, including a final physical barrier to radioactive release around the
                plant itself. Defense in depth is particularly important in the nuclear industry
                due to the zero tolerance for failures and incidents.<a data-primary=""
                data-startref="CILprepared33" data-type="indexterm" id=
                "id-9nCdFvFys0CKn"></a><a data-primary="" data-startref="dtest33" data-type=
                "indexterm" id="id-ZbCnIMFZsZCGP"></a>
              </p>
            </section>
          </section>
          <section data-type="sect1" id="postmortem-culture-rWsbsYg">
            <h1 class="heading">
              Postmortem Culture
            </h1>
            <p>
              <a data-primary="cross-industry lessons" data-secondary="postmortem culture"
              data-type="indexterm" id="CILpost33"></a><a data-primary="postmortems"
              data-secondary="cross-industry lessons" data-type="indexterm" id=
              "PMcross33"></a><a data-primary="CAPA (corrective and preventative action)"
              data-type="indexterm" id="id-LnCNIGFbsbv"></a>Corrective and preventative action
              (CAPA)<sup><a data-type="noteref" href="lessons-learned.html#id-YAJuMtYFDs8D" id=
              "id-YAJuMtYFDs8D-marker">157</a></sup> is a well-known concept for improving
              reliability that focuses on the systematic investigation of root causes of identified
              issues or risks in order to prevent recurrence. This principle is embodied by SRE's
              strong culture of blameless postmortems. When something goes wrong (and given the
              scale, complexity, and rapid rate of change at Google, something inevitably
              <em>will</em> go wrong), it's important to evaluate all of the following:
            </p>
            <ul>
              <li>What happened
              </li>
              <li>The effectiveness of the response
              </li>
              <li>What we would do differently next time
              </li>
              <li>What actions will be taken to make sure a particular incident doesn't happen
              again
              </li>
            </ul>
            <p>
              This exercise is undertaken without pointing fingers at any individual. Instead of
              assigning blame, it is far more important to figure out <em>what</em> went wrong, and
              how, as an organization, we will rally to ensure it doesn't happen again. Dwelling on
              <em>who</em> might have caused the outage is counterproductive. Postmortems are
              conducted after incidents and published across SRE teams so that all can benefit from
              the lessons learned.
            </p>
            <p>
              Our interviews uncovered that many industries perform a version of the postmortem
              (although many do not use this specific moniker, for obvious reasons). The
              <em>motivation</em> behind these exercises appears to be the main differentiator
              among industry practices.
            </p>
            <p>
              Many industries are heavily regulated and are held accountable by specific government
              authorities when something goes wrong. Such regulation is especially ingrained when
              the stakes of failure are high (e.g., lives are at stake). Relevant government
              agencies include the FCC (telecommunications), FAA (aviation), OSHA (the
              manufacturing and chemical industries), FDA (medical devices), and the various
              National Competent Authorities in the EU.<sup><a data-type="noteref" href=
              "lessons-learned.html#id-vkKuPSzTAsXz" id="id-vkKuPSzTAsXz-marker">158</a></sup> The nuclear power and
              transportation industries are also heavily regulated.
            </p>
            <p>
              Safety considerations are another motivating factor behind postmortems. In the
              manufacturing and chemical industries, the risk of injury or death is ever-present
              due to the nature of the conditions required to produce the final product (high
              temperature, pressure, toxicity, and corrosivity, to name a few). For example, Alcoa
              features a noteworthy safety culture. Former CEO Paul O'Neill required staff to
              notify him within 24 hours of any injury that lost a worker day. He even distributed
              his home phone number to workers on the factory floor so that they could personally
              alert him to safety concerns.<sup><a data-type="noteref" href="lessons-learned.html#id-lM2uPSAcLsPk" id=
              "id-lM2uPSAcLsPk-marker">159</a></sup>
            </p>
            <p>
              The stakes are so high in the manufacturing and chemical industries that even "near
              misses"—when a given event could have caused serious harm, but did not—are carefully
              scrutinized. These scenarios function as a type of preemptive postmortem. According
              to VM Brasseur in a talk given at YAPC NA 2015, "There are multiple near misses in
              just about every disaster and business crisis, and typically they're ignored at the
              time they occur. Latent error, plus an enabling condition, equals things not working
              quite the way you planned" <a data-type="xref" href=
              "bibliography.html#Bra15" target="_blank">[Bra15]</a>. Near misses
              are effectively disasters waiting to happen. For example, scenarios in which a worker
              doesn't follow the standard operating procedure, an employee jumps out of the way at
              the last second to avoid a splash, or a spill on the staircase isn't cleaned up, all
              represent near misses and opportunities to learn and improve. Next time, the employee
              and the company might not be so lucky. The United Kingdom's CHIRP (Confidential
              Reporting Programme for Aviation and Maritime) seeks to raise awareness about such
              incidents across the industry by providing a central reporting point where aviation
              and maritime personnel can report near misses confidentially. Reports and analyses of
              these near misses are then published in periodic newsletters.
            </p>
            <p>
              Lifeguarding has a deeply embedded culture of post-incident analysis and action
              planning. Mike Doherty quips, "If a lifeguard's feet go in the water, there will be
              paperwork!" A detailed write-up is required after any incident at the pool or on the
              beach. In the case of serious incidents, the team collectively examines the incident
              end to end, discussing what went right and what went wrong. Operational changes are
              then made based on these findings, and training is often scheduled to help people
              build confidence around their ability to handle a similar incident in the future. In
              cases of particularly shocking or traumatic incidents, a counselor is brought on site
              to help staff cope with the psychological aftermath. The lifeguards may have been
              well prepared for what happened in practice, but might <em>feel</em> like they
              haven't done an adequate job. Similar to Google, lifeguarding embraces a culture of
              blameless incident analysis. Incidents are chaotic, and many factors contribute to
              any given incident. In this field, it's not helpful to place blame on a single
              individual.<a data-primary="" data-startref="CILpost33" data-type="indexterm" id=
              "id-kVCEFmuvsbk"></a><a data-primary="" data-startref="PMcross33" data-type=
              "indexterm" id="id-4nCkIVuPs8d"></a>
            </p>
          </section>
          <section data-type="sect1" id=
          "automating-away-repetitive-work-and-operational-overhead-wPsbHPD">
            <h1 class="heading">
              Automating Away Repetitive Work and Operational Overhead
            </h1>
            <p>
              <a data-primary="cross-industry lessons" data-secondary=
              "repetitive work/operational overhead" data-type="indexterm" id=
              "id-0nCXSGFDH10"></a><a data-primary="toil" data-secondary="cross-industry lessons"
              data-type="indexterm" id="id-LnCbFGFaHbv"></a><a data-primary="automation"
              data-secondary="cross-industry lessons" data-type="indexterm" id=
              "id-AnCKIZFwHeG"></a><a data-primary="operational load" data-secondary=
              "cross-industry lessons" data-type="indexterm" id="id-W7CetLFqHWv"></a>At their core,
              Google's Site Reliability Engineers are software engineers with a low tolerance for
              repetitive reactive work. It is strongly ingrained in our culture to avoid repeating
              an operation that doesn't add value to a service. If a task can be automated away,
              why would you run a system on repetitive work that is of low value? Automation lowers
              operational overhead and frees up time for our engineers to proactively assess and
              improve the services they support.
            </p>
            <p>
              The industries that we surveyed were mixed in terms of if, how, and why they embraced
              automation. Certain industries trusted humans more than machines. During the tenure
              of our industry veteran, the US nuclear Navy eschewed automation in favor of a series
              of interlocks and administrative procedures. For example, according to Jeff
              Stevenson, operating a valve required an operator, a supervisor, and a crew member on
              the phone with the engineering watch officer tasked with monitoring the response to
              the action taken. These operations were very manual due to concern that an automated
              system might not spot a problem that a human would definitely notice. Operations on a
              submarine are ruled by a trusted human decision chain—a <em>series</em> of people,
              rather than one individual. The nuclear Navy was also concerned that automation and
              computers move so rapidly that they are all too capable of committing a large,
              irreparable mistake. When you are dealing with nuclear reactors, a slow and steady
              methodical approach is more important than accomplishing a task quickly.
            </p>
            <p id="xref_other-reliability-efforts_knight-capital">
              According to John Li, the proprietary trading industry has become increasingly
              cautious in its application of automation in recent years. Experience has shown that
              incorrectly configured automation can inflict significant damage and incur a great
              deal of financial loss in a very short period of time. For example, in 2012 Knight
              Capital Group encountered a "software glitch" that led to a loss of $440M in just a
              few hours.<sup><a data-type="noteref" href="lessons-learned.html#id-YAJuDS1tmH8D" id=
              "id-YAJuDS1tmH8D-marker">160</a></sup> Similarly, in 2010 the US stock market
              experienced a Flash Crash that was ultimately blamed on a rogue trader attempting to
              manipulate the market with automated means. While the market was quick to recover,
              the Flash Crash resulted in a loss on the magnitude of trillions of dollars in just
              <em>30 minutes</em>.<sup><a data-type="noteref" href="lessons-learned.html#id-lM2uAIztJHPk" id=
              "id-lM2uAIztJHPk-marker">161</a></sup> Computers can execute tasks very quickly, and
              speed can be a negative if these tasks are configured incorrectly.
            </p>
            <p>
              In contrast, some companies embrace automation precisely <em>because</em> computers
              act more quickly than people. According to Eddie Kennedy, efficiency and monetary
              savings are key in the manufacturing industry, and automation provides a means to
              accomplish tasks more efficiently and cost-effectively. Furthermore, automation is
              generally more reliable and repeatable than work conducted manually by humans, which
              means that it produces higher-quality standards and tighter tolerances. Dan Sheridan
              discussed automation as deployed in the UK nuclear industry. Here, a rule of thumb
              dictates that if a plant is required to respond to a given situation in less than 30
              minutes, that response must be automated.
            </p>
            <p>
              In Matt Toia's experience, the aviation industry applies automation selectively. For
              example, operational failover is performed automatically, but when it comes to
              certain other tasks, the industry trusts automation only when it's verified by a
              human. While the industry employs a good deal of automatic monitoring, actual
              air-traffic–control-system implementations must be manually inspected by humans.
            </p>
            <p>
              According to Erik Gross, automation has been quite effective in reducing user error
              in laser eye surgery. Before LASIK surgery is performed, the doctor measures the
              patient using a refractive eye test. Originally, the doctor would type in the numbers
              and press a button, and the laser would go to work correcting the patient's vision.
              However, data entry errors could be a big issue. This process also entailed the
              possibility of mixing up patient data or jumbling numbers for the left and right eye.
            </p>
            <p>
              Automation now greatly lessens the chance that humans make a mistake that impacts
              someone's vision. A computerized sanity check of manually entered data was the first
              major automated improvement: if a human operator inputs measurements outside an
              expected range, automation promptly and prominently flags this case as unusual. Other
              automated improvements followed this development: now the iris is photographed during
              the preliminary refractive eye test. When it's time to perform the surgery, the iris
              of the patient is automatically matched to the iris in the photo, thus eliminating
              the possibility of mixing up patient data. When this automated solution was
              implemented, an entire class of medical errors disappeared.
            </p>
          </section>
          <section data-type="sect1" id="structured-and-rational-decision-making-xqsKfBq">
            <h1 class="heading">
              Structured and Rational Decision Making
            </h1>
            <p>
              <a data-primary="cross-industry lessons" data-secondary="decision-making skills"
              data-type="indexterm" id="CILdecision33"></a><a data-primary="decision-making skills"
              data-type="indexterm" id="decision33"></a>At Google in general, and in Site
              Reliability Engineering in particular, data is critical. The team aspires to make
              decisions in a structured and rational way by ensuring that:
            </p>
            <ul>
              <li>The basis for the decision is agreed upon advance, rather than justified ex post
              facto
              </li>
              <li>The inputs to the decision are clear
              </li>
              <li>Any assumptions are explicitly stated
              </li>
              <li>Data-driven decisions win over decisions based on feelings, hunches, or the
              opinion of the most senior employee in the room
              </li>
            </ul>
            <p>
              Google SRE operates under the baseline assumption that everyone on the team:
            </p>
            <ul>
              <li>Has the best interests of a service's users at heart
              </li>
              <li>Can figure out how to proceed based on the data available
              </li>
            </ul>
            <p>
              Decisions should be informed rather than prescriptive, and are made without deference
              to personal opinions—even that of the most-senior person in the room, who Eric
              Schmidt and Jonathan Rosenberg dub the "HiPPO," for "Highest-Paid Person's Opinion"
              <a data-type="xref" href="bibliography.html#Sch14" target=
              "_blank">[Sch14]</a>.
            </p>
            <p>
              Decision making in different industries varies widely. We learned that some
              industries use an approach of <em>if it ain't broke, don't fix it…ever</em>.
              Industries featuring systems whose design entailed much thought and effort are often
              characterized by a reluctance to change the underlying technology. For example, the
              telecom industry still uses long-distance switches that were implemented in the
              1980s. Why do they rely on technology developed a few decades ago? These switches
              "are pretty much bulletproof and massively redundant," according to Gus Hartmann. As
              reported by Dan Sheridan, the nuclear industry is similarly slow to change. All
              decisions are underpinned by the thought: <em>if it works now, don't change it</em>.
            </p>
            <p>
              Many industries heavily focus on playbooks and procedures rather than open-ended
              problem solving. Every humanly conceivable scenario is captured in a checklist or in
              "the binder." When something goes wrong, this resource is the authoritative source
              for how to react. This prescriptive approach works for industries that evolve and
              develop relatively slowly, because the scenarios of what could go wrong are not
              constantly evolving due to system updates or changes. This approach is also common in
              industries in which the skill level of the workers may be limited, and the best way
              to make sure that people will respond appropriately in an emergency is to provide a
              simple, clear set of instructions.
            </p>
            <p>
              Other industries also take a clear, data-driven approach to decision making. In Eddie
              Kennedy's experience, research and manufacturing environments are characterized by a
              rigorous experimentation culture that relies heavily on formulating and testing
              hypotheses. These industries regularly conduct controlled experiments to make sure
              that a given change yields the expected result at a statistically significant level
              and that nothing unexpected occurs. Changes are only implemented when data yielded by
              the experiment supports the decision.
            </p>
            <p>
              Finally, some industries, like proprietary trading, divide decision making to better
              manage risk. According to John Li, this industry features an enforcement team
              separate from the traders to ensure that undue risks aren't taken in pursuit of
              achieving a profit. The enforcement team is responsible for monitoring events on the
              floor and halting trading if events spin out of hand. If a system abnormality occurs,
              the enforcement team's first response is to shut down the system. As put by John Li,
              "If we aren't trading, we aren't losing money. We aren't making money either, but at
              least we aren't losing money." Only the enforcement team can bring the system back
              up, despite how excruciating a delay might seem to traders who are missing a
              potentially profitable opportunity.<a data-primary="" data-startref="CILdecision33"
              data-type="indexterm" id="id-JnCDSMUZfko"></a>
            </p>
          </section>
          <section data-type="sect1" id="conclusions-PasOS7A">
            <h1 class="heading">
              Conclusions
            </h1>
            <p>
              <a data-primary="cross-industry lessons" data-secondary="Google's application of"
              data-type="indexterm" id="id-AnCPSZFeSeG"></a>Many of the principles that are core to
              Site Reliability Engineering at Google are evident across a wide range of industries.
              The lessons already learned by well-established industries likely inspired some of
              the practices in use at Google today.
            </p>
            <p>
              A main takeaway of our cross-industry survey was that in many parts of its software
              business, Google has a higher appetite for velocity than players in most other
              industries. The ability to move or change quickly must be weighed against the
              differing implications of a failure. In the nuclear, aviation, or medical industries,
              for example, people could be injured or even die in the event of an outage or
              failure. When the stakes are high, a conservative approach to achieving high
              reliability is warranted.
            </p>
            <p>
              At Google, we constantly walk a tightrope between user expectations for high
              reliability versus a laser-sharp focus on rapid change and innovation. While Google
              is incredibly serious about reliability, we must adapt our approaches to our high
              rate of change. As discussed in earlier chapters, many of our software businesses
              such as Search make conscious decisions as to how reliable "reliable enough" really
              is.
            </p>
            <p class="pagebreak-before">
              Google has that flexibility in most of our software products and services, which
              operate in an environment in which lives are not directly at risk if something goes
              wrong. Therefore, we're able to use tools such as error budgets (<a data-type="xref"
              href=
              "embracing-risk.html#xref_risk-management_unreliability-budgets">Motivation
              for Error Budgets</a>) as a means to "fund" a culture of innovation and calculated
              risk taking. In essence, Google has adapted known reliability principles that were in
              many cases developed and honed in other industries to create its own unique
              reliability culture, one that addresses a complicated equation that balances scale,
              complexity, and velocity with high reliability.
            </p>
          </section>
          <div class="footnotes" data-type="footnotes">
            <p data-type="footnote" id="id-YAJuxIxhdU8D">
              <sup><a href="lessons-learned.html#id-YAJuxIxhdU8D-marker">154</a></sup>E911 (Enhanced 911): Emergency
              response line in the US that leverages location data.
            </p>
            <p data-type="footnote" id="id-YAJueFGTdU8D">
              <sup><a href="lessons-learned.html#id-YAJueFGTdU8D-marker">155</a></sup>Electrocardiogram readings:
              <a href="https://en.wikipedia.org/wiki/Electrocardiography" target=
              "_blank"><em class="hyperlink">https://en.wikipedia.org/wiki/Electrocardiography</em></a>.
            </p>
            <p data-type="footnote" id="id-vkKuPSMFMTpC2Y">
              <sup><a href="lessons-learned.html#id-vkKuPSMFMTpC2Y-marker">156</a></sup><a href=
              "https://en.wikipedia.org/wiki/Safety_integrity_level" target="_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Safety_integrity_level</em></a>
            </p>
            <p data-type="footnote" id="id-YAJuMtYFDs8D">
              <sup><a href="lessons-learned.html#id-YAJuMtYFDs8D-marker">157</a></sup><a href=
              "https://en.wikipedia.org/wiki/Corrective_and_preventive_action" target=
              "_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Corrective_and_preventive_action</em></a>
            </p>
            <p data-type="footnote" id="id-vkKuPSzTAsXz">
              <sup><a href="lessons-learned.html#id-vkKuPSzTAsXz-marker">158</a></sup><a href=
              "https://en.wikipedia.org/wiki/Competent_authority" target="_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Competent_authority</em></a>
            </p>
            <p data-type="footnote" id="id-lM2uPSAcLsPk">
              <sup><a href="lessons-learned.html#id-lM2uPSAcLsPk-marker">159</a></sup><a href=
              "http://ehstoday.com/safety/nsc-2013-oneill-exemplifies-safety-leadership" target=
              "_blank"><em class=
              "hyperlink">http://ehstoday.com/safety/nsc-2013-oneill-exemplifies-safety-leadership</em></a>.
            </p>
            <p data-type="footnote" id="id-YAJuDS1tmH8D">
              <sup><a href="lessons-learned.html#id-YAJuDS1tmH8D-marker">160</a></sup>See "FACTS, Section B" for the
              discussion of Knight and Power Peg software in <a data-type="xref" href=
              "bibliography.html#Sec13" target="_blank">[Sec13]</a>.
            </p>
            <p data-type="footnote" id="id-lM2uAIztJHPk">
              <sup><a href="lessons-learned.html#id-lM2uAIztJHPk-marker">161</a></sup>"Regulators blame computer
              algorithm for stock market 'flash crash'," Computerworld, <a href=
              "http://www.computerworld.com/article/2516076/financial-it/regulators-blame-computer-algorithm-for-stock-market%E2%80%94flash-crash-.html"
              target="_blank"><em class=
              "hyperlink">http://www.computerworld.com/article/2516076/financial-it/regulators-blame-computer-algorithm-for-stock-market—flash-crash-.html</em></a>.
            </p>
          </div>
        </section>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="part5.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Part V - Conclusions
          </p></a>
        </div>
        <div class="next">
          <a href="conclusion.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 34- Conclusion
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
