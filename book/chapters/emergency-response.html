<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 13 - Emergency Response
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <h1 class="heading">
          Emergency Response
        </h1>
        <p class="byline author">
          Written by Corey Adam Baye<br>
          Edited by Diane Bates
        </p>
        <p>
          Things break; that’s life.
        </p>
        <p>
          Regardless of the stakes involved or the size of an organization, one trait that’s vital
          to the long-term health of an organization, and that consequently sets that organization
          apart from others, is how the people involved respond to an emergency. Few of us
          naturally respond well during an emergency. A proper response takes preparation and
          periodic, pertinent, hands-on training. Establishing and maintaining thorough training
          and testing processes requires the support of the board and management, in addition to
          the careful attention of staff. All of these elements are essential in fostering an
          environment in which teams can spend money, time, energy, and possibly even uptime to
          ensure that systems, processes, and people respond efficiently during an emergency.
        </p>
        <p>
          Note that the chapter on postmortem culture discusses the specifics of how to write
          postmortems in order to make sure that incidents that require emergency response also
          become a learning opportunity (see <a href=
          "postmortem-culture.html">Postmortem Culture: Learning from
          Failure</a>). This chapter provides more concrete examples of such incidents.
        </p>
        <h1 class="heading">
          What to Do When Systems Break
        </h1>
        <p>
          First of all, don’t panic! You aren’t alone, and the sky isn’t falling. You’re a
          professional and trained to handle this sort of situation. Typically, no one is in
          physical danger—only those poor electrons are in peril. At the very worst, half of the
          Internet is down. So take a deep breath…and carry on.
        </p>
        <p>
          If you feel overwhelmed, pull in more people. Sometimes it may even be necessary to page
          the entire company. If your company has an incident response process (see <a href=
          "managing-incidents.html">Managing Incidents</a>), make sure that
          you’re familiar with it and follow that process.
        </p>
        <h1 class="heading">
          Test-Induced Emergency
        </h1>
        <p>
          Google has adopted a proactive approach to disaster and emergency testing (see <a href=
          "bibliography.html#Kri12" target="_blank">[Kri12]</a>). SREs break our
          systems, watch how they fail, and make changes to improve reliability and prevent the
          failures from recurring. Most of the time, these controlled failures go as planned, and
          the target system and dependent systems behave in roughly the manner we expect. We
          identify some weaknesses or hidden dependencies and document follow-up actions to rectify
          the flaws we uncover. However, sometimes our assumptions and the actual results are
          worlds apart.
        </p>
        <p>
          Here’s one example of a test that unearthed a number of unexpected dependencies.
        </p>
        <h2 class="subheaders">
          Details
        </h2>
        <p>
          We wanted to flush out hidden dependencies on a test database within one of our larger
          distributed MySQL databases. The plan was to block all access to just one database out of
          a hundred. No one foresaw the results that would unfold.
        </p>
        <h2 class="subheaders">
          Response
        </h2>
        <p>
          Within minutes of commencing the test, numerous dependent services reported that both
          external and internal users were unable to access key systems. Some systems were
          intermittently or only partially accessible.
        </p>
        <p>
          Assuming that the test was responsible, SRE immediately aborted the exercise. We
          attempted to roll back the permissions change, but were unsuccessful. Instead of
          panicking, we immediately brainstormed how to restore proper access. Using an already
          tested approach, we restored permissions to the replicas and failovers. In a parallel
          effort, we reached out to key developers to correct the flaw in the database application
          layer library.
        </p>
        <p>
          Within an hour of the original decision, all access was fully restored, and all services
          were able to connect once again. The broad impact of this test motivated a rapid and
          thorough fix to the libraries and a plan for periodic retesting to prevent such a major
          flaw from recurring.
        </p>
        <h2 class="subheaders">
          Findings
        </h2>
        <h3 class="subheaders">
          What went well
        </h3>
        <p>
          Dependent services that were affected by the incident immediately escalated the issues
          within the company. We assumed, correctly, that our controlled experiment had gotten out
          of hand and immediately aborted the test.
        </p>
        <p>
          We were able to fully restore permissions within an hour of the first report, at which
          time systems started behaving properly. Some teams took a different approach and
          reconfigured their systems to avoid the test database. These parallel efforts helped to
          restore service as quickly as possible.
        </p>
        <p>
          Follow-up action items were resolved quickly and thoroughly to avoid a similar outage,
          and we instituted periodic testing to ensure that similar flaws do not recur.
        </p>
        <h3 class="subheaders">
          What we learned
        </h3>
        <p>
          Although this test was thoroughly reviewed and thought to be well scoped, reality
          revealed we had an insufficient understanding of this particular interaction among the
          dependent systems.
        </p>
        <p>
          We failed to follow the incident response process, which had been put in place only a few
          weeks before and hadn’t been thoroughly disseminated. This process would have ensured
          that all services and customers were aware of the outage. To avoid similar scenarios in
          the future, SRE continually refines and tests our incident response tools and processes,
          in addition to making sure that updates to our incident management procedures are clearly
          communicated to all relevant parties.
        </p>
        <p>
          Because we hadn’t tested our rollback procedures in a test environment, these procedures
          were flawed, which lengthened the outage. We now require thorough testing of rollback
          procedures before such large-scale tests.
        </p>
        <h1 class="heading">
          Change-Induced Emergency
        </h1>
        <p>
          As you can imagine, Google has a lot of configuration—complex configuration—and we
          constantly make changes to that configuration. To prevent breaking our systems outright,
          we perform numerous tests on configuration changes to make sure they don’t result in
          unexpected and undesired behavior. However, the scale and complexity of Google’s
          infrastructure make it impossible to anticipate every dependency or interaction;
          sometimes configuration changes don’t go entirely according to plan.
        </p>
        <p>
          The following is one such example.
        </p>
        <h2 class="subheaders">
          Details
        </h2>
        <p>
          A configuration change to the infrastructure that helps protect our services from abuse
          was pushed globally on a Friday. This infrastructure interacts with essentially all of
          our externally facing systems, and the change triggered a crash-loop bug in those
          systems, which caused the entire fleet to begin to crash-loop almost simultaneously.
          Because Google’s internal infrastructure also depends upon our own services, many
          internal applications suddenly became unavailable as well.
        </p>
        <h2 class="subheaders">
          Response
        </h2>
        <p>
          Within seconds, monitoring alerts started firing, indicating that certain sites were
          down. Some on-call engineers simultaneously experienced what they believed to be a
          failure of the corporate network and relocated to dedicated secure rooms (panic rooms)
          with backup access to the production environment. They were joined by additional
          engineers who were struggling with their corporate access.
        </p>
        <p>
          Within five minutes of that first configuration push, the engineer responsible for the
          push, having become aware of the corporate outage but still unaware of the broader
          outage, pushed another configuration change to roll back the first change. At this point,
          services began to recover.
        </p>
        <p>
          Within 10 minutes of the first push, on-call engineers declared an incident and proceeded
          to follow internal procedures for incident response. They began notifying the rest of the
          company about the situation. The push engineer informed the on-call engineers that the
          outage was likely due to the change that had been pushed and later <span>rolled</span>
          back. Nevertheless, some services experienced unrelated bugs or misconfigurations
          triggered by the original event and didn’t fully recover for up to an hour.
        </p>
        <h2 class="subheaders">
          Findings
        </h2>
        <h3 class="subheaders">
          What went well
        </h3>
        <p>
          There were several factors at play that prevented this incident from resulting in a
          longer-term outage of many of Google’s internal systems.
        </p>
        <p>
          To begin with, monitoring almost immediately detected and alerted us to the problem.
          However, it should be noted that in this case, our monitoring was less than ideal: alerts
          fired repeatedly and constantly, overwhelming the on-calls and spamming regular and
          emergency communication channels.
        </p>
        <p>
          Once the problem was detected, incident management generally went well and updates were
          communicated often and clearly. Our out-of-band communications systems kept everyone
          connected even while some of the more complicated software stacks were unusable. This
          experience reminded us why SRE retains highly reliable, low overhead backup systems,
          which we use regularly.
        </p>
        <p>
          In addition to these out-of-band communications systems, Google has command-line tools
          and alternative access methods that enable us to perform updates and roll back changes
          even when other interfaces are inaccessible. These tools and access methods worked well
          during the outage, with the caveat that engineers needed to be more familiar with the
          tools and to test them more routinely.
        </p>
        <p>
          Google’s infrastructure provided yet another layer of protection in that the affected
          system rate-limited how quickly it provided full updates to new clients. This behavior
          may have throttled the crash-loop and prevented a complete outage, allowing jobs to
          remain up long enough to service a few requests in between crashes.
        </p>
        <p>
          Finally, we should not overlook the element of luck in the quick resolution of this
          incident: the push engineer happened to be following real-time communication channels—an
          additional level of diligence that’s not a normal part of the release process. The push
          engineer noticed a large number of complaints about corporate access directly following
          the push and rolled back the change almost immediately. Had this swift rollback not
          occurred, the outage could have lasted considerably longer, becoming immensely more
          difficult to troubleshoot.
        </p>
        <h3 class="subheaders">
          What we learned
        </h3>
        <p>
          An earlier push of the new feature had involved a thorough canary but didn’t trigger the
          same bug, as it had not exercised a very rare and specific configuration keyword in
          combination with the new feature. The specific change that triggered this bug wasn’t
          considered risky, and therefore followed a less stringent canary process. When the change
          was pushed globally, it used the untested keyword/feature combination that triggered the
          failure.
        </p>
        <p>
          Ironically, improvements to canarying and automation were slated to become higher
          priority in the following quarter. This incident immediately raised their priority and
          reinforced the need for thorough canarying, regardless of the perceived risk.
        </p>
        <p>
          As one would expect, alerting was vocal during this incident because every location was
          essentially offline for a few minutes. This disrupted the real work being performed by
          the on-call engineers and made communication among those involved in the incident more
          difficult.
        </p>
        <p>
          Google relies upon our own tools. Much of the software stack that we use for
          troubleshooting and communicating lies behind jobs that were crash-looping. Had this
          outage lasted any longer, debugging would have been severely hindered.
        </p>
        <h1 class="heading">
          Process-Induced Emergency
        </h1>
        <p>
          We have poured a considerable amount of time and energy into the automation that manages
          our machine fleet. It’s amazing how many jobs one can start, stop, or retool across the
          fleet with very little effort. Sometimes, the efficiency of our automation can be a bit
          frightening when things do not go quite according to plan.
        </p>
        <p>
          This is one example where moving fast was not such a good thing.
        </p>
        <h2 class="subheaders">
          Details
        </h2>
        <p>
          As part of routine automation testing, two consecutive turndown requests for the same
          soon-to-be-decommissioned server installation were submitted. In the case of the second
          turndown request, a subtle bug in the automation sent all of the machines in all of these
          installations globally to the Diskerase queue, where their hard drives were destined to
          be wiped; see <a href=
          "automation-at-google.html#xref_automation_diskerase-sidebar">Automation:
          Enabling Failure at Scale</a> for more details.
        </p>
        <h2 class="subheaders">
          Response
        </h2>
        <p>
          Soon after the second turndown request was issued, the on-call engineers received a page
          as the first small server installation was taken offline to be decommissioned. Their
          investigation determined that the machines had been transferred to the Diskerase queue,
          so following normal procedure, the on-call engineers drained traffic from the location.
          Because the machines in that location had been wiped, they were unable to respond to
          requests. To avoid failing those requests outright, on-call engineers drained traffic
          away from that location. Traffic was redirected to locations that could properly respond
          to the requests.
        </p>
        <p>
          Before long, pagers everywhere were firing for all such server installations around the
          world. In response, the on-call engineers disabled all team automation in order to
          prevent further damage. They stopped or froze additional automation and production
          maintenance shortly thereafter.
        </p>
        <p>
          Within an hour, all traffic had been diverted to other locations. Although users may have
          experienced elevated latencies, their requests were fulfilled. The outage was officially
          over.
        </p>
        <p>
          Now the hard part began: recovery. Some network links were reporting heavy congestion, so
          network engineers implemented mitigations as choke points surfaced. A server installation
          in one such location was chosen to be the first of many to rise from the ashes. Within
          three hours of the initial outage, and thanks to the tenacity of several engineers, the
          installation was rebuilt and brought back online, happily accepting user requests once
          again.
        </p>
        <p>
          US teams handed off to their European counterparts, and SRE hatched a plan to prioritize
          reinstallations using a streamlined but manual process. The team was divided into three
          parts, with each part responsible for one step in the manual reinstall process. Within
          three days, the vast majority of capacity was back online, while any stragglers would be
          recovered over the next month or two.
        </p>
        <h2 class="subheaders">
          Findings
        </h2>
        <h3 class="subheaders">
          What went well
        </h3>
        <p>
          Reverse proxies in large server installations are managed very differently than reverse
          proxies in these small installations, so large installations were not impacted. On-call
          engineers were able to quickly move traffic from smaller installations to large
          installations. By design, these large installations can handle a full load without
          difficulty. However, some network links became congested, and therefore required network
          engineers to develop workarounds. In order to reduce the impact on end users, on-call
          engineers targeted congested networks as their highest priority.
        </p>
        <p>
          The turndown process for the small installations worked efficiently and well. From start
          to finish, it took less than an hour to successfully turn down and securely wipe a large
          number of these installations.
        </p>
        <p>
          Although turndown automation quickly tore down monitoring for the small installations,
          on-call engineers were able to promptly revert those monitoring changes. Doing so helped
          them to assess the extent of the damage.
        </p>
        <p>
          The engineers quickly followed incident response protocols, which had matured
          considerably in the year since the first outage described in this chapter. Communication
          and collaboration throughout the company and across teams was superb—a real testament to
          the incident management program and training. All hands within the respective teams
          chipped in, bringing their vast experience to bear.
        </p>
        <h3 class="subheaders">
          What we learned
        </h3>
        <p>
          The root cause was that the turndown automation server lacked the appropriate sanity
          checks on the commands it sent. When the server ran again in response to the initial
          failed turndown, it received an empty response for the machine rack. Instead of filtering
          the response, it passed the empty filter to the machine database, telling the machine
          database to Diskerase all machines involved. Yes, sometimes zero does mean all. The
          machine database complied, so the turndown workflow started churning through the machines
          as quickly as possible.
        </p>
        <p>
          Reinstallations of machines were slow and unreliable. This behavior was due in large part
          to the use of the Trivial File Transfer Protocol (TFTP) at the lowest network Quality of
          Service (QoS) from the distant locations. The BIOS for each machine in the system dealt
          poorly with the failures.<sup><a href="emergency-response.html#id-Zo9unImIkIdTbUa" id=
          "id-Zo9unImIkIdTbUa-marker">77</a></sup> Depending on the network cards involved, the
          BIOS either halted or went into a constant reboot cycle. They were failing to transfer
          the boot files on each cycle and were further taxing the installers. On-call engineers
          were able to fix these reinstall problems by reclassifying installation traffic at
          slightly higher priority and using automation to restart any machines that were stuck.
        </p>
        <p>
          The machine reinstallation infrastructure was unable to handle the simultaneous setup of
          thousands of machines. This inability was partly due to a regression that prevented the
          infrastructure from running more than two setup tasks per worker machine. The regression
          also used improper QoS settings to transfer files and had poorly tuned timeouts. It
          forced kernel reinstallation, even on machines that still had the proper kernel and on
          which Diskerase had yet to occur. To remedy this situation, on-call engineers escalated
          to parties responsible for this infrastructure who were able to quickly retune it to
          support this unusual load.
        </p>
        <h1 class="heading">
          All Problems Have Solutions
        </h1>
        <p>
          Time and experience have shown that systems will not only break, but will break in ways
          that one could never previously imagine. One of the greatest lessons Google has learned
          is that a solution exists, even if it may not be obvious, especially to the person whose
          pager is screaming. If you can’t think of a solution, cast your net farther. Involve more
          of your teammates, seek help, do whatever you have to do, but do it quickly. The highest
          priority is to resolve the issue at hand quickly. Oftentimes, the person with the most
          state is the one whose actions somehow triggered the event. Utilize that person.
        </p>
        <p>
          Very importantly, once the emergency has been mitigated, do not forget to set aside time
          to clean up, write up the incident, and to…
        </p>
        <h1 class="heading">
          Learn from the Past. Don’t Repeat It.
        </h1>
        <h2 class="subheaders">
          Keep a History of Outages
        </h2>
        <p>
          There is no better way to learn than to document what has broken in the past. History is
          about learning from everyone’s mistakes. Be thorough, be honest, but most of all, ask
          hard questions. Look for specific actions that might prevent such an outage from
          recurring, not just tactically, but also strategically. Ensure that everyone within the
          company can learn what you have learned by publishing and organizing
          <span>postmortems</span>.
        </p>
        <p>
          Hold yourself and others accountable to following up on the specific actions detailed in
          these postmortems. Doing so will prevent a future outage that’s nearly identical to, and
          caused by nearly the same triggers as, an outage that has already been documented. Once
          you have a solid track record for learning from past outages, see what you can do to
          prevent future ones.
        </p>
        <h2 class="subheaders">
          Ask the Big, Even Improbable, Questions: What If…?
        </h2>
        <p>
          There is no greater test than reality. Ask yourself some big, open-ended questions. What
          if the building power fails…? What if the network equipment racks are standing in two
          feet of water…? What if the primary datacenter suddenly goes dark…? What if someone
          compromises your web server…? What do you do? Who do you call? Who will write the check?
          Do you have a plan? Do you know how to react? Do you know how your systems will react?
          Could you minimize the impact if it were to happen now? Could the person sitting next to
          you do the same?
        </p>
        <h2 class="subheaders">
          Encourage Proactive Testing
        </h2>
        <p>
          When it comes to failures, theory and reality are two very different realms. Until your
          system has actually failed, you don’t truly know how that system, its dependent systems,
          or your users will react. Don’t rely on assumptions or what you can’t or haven’t tested.
          Would you prefer that a failure happen at 2 a.m. Saturday morning when most of the
          company is still away on a team-building offsite in the Black Forest—or when you have
          your best and brightest close at hand, monitoring the test that they
          <span>painstakingly</span> reviewed in the previous weeks?
        </p>
        <h1 class="heading">
          Conclusion
        </h1>
        <p>
          We’ve reviewed three different cases where parts of our systems broke. Although all three
          emergencies were triggered differently—one by a proactive test, another by a
          configuration change, and yet another by turndown automation—the responses shared many
          characteristics. The responders didn’t panic. They pulled in others when they thought it
          necessary. The responders studied and learned from earlier outages. Subsequently, they
          built their systems to better respond to those types of outages. Each time new failure
          modes presented themselves, responders documented those failure modes. This follow-up
          helped other teams learn how to better troubleshoot and fortify their systems against
          similar outages. Responders proactively tested their systems. Such testing ensured that
          the changes fixed the underlying problems, and identified other weaknesses before they
          became outages.
        </p>
        <p>
          And as our systems evolve the cycle continues, with each outage or test resulting in
          incremental improvements to both processes and systems. While the case studies in this
          chapter are specific to Google, this approach to emergency response can be applied over
          time to any organization of any size.
        </p>
        <div class="footnotes">
          <p id="id-Zo9unImIkIdTbUa">
            <sup><a href="emergency-response.html#id-Zo9unImIkIdTbUa-marker">77</a></sup>BIOS: Basic Input/Output System.
            BIOS is the software built into a computer to send simple instructions to the hardware,
            allowing input and output before the operating system has been loaded.
          </p>
        </div>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="effective-troubleshooting.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Chapter 12- Effective Troubleshooting
          </p></a>
        </div>
        <div class="next">
          <a href="managing-incidents.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 14- Managing Incidents
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
