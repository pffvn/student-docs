<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body>
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 22 - Addressing Cascading Failures
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <h1 class="heading">
          Addressing Cascading Failures
        </h1>
        <p class="byline author">
          Written by Mike Ulrich
        </p>
        <blockquote>
          <p class="quote">
            If at first you don't succeed, back off exponentially."
          </p>
          <p class="quote-author">
            Dan Sandler, Google Software Engineer
          </p>
        </blockquote>
        <blockquote>
          <p class="quote">
            Why do people always forget that you need to add a little jitter?"
          </p>
          <p class="quote-author">
            Ade Oshineye, Google Developer Advocate
          </p>
        </blockquote>
        <p>
          A cascading failure is a failure that grows over time as a result of positive
          feedback.<sup><a href="addressing-cascading-failures.html#id-GbduZFnh9" id="id-GbduZFnh9-marker">107</a></sup> It can occur
          when a portion of an overall system fails, increasing the probability that other portions
          of the system fail. For example, a single replica for a service can fail due to overload,
          increasing load on remaining replicas and increasing their probability of failing,
          causing a domino effect that takes down all the replicas for a service.
        </p>
        <p>
          We’ll use the Shakespeare search service discussed in <a href=
          "production-environment.html#xref_production-environment_shakespeare">Shakespeare:
          A Sample Service</a> as an example throughout this chapter. Its production configuration
          might look something like <a href="addressing-cascading-failures.html#fig_cascading-failure_basic-architecture">Figure
          22-1</a>.
        </p>
        <figure class="horizontal vertical" id="fig_cascading-failure_basic-architecture">
          <img alt="Example production configuration for the Shakespeare search service." src=
          "../images/srle-2201.jpg">
          <figcaption>
            <span>Figure 22-1.</span> Example production configuration for the Shakespeare search
            service
          </figcaption>
        </figure>
        <h1 class="heading">
          Causes of Cascading Failures and Designing to Avoid Them
        </h1>
        <p>
          Well-thought-out system design should take into account a few typical scenarios that
          account for the majority of cascading failures.
        </p>
        <h2 class="subheaders" id="xref_cascading-failure_server-overload">
          Server Overload
        </h2>
        <p>
          The most common cause of cascading failures is overload. Most cascading failures
          described here are either directly due to server overload, or due to extensions or
          variations of this scenario.
        </p>
        <p>
          Suppose the frontend in cluster A is handling 1,000 requests per second (QPS), as in
          <a href="addressing-cascading-failures.html#fig_cascading-failure_load-distribution">Figure 22-2</a>.
        </p>
        <figure class="horizontal vertical" id="fig_cascading-failure_load-distribution">
          <img alt="Normal server load distribution between clusters A and B." class=
          "horizontal vertical" src="../images/srle-2202.jpg">
          <figcaption>
            <span>Figure 22-2.</span> Normal server load distribution between clusters A and B
          </figcaption>
        </figure>
        <p>
          If cluster B fails (<a href="addressing-cascading-failures.html#fig_cascading-failure_failure-load-distribution">Figure
          22-3</a>), requests to cluster A increase to 1,200 QPS. The <span>frontends</span> in A
          are not able to handle requests at 1,200 QPS, and therefore start running out of
          resources, which causes them to crash, miss deadlines, or otherwise misbehave. As a
          result, the rate of successfully handled requests in A dips well below 1,000 QPS.
        </p>
        <figure class="horizontal vertical" id="fig_cascading-failure_failure-load-distribution">
          <img alt="Cluster B fails, sending all traffic to cluster A." class="horizontal vertical"
          src="../images/srle-2203.jpg">
          <figcaption>
            <span>Figure 22-3.</span> Cluster B fails, sending all traffic to cluster A
          </figcaption>
        </figure>
        <p>
          This reduction in the rate of useful work being done can spread into other failure
          domains, potentially spreading globally. For example, local overload in one cluster may
          lead to its servers crashing; in response, the load balancing controller sends requests
          to other clusters, overloading their servers, leading to a service-wide overload failure.
          It may not take long for these events to transpire (e.g., on the order of a couple
          minutes), because the load balancer and task scheduling systems involved may act very
          quickly.
        </p>
        <h2 class="subheaders">
          Resource Exhaustion
        </h2>
        <p>
          Running out of a resource can result in higher latency, elevated error rates, or the
          substitution of lower-quality results. These are in fact desired effects of running out
          of resources: something eventually needs to give as the load increases beyond what a
          server can handle.
        </p>
        <p>
          Depending on what resource becomes exhausted in a server and how the server is built,
          resource exhaustion can render the server less efficient or cause the server to crash,
          prompting the load balancer to distribute the resource problems to other servers. When
          this happens, the rate of successfully handled requests can drop and possibly send the
          cluster or an entire service into a cascade failure.
        </p>
        <p>
          Different types of resources can be exhausted, resulting in varying effects on servers.
        </p>
        <h3 class="subheaders">
          CPU
        </h3>
        <p>
          If there is insufficient CPU to handle the request load, typically all requests become
          slower. This scenario can result in various secondary effects, including the following:
        </p>
        <dl>
          <dt class="dt-heading">
            Increased number of in-flight requests
          </dt>
          <dd>
            <p>
              Because requests take longer to handle, more requests are handled concurrently (up to
              a possible maximum capacity at which queuing may occur). This affects almost all
              resources, including memory, number of active threads (in a thread-per-request server
              model), number of file descriptors, and backend resources (which in turn can have
              other effects).
            </p>
          </dd>
          <dt class="dt-heading">
            Excessively long queue lengths
          </dt>
          <dd>
            <p>
              If there is insufficient capacity to handle all the requests at steady state, the
              server will saturate its queues. This means that latency increases (the requests are
              queued for longer amounts of time) and the queue uses more memory. See <a href=
              "addressing-cascading-failures.html#xref_cascading-failure_queue-management">Queue Management</a> for a discussion of
              mitigation strategies.
            </p>
          </dd>
          <dt class="dt-heading">
            Thread starvation
          </dt>
          <dd>
            <p>
              When a thread can’t make progress because it’s waiting for a lock, health checks may
              fail if the health check endpoint can’t be served in time.
            </p>
          </dd>
          <dt class="dt-heading">
            CPU or request starvation
          </dt>
          <dd>
            <p>
              Internal watchdogs<sup><a href="addressing-cascading-failures.html#id-p8BuzSVSmijIlh0tjiq" id=
              "id-p8BuzSVSmijIlh0tjiq-marker">108</a></sup> in the server detect that the server
              isn’t making progress, causing the servers to crash due to CPU starvation, or due to
              request starvation if watchdog events are triggered remotely and processed as part of
              the request queue.
            </p>
          </dd>
          <dt class="dt-heading">
            Missed RPC deadlines
          </dt>
          <dd>
            <p>
              As a server becomes overloaded, its responses to RPCs from its clients arrive later,
              which may exceed any deadlines those clients set. The work the server did to respond
              is then wasted, and clients may retry the RPCs, leading to even more overload.
            </p>
          </dd>
          <dt class="dt-heading">
            Reduced CPU caching benefits
          </dt>
          <dd>
            <p>
              As more CPU is used, the chance of spilling on to more cores increases, resulting in
              decreased usage of local caches and decreased CPU efficiency.
            </p>
          </dd>
        </dl>
        <h3 class="subheaders">
          Memory
        </h3>
        <p>
          If nothing else, more in-flight requests consume more RAM from allocating the request,
          response, and RPC objects. Memory exhaustion can cause the following effects:
        </p>
        <dl>
          <dt class="dt-heading">
            Dying tasks
          </dt>
          <dd>
            <p>
              For example, a task might be evicted by the container manager (VM or otherwise) for
              exceeding available resource limits, or application-specific crashes may cause tasks
              to die.
            </p>
          </dd>
          <dt class="dt-heading">
            Increased rate of garbage collection (GC) in Java, resulting in increased CPU usage
          </dt>
          <dd>
            <p>
              A vicious cycle can occur in this scenario:less CPU is available, resulting in slower
              requests, resulting in increased RAM usage, resulting in more GC, resulting in even
              lower availability of CPU.This is known colloquially as the “GC death spiral.”
            </p>
          </dd>
          <dt class="dt-heading">
            Reduction in cache hit rates
          </dt>
          <dd>
            <p>
              Reduction in available RAM can reduce application-level cache hit rates, resulting in
              more RPCs to the backends, which can possibly cause the backends to become
              overloaded.
            </p>
          </dd>
        </dl>
        <h3 class="subheaders">
          Threads
        </h3>
        <p>
          Thread starvation can directly cause errors or lead to health check failures. If the
          server adds threads as needed, thread overhead can use too much RAM. In extreme cases,
          thread starvation can also cause you to run out of process IDs.
        </p>
        <h3 class="subheaders">
          File descriptors
        </h3>
        <p>
          Running out of file descriptors can lead to the inability to initialize network
          connections, which in turn can cause health checks to fail.
        </p>
        <h3 class="subheaders">
          Dependencies among resources
        </h3>
        <p>
          Note that many of these resource exhaustion scenarios feed from one another—a service
          experiencing overload often has a host of secondary symptoms that can look like the root
          cause, making debugging difficult.
        </p>
        <p>
          For example, imagine the following scenario:
        </p>
        <ol>
          <li>A Java frontend has poorly tuned garbage collection (GC) parameters.
          </li>
          <li>Under high (but expected) load, the frontend runs out of CPU due to GC.
          </li>
          <li>CPU exhaustion slows down completion of requests.
          </li>
          <li>The increased number of in-progress requests causes more RAM to be used to process
          the requests.
          </li>
          <li>Memory pressure due to requests, in combination with a fixed memory allocation for
          the frontend process as a whole, leaves less RAM available for caching.
          </li>
          <li>The reduced cache size means fewer entries in the cache, in addition to a lower hit
          rate.
          </li>
          <li>The increase in cache misses means that more requests fall through to the backend for
          servicing.
          </li>
          <li>The backend, in turn, runs out of CPU or threads.
          </li>
          <li>Finally, the lack of CPU causes basic health checks to fail, starting a cascading
          failure.
          </li>
        </ol>
        <p>
          In situations as complex as the preceding scenario, it’s unlikely that the causal chain
          will be fully diagnosed during an outage. It might be very hard to determine that the
          backend crash was caused by a decrease in the cache rate in the frontend, particularly if
          the frontend and backend components have different owners.
        </p>
        <h2 class="subheaders">
          Service Unavailability
        </h2>
        <p>
          Resource exhaustion can lead to servers crashing; for example, servers might crash when
          too much RAM is allocated to a container. Once a couple of servers crash on overload, the
          load on the remaining servers can increase, causing them to crash as well. The problem
          tends to snowball and soon all servers begin to crash-loop. It’s often difficult to
          escape this scenario because as soon as servers come back online they’re bombarded with
          an extremely high rate of requests and fail almost immediately.
        </p>
        <p>
          For example, if a service was healthy at 10,000 QPS, but started a cascading failure due
          to crashes at 11,000 QPS, dropping the load to 9,000 QPS will almost certainly not stop
          the crashes. This is because the service will be handling increased demand with reduced
          capacity; only a small fraction of servers will usually be healthy enough to handle
          requests. The fraction of servers that will be healthy depends on a few factors: how
          quickly the system is able to start the tasks, how quickly the binary can start serving
          at full capacity, and how long a freshly started task is able to survive the load. In
          this example, if 10% of the servers are healthy enough to handle requests, the request
          rate would need to drop to about 1,000 QPS in order for the system to stabilize and
          recover.
        </p>
        <p>
          Similarly, servers can appear unhealthy to the load balancing layer, resulting in reduced
          load balancing capacity: servers may go into “lame duck” state (see <a href=
          "load-balancing-datacenter.html#robust_approach_lame_duck">A Robust
          Approach to Unhealthy Tasks: Lame Duck State</a>) or fail health checks without crashing.
          The effect can be very similar to crashing: more servers appear unhealthy, the healthy
          servers tend to accept requests for a very brief period of time before becoming
          unhealthy, and fewer servers participate in handling requests.
        </p>
        <p>
          Load balancing policies that avoid servers that have served errors can exacerbate
          problems further—a few backends serve some errors, so they don’t contribute to the
          available capacity for the service. This increases the load on the remaining servers,
          starting the snowball effect.
        </p>
        <h1 class="heading" id="xref_cascading-failure_preventing-server-overload">
          Preventing Server Overload
        </h1>
        <p>
          The following list presents strategies for avoiding server overload in rough priority
          order:
        </p>
        <dl>
          <dt class="dt-heading">
            Load test the server’s capacity limits, and test the failure mode for overload
          </dt>
          <dd>
            <p>
              This is the most important important exercise you should conduct in order to prevent
              server overload. Unless you test in a realistic environment, it’s very hard to
              predict exactly which resource will be exhausted and how that resource exhaustion
              will manifest. For details, see <a href="addressing-cascading-failures.html#xref_cascading-failure_testing">Testing for
              Cascading Failures</a>.
            </p>
          </dd>
          <dt class="dt-heading">
            Serve degraded results
          </dt>
          <dd>
            <p>
              Serve lower-quality, cheaper-to-compute results to the user. Your strategy here will
              be service-specific.See <a href=
              "addressing-cascading-failures.html#xref_cascading-failure_load-shed-graceful-degredation">Load Shedding and Graceful
              Degradation</a>.
            </p>
          </dd>
          <dt class="dt-heading">
            Instrument the server to reject requests when overloaded
          </dt>
          <dd>
            <p>
              Servers should protect themselves from becoming overloaded and crashing.When
              overloaded at either the frontend or backend layers, fail early and cheaply. For
              details, see <a href="addressing-cascading-failures.html#xref_cascading-failure_load-shed-graceful-degredation">Load
              Shedding and Graceful Degradation</a>.
            </p>
          </dd>
          <dt class="dt-heading">
            Instrument higher-level systems to reject requests, rather than overloading servers
          </dt>
          <dd>
            <p>
              Note that because rate limiting often doesn’t take overall service health into
              account, it may not be able to stop a failure that has already begun.Simple
              rate-limiting implementations are also likely to leave capacity unused. Rate limiting
              can be implemented in a number of places:
            </p>
            <ul>
              <li>
                <em>At the reverse proxies</em>, by limiting the volume of requests by criteria
                such as IP address to mitigate attempted denial-of-service attacks and abusive
                <span>clients</span>.
              </li>
              <li>
                <em>At the load balancers</em>, by dropping requests when the service enters global
                overload. Depending on the nature and complexity of the service, this rate limiting
                can be indiscriminate (“drop all traffic above X requests per second”) or more
                selective (“drop requests that aren’t from users who have recently interacted with
                the service” or “drop requests for low-priority operations like background
                synchronization, but keep serving interactive user <span>sessions</span>”).
              </li>
              <li>
                <em>At individual tasks</em>, to prevent random fluctuations in load balancing from
                overwhelming the server.
              </li>
            </ul>
          </dd>
          <dt class="dt-heading">
            Perform capacity planning
          </dt>
          <dd>
            <p>
              Good capacity planning can reduce the probability that a cascading failure will
              occur.Capacity planning should be coupled with performance testing to determine the
              load at which the service will fail.For instance, if every cluster’s breaking point
              is 5,000 QPS, the load is evenly spread across clusters,<sup><a href=
              "addressing-cascading-failures.html#id-jE2ujIQSjUEIKuv" id="id-jE2ujIQSjUEIKuv-marker">109</a></sup> and the service’s
              peak load is 19,000 QPS, then approximately six clusters are needed to run the
              service at <em>N</em> + 2.
            </p>
          </dd>
        </dl>
        <p>
          Capacity planning reduces the probability of triggering a cascading failure, but it is
          not sufficient to protect the service from cascading failures. When you lose major parts
          of your infrastructure during a planned or unplanned event, no amount of capacity
          planning may be sufficient to prevent cascading failures. Load balancing problems,
          network partitions, or unexpected traffic increases can create pockets of high load
          beyond what was planned. Some systems can grow the number of tasks for your service on
          demand, which may prevent overload; however, proper capacity planning is still needed.
        </p>
        <h2 class="subheaders" id="xref_cascading-failure_queue-management">
          Queue Management
        </h2>
        <p>
          Most thread-per-request servers use a queue in front of a thread pool to handle
          requests.Requests come in, they sit on a queue, and then threads pick requests off the
          queue and perform the actual work (whatever actions are required by the server). Usually,
          if the queue is full, the server will reject new requests.
        </p>
        <p>
          If the request rate and latency of a given task is constant, there is no reason to queue
          requests: a constant number of threads should be occupied. Under this idealized scenario,
          requests will only be queued if the steady state rate of incoming requests exceeds the
          rate at which the server can process requests, which results in saturation of both the
          thread pool and the queue.
        </p>
        <p>
          Queued requests consume memory and increase latency. For example, if the queue size is
          10x the number of threads, the time to handle the request on a thread is 100
          milliseconds. If the queue is full, then a request will take 1.1 seconds to handle, most
          of which time is spent on the queue.
        </p>
        <p>
          For a system with fairly steady traffic over time, it is usually better to have small
          queue lengths relative to the thread pool size (e.g., 50% or less), which results in the
          server rejecting requests early when it can’t sustain the rate of incoming requests. For
          example, Gmail often uses queueless servers, relying instead on failover to other server
          tasks when the threads are full. On the other end of the spectrum, systems with “bursty”
          load for which traffic patterns fluctuate drastically may do better with a queue size
          based on the current number of threads in use, processing time for each request, and the
          size and frequency of bursts.
        </p>
        <h2 class="subheaders" id="xref_cascading-failure_load-shed-graceful-degredation">
          Load Shedding and Graceful Degradation
        </h2>
        <p>
          <em>Load shedding</em> drops some proportion of load by dropping traffic as the server
          approaches overload conditions. The goal is to keep the server from running out of RAM,
          failing health checks, serving with extremely high latency, or any of the other symptoms
          associated with overload, while still doing as much useful work as it can.
        </p>
        <p>
          One straightforward way to shed load is to do per-task throttling based on CPU, memory,
          or queue length; limiting queue length as discussed in <a href=
          "addressing-cascading-failures.html#xref_cascading-failure_queue-management">Queue Management</a> is a form of this
          strategy. For example, one effective approach is to return an HTTP 503 (service
          unavailable) to any incoming request when there are more than a given number of client
          requests in flight.
        </p>
        <p>
          Changing the queuing method from the standard <em>first-in, first-out</em> (FIFO) to
          <em>last-in, first-out</em> (LIFO) or using the <em>controlled delay</em> (CoDel)
          algorithm <a href="bibliography.html#Nic12" target=
          "_blank">[Nic12]</a> or similar approaches can reduce load by removing requests that are
          unlikely to be worth processing <a href="bibliography.html#Mau15"
          target="_blank">[Mau15]</a>. If a user’s web search is slow because an RPC has been
          queued for 10 seconds, there’s a good chance the user has given up and refreshed their
          browser, issuing another request: there’s no point in responding to the first one, since
          it will be ignored! This strategy works well when combined with propagating RPC deadlines
          throughout the stack, described in <a href=
          "addressing-cascading-failures.html#xref_cascading-failure_latency-and-deadlines">Latency and Deadlines</a>.
        </p>
        <p>
          More sophisticated approaches include identifying clients to be more selective about what
          work is dropped, or picking requests that are more important and prioritizing.Such
          strategies are more likely to be needed for shared services.
        </p>
        <p>
          <em>Graceful degradation</em> takes the concept of load shedding one step further by
          reducing the amount of work that needs to be performed. In some applications, it’s
          possible to significantly decrease the amount of work or time needed by decreasing the
          quality of responses. For instance, a search application might only search a subset of
          data stored in an in-memory cache rather than the full on-disk database or use a
          less-accurate (but faster) ranking algorithm when overloaded.
        </p>
        <p>
          When evaluating load shedding or graceful degradation options for your service, consider
          the following:
        </p>
        <ul>
          <li>Which metrics should you use to determine when load shedding or graceful degradation
          should kick in (e.g,. CPU usage, latency, queue length, number of threads used, whether
          your service enters degraded mode automatically or if manual intervention is necessary)?
          </li>
          <li>What actions should be taken when the server is in degraded mode?
          </li>
          <li>At what layer should load shedding and graceful degradation be implemented? Does it
          make sense to implement these strategies at every layer in the stack, or is it sufficient
          to have a high-level choke-point?
          </li>
        </ul>
        <p>
          As you evaluate options and deploy, keep the following in mind:
        </p>
        <ul>
          <li>Graceful degradation shouldn’t trigger very often—usually in cases of a capacity
          planning failure or unexpected load shift. Keep the system simple and understandable,
          particularly if it isn’t used often.
          </li>
          <li>Remember that the code path you never use is the code path that (often) doesn’t work.
          In steady-state operation, graceful degradation mode won’t be used, implying that you’ll
          have much less operational experience with this mode and any of its quirks, which
          <em>increases</em> the level of risk. You can make sure that graceful degradation stays
          working by regularly running a small subset of servers near overload in order to exercise
          this code path.
          </li>
          <li>Monitor and alert when too many servers enter these modes.
          </li>
          <li>Complex load shedding and graceful degradation can cause problems
          themselves—excessive complexity may cause the server to trip into a degraded mode when it
          is not desired, or enter feedback cycles at undesired times. Design a way to quickly turn
          off complex graceful degradation or tune parameters if needed. Storing this configuration
          in a consistent system that each server can watch for changes, such as Chubby, can
          increase deployment speed, but also introduces its own risks of synchronized failure.
          </li>
        </ul>
        <h2 class="subheaders">
          Retries
        </h2>
        <p>
          Suppose the code in the frontend that talks to the backend implements retries naively.It
          retries after encountering a failure and caps the number of backend RPCs per logical
          request to 10.Consider this code in the frontend, using gRPC in Go:
        </p>
        <pre>
func exampleRpcCall(client pb.ExampleClient, request pb.Request) *pb.Response {

    // Set RPC timeout to 5 seconds.
    opts := grpc.WithTimeout(5 * time.Second)

    // Try up to 20 times to make the RPC call.
    attempts := 20
    for attempts &gt; 0 {
        conn, err := grpc.Dial(*serverAddr, opts...)
        if err != nil {
            // Something went wrong in setting up the connection.  Try again.
            attempts--
            continue
        }
        defer conn.Close()

        // Create a client stub and make the RPC call.
        client := pb.NewBackendClient(conn)
        response, err := client.MakeRequest(context.<wbr>Background, request)
        if err != nil {
            // Something went wrong in making the call. Try again.
            attempts--
            continue
        }

        return response
    }

    grpclog.Fatalf("ran out of attempts")
}
</pre>
        <p>
          This system can cascade in the following way:
        </p>
        <ol>
          <li>Assume our backend has a known limit of 10,000 QPS per task, after which point all
          further requests are rejected in an attempt at graceful degradation.
          </li>
          <li>The frontend calls <code>MakeRequest</code> at a constant rate of 10,100 QPS and
          overloads the backend by 100 QPS, which the backend rejects.
          </li>
          <li>Those 100 failed QPS are retried in <code>MakeRequest</code> every 1,000 ms, and
          probably succeed. But the retries are themselves adding to the requests sent to the
          backend, which now receives 10,200 QPS—200 QPS of which are failing due to
          <span>overload</span>.
          </li>
          <li>The volume of retries grows: 100 QPS of retries in the first second leads to 200 QPS,
          then to 300 QPS, and so on. Fewer and fewer requests are able to succeed on their first
          attempt, so less useful work is being performed as a fraction of requests to the backend.
          </li>
          <li>If the backend task is unable to handle the increase in load—which is consuming file
          descriptors, memory, and CPU time on the backend—it can melt down and crash under the
          sheer load of requests and retries. This crash then redistributes the requests it was
          receiving across the remaining backend tasks, in turn further overloading those tasks.
          </li>
        </ol>
        <p>
          Some simplifying assumptions were made here to illustrate this scenario,<sup><a href=
          "addressing-cascading-failures.html#id-OdauNSrT7cvuy" id="id-OdauNSrT7cvuy-marker">110</a></sup> but the point remains that
          retries can destabilize a system. Note that both temporary load spikes and slow increases
          in usage can cause this effect.
        </p>
        <p>
          Even if the rate of calls to <code>MakeRequest</code> decreases to pre-meltdown levels
          (9,000 QPS, for example), depending on how much returning a failure costs the backend,
          the problem might not go away. Two factors are at play here:
        </p>
        <ul>
          <li>If the backend spends a significant amount of resources processing requests that will
          ultimately fail due to overload, then the retries themselves may be keeping the backend
          in an overloaded mode.
          </li>
          <li>The backend servers themselves may not be stable. Retries can amplify the effects
          seen in <a href="addressing-cascading-failures.html#xref_cascading-failure_server-overload">Server Overload</a>.
          </li>
        </ul>
        <p>
          If either of these conditions is true, in order to dig out of this outage, you must
          dramatically reduce or eliminate the load on the frontends until the retries stop and the
          backends stabilize.
        </p>
        <p>
          This pattern has contributed to several cascading failures, whether the frontends and
          backends communicate via RPC messages, the “frontend” is client JavaScript code issuing
          <code>XmlHttpRequest</code> calls to an endpoint and retries on failure, or the retries
          originate from an offline sync protocol that retries aggressively when it encounters a
          failure.
        </p>
        <p>
          When issuing automatic retries, keep in mind the following considerations:
        </p>
        <ul>
          <li>Most of the backend protection strategies described in <a href=
          "addressing-cascading-failures.html#xref_cascading-failure_preventing-server-overload">Preventing Server Overload</a>
          apply. In particular, testing the system can highlight problems, and graceful degradation
          can reduce the effect of the retries on the backend.
          </li>
          <li>Always use randomized exponential backoff when scheduling retries. See also
            <a href="http://www.awsarchitectureblog.com/2015/03/backoff.html" target=
            "_blank">"Exponential Backoff and Jitter"</a> in the AWS Architecture Blog <a href=
            "bibliography.html#Bro15" target="_blank">[Bro15]</a>. If retries
            aren’t randomly distributed over the retry window, a small perturbation (e.g., a
            network blip) can cause retry ripples to schedule at the same time, which can then
            amplify themselves <a href="bibliography.html#Flo94" target=
            "_blank">[Flo94]</a>.
          </li>
          <li>Limit retries per request. Don’t retry a given request indefinitely.
          </li>
          <li>Consider having a server-wide retry budget. For example, only allow 60 retries per
          minute in a process, and if the retry budget is exceeded, don’t retry; just fail the
          request. This strategy can contain the retry effect and be the difference between a
          capacity planning failure that leads to some dropped queries and a global cascading
          failure.
          </li>
          <li>Think about the service holistically and decide if you really need to perform retries
          at a given level. In particular, avoid amplifying retries by issuing retries at multiple
          levels: a single request at the highest layer may produce a number of attempts as large
          as the <em>product</em> of the number of attempts at each layer to the lowest layer. If
          the database can’t service requests because it’s overloaded, and the backend, frontend,
          and JavaScript layers all issue 3 retries (4 attempts), then a single user action may
          create 64 attempts (<span>4^3</span>) on the database.This behavior is undesirable when
          the database is returning those errors because it’s overloaded.
          </li>
          <li>Use clear response codes and consider how different failure modes should be handled.
          For example, separate retriable and nonretriable error conditions. Don’t retry permanent
          errors or malformed requests in a client, because neither will ever succeed. Return a
          specific status when overloaded so that clients and other layers back off and do not
          retry.
          </li>
        </ul>
        <p>
          In an emergency, it may not be obvious that an outage is due to bad retry behavior.
          Graphs of retry rates can be an indication of bad retry behavior, but may be confused as
          a symptom instead of a compounding cause. In terms of mitigation, this is a special case
          of the insufficient capacity problem, with the additional caveat that you must either fix
          the retry behavior (usually requiring a code push), reduce load significantly, or cut
          requests off entirely.
        </p>
        <h2 class="subheaders" id="xref_cascading-failure_latency-and-deadlines">
          Latency and Deadlines
        </h2>
        <p>
          When a frontend sends an RPC to a backend server, the frontend consumes resources waiting
          for a reply. RPC deadlines define how long a request can wait before the
          <span>frontend</span> gives up, limiting the time that the backend may consume the
          frontend’s <span>resources</span>.
        </p>
        <h3 class="subheaders">
          Picking a deadline
        </h3>
        <p>
          It’s usually wise to set a deadline. Setting either no deadline or an extremely high
          deadline may cause short-term problems that have long since passed to continue to consume
          server resources until the server restarts.
        </p>
        <p>
          High deadlines can result in resource consumption in higher levels of the stack when
          lower levels of the stack are having problems. Short deadlines can cause some more
          expensive requests to fail consistently. Balancing these constraints to pick a good
          deadline can be something of an art.
        </p>
        <h3 class="subheaders">
          Missing deadlines
        </h3>
        <p>
          A common theme in many cascading outages is that servers spend resources handling
          requests that will exceed their deadlines on the client. As a result, resources are spent
          while no progress is made: you don’t get credit for late assignments with RPCs.
        </p>
        <p>
          Suppose an RPC has a 10-second deadline, as set by the client.The server is very
          overloaded, and as a result, it takes 11 seconds to move from a queue to a thread pool.
          At this point, the client has already given up on the request. Under most circumstances,
          it would be unwise for the server to attempt to handle this request, because it would be
          doing work for which no credit will be granted—the client doesn’t care what work the
          server does after the deadline has passed, because it’s given up on the request already.
        </p>
        <p>
          If handling a request is performed over multiple stages (e.g., there are a few callbacks
          and RPC calls), the server should check the deadline left at each stage before attempting
          to perform any more work on the request. For example, if a request is split into parsing,
          backend request, and processing stages, it may make sense to check that there is enough
          time left to handle the request before each stage.
        </p>
        <h3 class="subheaders">
          Deadline propagation
        </h3>
        <p>
          Rather than inventing a deadline when sending RPCs to backends, servers should employ
          deadline propagation and cancellation propagation.
        </p>
        <p>
          With deadline propagation, a deadline is set high in the stack (e.g., in the frontend).
          The tree of RPCs emanating from an initial request will all have the same absolute
          deadline. For example, if server <em>A</em> selects a 30-second deadline, and processes
          the request for 7 seconds before sending an RPC to server <em>B</em>, the RPC from
          <em>A</em> to <em>B</em> will have a 23-second deadline. If server <em>B</em> takes 4
          seconds to handle the request and sends an RPC to server <em>C</em>, the RPC from
          <em>B</em> to <em>C</em> will have a 19-second deadline, and so on. Ideally, each server
          in the request tree implements deadline propagation.
        </p>
        <p>
          Without deadline propagation, the following scenario may occur:
        </p>
        <ol>
          <li>Server <em>A</em> sends an RPC to server <em>B</em> with a 10-second deadline.
          </li>
          <li>Server <em>B</em> takes 8 seconds to start processing the request and then sends an
          RPC to server <em>C</em>.
          </li>
          <li>If server <em>B</em> uses deadline propagation, it should set a 2-second deadline,
          but suppose it instead uses a hardcoded 20-second deadline for the RPC to server
          <em>C</em>.
          </li>
          <li>Server <em>C</em> pulls the request off its queue after 5 seconds.
          </li>
        </ol>
        <p>
          Had server <em>B</em> used deadline propagation, server <em>C</em> could immediately give
          up on the request because the 2-second deadline was exceeded. However, in this scenario,
          server <em>C</em> processes the request thinking it has 15 seconds to spare, but is not
          doing useful work, since the request from server <em>A</em> to server <em>B</em> has
          already exceeded its deadline.
        </p>
        <p>
          You may want to reduce the outgoing deadline a bit (e.g., a few hundred milliseconds) to
          account for network transit times and post-processing in the client.
        </p>
        <p>
          Also consider setting an upper bound for outgoing deadlines. You may want to limit how
          long the server waits for outgoing RPCs to noncritical backends, or for RPCs to backends
          that typically complete in a short duration. However, be sure to understand your traffic
          mix, because you might otherwise inadvertently make particular types of requests fail all
          the time (e.g., requests with large payloads, or requests that require responding to a
          lot of computation).
        </p>
        <p>
          There are some exceptions for which servers may wish to continue processing a request
          after the deadline has elapsed. For example, if a server receives a request that involves
          performing some expensive catchup operation and periodically checkpoints the progress of
          the catchup, it would be a good idea to check the deadline only after writing the
          checkpoint, instead of after the expensive operation.
        </p>
        <p>
          Propagating cancellations avoids the potential RPC leakage that occurs if an initial RPC
          has a long deadline, but RPCs between deeper layers of the stack have short deadlines and
          time out. Using simple deadline propagation, the initial RPC continues to use server
          resources until it eventually times out, despite being unable to make progress.
        </p>
        <h3 class="subheaders">
          Bimodal latency
        </h3>
        <p>
          Suppose that the frontend from the preceding example consists of 10 servers, each with
          100 worker threads. This means that the frontend has a total of 1,000 threads of
          capacity.During usual operation, the frontends perform 1,000 QPS and requests complete in
          100 ms. This means that the frontends usually have 100 worker threads occupied out of the
          1,000 configured worker threads (1,000 QPS * 0.1 seconds).
        </p>
        <p>
          Suppose an event causes 5% of the requests to never complete.This could be the result of
          the unavailability of some Bigtable row ranges, which renders the requests corresponding
          to that Bigtable keyspace unservable. As a result, 5% of the requests hit the deadline,
          while the remaining 95% of the requests take the usual 100 ms.
        </p>
        <p>
          With a 100-second deadline, 5% of requests would consume 5,000 threads (50 QPS * 100
          seconds), but the frontend doesn’t have that many threads available. Assuming no other
          secondary effects, the frontend will only be able to handle 19.6% of the requests (1,000
          threads available / (5,000 + 95) threads’ worth of work), resulting in an 80.4% error
          rate.
        </p>
        <p>
          Therefore, instead of only 5% of requests receiving an error (those that didn’t complete
          due to keyspace unavailability), most requests receive an error.
        </p>
        <p>
          The following guidelines can help address this class of problems:
        </p>
        <ul>
          <li>Detecting this problem can be very hard. In particular, it may not be clear that
          bimodal latency is the cause of an outage when you are looking at <em>mean</em> latency.
          When you see a latency increase, try to look at the <em>distribution</em> of latencies in
          addition to the averages.
          </li>
          <li>This problem can be avoided if the requests that don’t complete return with an error
          early, rather than waiting the full deadline. For example, if a backend is unavailable,
          it’s usually best to immediately return an error for that backend, rather than consuming
          resources until it the backend available. If your RPC layer supports a fail-fast option,
          use it.
          </li>
          <li>Having deadlines several orders of magnitude longer than the mean request latency is
          usually bad. In the preceding example, a small number of requests initially hit the
          deadline, but the deadline was three orders of magnitude larger than the normal mean
          latency, leading to thread exhaustion.
          </li>
          <li>When using shared resources that can be exhausted by some keyspace, consider either
          limiting in-flight requests by that keyspace or using other kinds of abuse tracking.
          Suppose your backend processes requests for different clients that have wildly different
          performance and request characteristics. You might consider only allowing 25% of your
          threads to be occupied by any one client in order to provide fairness in the face of
          heavy load by any single client misbehaving.
          </li>
        </ul>
        <h1 class="heading">
          Slow Startup and Cold Caching
        </h1>
        <p>
          Processes are often slower at responding to requests immediately after starting than they
          will be in steady state. This slowness can be caused by either or both of the following:
        </p>
        <dl>
          <dt class="dt-heading">
            Required initialization
          </dt>
          <dd>
            <p>
              Setting up connections upon receiving the first request that needs a given
              <span>backend</span>
            </p>
          </dd>
          <dt class="dt-heading">
            Runtime performance improvements in some languages, particularly Java
          </dt>
          <dd>
            <p>
              Just-In-Time compilation, hotspot optimization, and deferred class loading
            </p>
          </dd>
        </dl>
        <p>
          Similarly, some binaries are less efficient when caches aren’t filled. For example, in
          the case of some of Google’s services, most requests are served out of caches, so
          requests that miss the cache are significantly more expensive. In steady-state operation
          with a warm cache, only a few cache misses occur, but when the cache is completely empty,
          100% of requests are costly. Other services might employ caches to keep a user’s state in
          RAM. This might be accomplished through hard or soft stickiness between reverse proxies
          and service frontends.
        </p>
        <p>
          If the service is not provisioned to handle requests under a cold cache, it’s at greater
          risk of outages and should take steps to avoid them.
        </p>
        <p>
          The following scenarios can lead to a cold cache:
        </p>
        <dl>
          <dt class="dt-heading">
            Turning up a new cluster
          </dt>
          <dd>
            <p>
              A recently added cluster will have an empty cache.
            </p>
          </dd>
          <dt class="dt-heading">
            Returning a cluster to service after maintenance
          </dt>
          <dd>
            <p>
              The cache may be stale.
            </p>
          </dd>
          <dt class="dt-heading">
            Restarts
          </dt>
          <dd>
            <p>
              If a task with a cache has recently restarted, filling its cache will take some time.
              It may be worthwhile to move caching from a server to a separate binary like
              memcache, which also allows cache sharing between many servers, albeit at the cost of
              introducing another RPC and slight additional latency.
            </p>
          </dd>
        </dl>
        <p>
          If caching has a significant effect on the service,<sup><a href="addressing-cascading-failures.html#id-wbau7SBiPUZ" id=
          "id-wbau7SBiPUZ-marker">111</a></sup> you may want to use one or some of the following
          strategies:
        </p>
        <ul>
          <li>Overprovision the service. It’s important to note the distinction between a latency
          cache versus a capacity cache: when a latency cache is employed, the service can sustain
          its expected load with an empty cache, but a service using a capacity cache cannot
          sustain its expected load under an empty cache. Service owners should be vigilant about
          adding caches to their service, and make sure that any new caches are either latency
          caches or are sufficiently well engineered to safely function as capacity caches.
          Sometimes caches are added to a service to improve performance, but actually wind up
          being hard dependencies.
          </li>
          <li>Employ general cascading failure prevention techniques.In particular, servers should
          reject requests when they’re overloaded or enter degraded modes, and testing should be
          performed to see how the service behaves after events such as a large restart.
          </li>
          <li>When adding load to a cluster, slowly increase the load. The initially small request
          rate warms up the cache; once the cache is warm, more traffic can be added. It’s a good
          idea to ensure that all clusters carry nominal load and that the caches are kept warm.
          </li>
        </ul>
        <h2 class="subheaders">
          Always Go Downward in the Stack
        </h2>
        <p>
          In the example Shakespeare service, the frontend talks to a backend, which in turn talks
          to the storage layer. A problem that manifests in the storage layer can cause problems
          for servers that talk to it, but fixing the storage layer will usually repair both the
          backend and frontend layers.
        </p>
        <p>
          However, suppose the backends cross-communicate amongst each other. For example, the
          backends might proxy requests to one another to change who owns a user when the storage
          layer can’t service a request. This intra-layer communication can be problematic for
          several reasons:
        </p>
        <ul>
          <li>
            <p>
              The communication is susceptible to a distributed deadlock. Backends may use the same
              thread pool to wait on RPCs sent to remote backends that are simultaneously receiving
              requests from remote backends. Suppose backend <em>A</em>’s thread pool is full.
              Backend <em>B</em> sends a request to backend <em>A</em> and uses a thread in backend
              <em>B</em> until backend <em>A</em>’s thread pool clears. This behavior can cause the
              thread pool saturation to spread.
            </p>
          </li>
          <li>
            <p>
              If intra-layer communication increases in response to some kind of failure or heavy
              load condition (e.g., load rebalancing that is more active under high load),
              intra-layer communication can quickly switch from a low to high intra-layer request
              mode when the load increases enough.
            </p>
            <p>
              For example, suppose a user has a primary backend and a predetermined hot standby
              secondary backend in a different cluster that can take over the user. The primary
              backend proxies requests to the secondary backend as a result of errors from the
              lower layer or in response to heavy load on the master. If the entire system is
              overloaded, primary to secondary proxying will likely increase and add even more load
              to the system, due to the additional cost of parsing and waiting on the request to
              the secondary in the primary.
            </p>
          </li>
          <li>
            <p>
              Depending on the criticality of the cross-layer communication, bootstrapping the
              system may become more complex.
            </p>
            <p>
              It’s usually better to avoid intra-layer communication—i.e., possible cycles in the
              communication path—in the user request path. Instead, have the client do the
              communication. For example, if a frontend talks to a backend but guesses the wrong
              backend, the backend should not proxy to the correct backend. Instead, the backend
              should tell the frontend to retry its request on the correct backend.
            </p>
          </li>
        </ul>
        <h1 class="heading">
          Triggering Conditions for Cascading Failures
        </h1>
        <p>
          When a service is susceptible to cascading failures, there are several possible
          disturbances that can initiate the domino effect. This section identifies some of the
          factors that trigger cascading failures.
        </p>
        <h2 class="subheaders">
          Process Death
        </h2>
        <p>
          Some server tasks may die, reducing the amount of available capacity. Tasks might die
          because of a Query of Death (an RPC whose contents trigger a failure in the process),
          cluster issues, assertion failures, or a number of other reasons. A very small event
          (e.g., a couple of crashes or tasks rescheduled to other machines) may cause a service on
          the brink of falling to break.
        </p>
        <h2 class="subheaders">
          Process Updates
        </h2>
        <p>
          Pushing a new version of the binary or updating its configuration may initiate a
          cascading failure if a large number of tasks are affected simultaneously. To prevent this
          scenario, either account for necessary capacity overhead when setting up the service’s
          update infrastructure, or push off-peak. Dynamically adjusting the number of in-flight
          task updates based on the volume of requests and available capacity may be a workable
          approach.
        </p>
        <h2 class="subheaders">
          New Rollouts
        </h2>
        <p>
          A new binary, configuration changes, or a change to the underlying infrastructure stack
          can result in changes to request profiles, resource usage and limits, backends, or a
          number of other system components that can trigger a cascading failure.
        </p>
        <p>
          During a cascading failure, it’s usually wise to check for recent changes and consider
          reverting them, particularly if those changes affected capacity or altered the request
          profile.
        </p>
        <p>
          Your service should implement some type of change logging, which can help quickly
          identify recent changes.
        </p>
        <h2 class="subheaders">
          Organic Growth
        </h2>
        <p>
          In many cases, a cascading failure isn’t triggered by a specific service change, but
          because a growth in usage wasn’t accompanied by an adjustment to capacity.
        </p>
        <h2 class="subheaders">
          Planned Changes, Drains, or Turndowns
        </h2>
        <p>
          If your service is multihomed, some of your capacity may be unavailable because of
          maintenance or outages in a cluster. Similarly, one of the service’s critical
          dependencies may be drained, resulting in a reduction in capacity for the upstream
          service due to drain dependencies, or an increase in latency due to having to send the
          requests to a more distant cluster.
        </p>
        <h3 class="subheaders">
          Request profile changes
        </h3>
        <p>
          A backend service may receive requests from different clusters because a frontend service
          shifted its traffic due to load balancing configuration changes, changes in the traffic
          mix, or cluster fullness. Also, the average cost to handle an individual payload may have
          changed due to frontend code or configuration changes. Similarly, the data handled by the
          service may have changed organically due to increased or differing usage by existing
          users: for instance, both the number and size of images, <em>per user</em>, for a photo
          storage service tend to increase over time.
        </p>
        <h3 class="subheaders">
          Resource limits
        </h3>
        <p>
          Some cluster operating systems allow resource overcommitment. CPU is a fungible resource;
          often, some machines have some amount of slack CPU available, which provides a bit of a
          safety net against CPU spikes. The availability of this slack CPU differs between cells,
          and also between machines within the cell.
        </p>
        <p>
          Depending upon this slack CPU as your safety net is dangerous. Its availability is
          entirely dependent on the behavior of the other jobs in the cluster, so it might suddenly
          drop out at any time. For example, if a team starts a MapReduce that consumes a lot of
          CPU and schedules on many machines, the aggregate amount of slack CPU can suddenly
          decrease and trigger CPU starvation conditions for unrelated jobs. When performing load
          tests, make sure that you remain within your committed resource limits.
        </p>
        <h1 class="heading" id="xref_cascading-failure_testing">
          Testing for Cascading Failures
        </h1>
        <p>
          The specific ways in which a service will fail can be very hard to predict from first
          principles. This section discusses testing strategies that can detect if services are
          susceptible to cascading failures.
        </p>
        <p>
          You should test your service to determine how it behaves under heavy load in order to
          gain confidence that it won’t enter a cascading failure under various circumstances.
        </p>
        <h2 class="subheaders">
          Test Until Failure and Beyond
        </h2>
        <p>
          Understanding the behavior of the service under heavy load is perhaps the most important
          first step in avoiding cascading failures. Knowing how your system behaves when it is
          overloaded helps to identify what engineering tasks are the most important for long-term
          fixes; at the very least, this knowledge may help bootstrap the debugging process for
          on-call engineers when an emergency arises.
        </p>
        <p>
          Load test components until they break. As load increases, a component typically handles
          requests successfully until it reaches a point at which it can’t handle more requests. At
          this point, the component should ideally start serving errors or degraded results in
          response to additional load, but not significantly reduce the rate at which it
          successfully handles requests. A component that is highly susceptible to a cascading
          failure will start crashing or serving a very high rate of errors when it becomes
          overloaded; a better designed component will instead be able to reject a few requests and
          survive.
        </p>
        <p>
          Load testing also reveals where the breaking point is, knowledge that’s fundamental to
          the capacity planning process. It enables you to test for regressions, provision for
          worst-case thresholds, and to trade off utilization versus safety margins.
        </p>
        <p>
          Because of caching effects, gradually ramping up load may yield different results than
          immediately increasing to expected load levels. Therefore, consider testing both gradual
          and impulse load patterns.
        </p>
        <p>
          You should also test and understand how the component behaves as it returns to nominal
          load after having been pushed well beyond that load. Such testing may answer questions
          such as:
        </p>
        <ul>
          <li>If a component enters a degraded mode on heavy load, is it capable of exiting the
          degraded mode without human intervention?
          </li>
          <li>If a couple of servers crash under heavy load, how much does the load need to drop in
          order for the system to stabilize?
          </li>
        </ul>
        <p>
          If you’re load testing a stateful service or a service that employs caching, your load
          test should track state between multiple interactions and check correctness at high load,
          which is often where subtle concurrency bugs hit.
        </p>
        <p>
          Keep in mind that individual components may have different breaking points, so load test
          each component separately. You won’t know in advance which component may hit the wall
          first, and you want to know how your system behaves when it does.
        </p>
        <p>
          If you believe your system has proper protections against being overloaded, consider
          performing failure tests in a small slice of production to find the point at which the
          components in your system fail under real traffic. These limits may not be adequately
          reflected by synthetic load test traffic, so real traffic tests may provide more
          realistic results than load tests, at the risk of causing user-visible pain. Be careful
          when testing on real traffic: make sure that you have extra capacity available in case
          your automatic protections don’t work and you need to manually fail over. You might
          consider some of the following production tests:
        </p>
        <ul>
          <li>Reducing task counts quickly or slowly over time, beyond expected traffic
          <span>patterns</span>
          </li>
          <li>Rapidly losing a cluster’s worth of capacity
          </li>
          <li>Blackholing various backends
          </li>
        </ul>
        <h2 class="subheaders">
          Test Popular Clients
        </h2>
        <p>
          Understand how large clients use your service.For example, you want to know if
          <span>clients</span>:
        </p>
        <ul>
          <li>Can queue work while the service is down
          </li>
          <li>Use randomized exponential backoff on errors
          </li>
          <li>Are vulnerable to external triggers that can create large amounts of load (e.g., an
          externally triggered software update might clear an offline client’s cache)
          </li>
        </ul>
        <p>
          Depending on your service, you may or may not be in control of all the client code that
          talks to your service. However, it’s still a good idea to have an understanding of how
          large clients that interact with your service will behave.
        </p>
        <p>
          The same principles apply to large internal clients. Stage system failures with the
          largest clients to see how they react. Ask internal clients how they access your service
          and what mechanisms they use to handle backend failure.
        </p>
        <h2 class="subheaders">
          Test Noncritical Backends
        </h2>
        <p>
          Test your noncritical backends, and make sure their unavailability does not interfere
          with the critical components of your service.
        </p>
        <p>
          For example, suppose your frontend has critical and noncritical backends. Often, a given
          request includes both critical components (e.g., query results) and noncritical
          components (e.g., spelling suggestions). Your requests may significantly slow down and
          consume resources waiting for noncritical backends to finish.
        </p>
        <p>
          In addition to testing behavior when the noncritical backend is unavailable, test how the
          frontend behaves if the noncritical backend never responds (for example, if it is
          blackholing requests). Backends advertised as noncritical can still cause problems on
          frontends when requests have long deadlines. The frontend should not start rejecting lots
          of requests, running out of resources, or serving with very high latency when a
          noncritical backend blackholes.
        </p>
        <h1 class="heading">
          Immediate Steps to Address Cascading Failures
        </h1>
        <p>
          Once you have identified that your service is experiencing a cascading failure, you can
          use a few different strategies to remedy the situation—and of course, a cascading failure
          is a good opportunity to use your incident management protocol (<a href=
          "managing-incidents.html">Managing Incidents</a>).
        </p>
        <h2 class="subheaders">
          Increase Resources
        </h2>
        <p>
          If your system is running at degraded capacity and you have idle resources, adding tasks
          can be the most expedient way to recover from the outage. However, if the service has
          entered a death spiral of some sort, adding more resources may not be sufficient to
          recover.
        </p>
        <h2 class="subheaders">
          Stop Health Check Failures/Deaths
        </h2>
        <p>
          Some cluster scheduling systems, such as Borg, check the health of tasks in a job and
          restart tasks that are unhealthy. This practice may create a failure mode in which
          health-checking itself makes the service unhealthy. For example, if half the tasks aren’t
          able to accomplish any work because they’re starting up and the other half will soon be
          killed because they’re overloaded and failing health checks, temporarily disabling health
          checks may permit the system to stabilize until all the tasks are running.
        </p>
        <p>
          Process health checking (“is this binary responding <em>at all</em>?”) and service health
          checking (“is this binary able to respond to <em>this class of requests</em> right now?”)
          are two conceptually distinct operations. Process health checking is relevant to the
          cluster scheduler, whereas service health checking is relevant to the load balancer.
          Clearly distinguishing between the two types of health checks can help avoid this
          scenario.
        </p>
        <h2 class="subheaders">
          Restart Servers
        </h2>
        <p>
          If servers are somehow wedged and not making progress, restarting them may help.Try
          restarting servers when:
        </p>
        <ul>
          <li>Java servers are in a GC death spiral
          </li>
          <li>Some in-flight requests have no deadlines but are consuming resources, leading them
          to block threads, for example
          </li>
          <li>The servers are deadlocked
          </li>
        </ul>
        <p>
          Make sure that you identify the source of the cascading failure before you restart your
          servers. Make sure that taking this action won’t simply shift around load. Canary this
          change, and make it slowly. Your actions may amplify an existing cascading failure if the
          outage is actually due to an issue like a cold cache.
        </p>
        <h2 class="subheaders">
          Drop Traffic
        </h2>
        <p>
          Dropping load is a big hammer, usually reserved for situations in which you have a true
          cascading failure on your hands and you cannot fix it by other means. For example, if
          heavy load causes most servers to crash as soon as they become healthy, you can get the
          service up and running again by:
        </p>
        <ol>
          <li>Addressing the initial triggering condition (by adding capacity, for example).
          </li>
          <li>Reducing load enough so that the crashing stops. Consider being aggressive here—if
          the entire service is crash-looping, only allow, say, 1% of the traffic through.
          </li>
          <li>Allowing the majority of the servers to become healthy.
          </li>
          <li>Gradually ramping up the load.
          </li>
        </ol>
        <p>
          This strategy allows caches to warm up, connections to be established, etc., before load
          returns to normal levels.
        </p>
        <p>
          Obviously, this tactic will cause a lot of user-visible harm. Whether or not you’re able
          to (or if you even <em>should</em>) drop traffic indiscriminately depends on how the
          service is configured. If you have some mechanism to drop less important traffic (e.g.,
          prefetching), use that mechanism first.
        </p>
        <p>
          It is important to keep in mind that this strategy enables you to recover from a
          cascading outage once the underlying problem is fixed. If the issue that started the
          cascading failure is not fixed (e.g., insufficient global capacity), then the cascading
          failure may trigger shortly after all traffic returns. Therefore, before using this
          strategy, consider fixing (or at least papering over) the root cause or triggering
          condition. For example, if the service ran out of memory and is now in a death spiral,
          adding more memory or tasks should be your first step.
        </p>
        <h2 class="subheaders">
          Enter Degraded Modes
        </h2>
        <p>
          Serve degraded results by doing less work or dropping unimportant traffic. This strategy
          must be engineered into your service, and can be implemented only if you know which
          traffic can be degraded and you have the ability to differentiate between the various
          payloads.
        </p>
        <h2 class="subheaders">
          Eliminate Batch Load
        </h2>
        <p>
          Some services have load that is important, but not critical. Consider turning off those
          sources of load. For example, if index updates, data copies, or statistics gathering
          consume resources of the serving path, consider turning off those sources of load during
          an outage.
        </p>
        <h2 class="subheaders">
          Eliminate Bad Traffic
        </h2>
        <p>
          If some queries are creating heavy load or crashes (e.g., queries of death), consider
          blocking them or eliminating them via other means.
        </p>
        <h5 class="subheaders">
          Cascading Failure and Shakespeare
        </h5>
        <p>
          A documentary about Shakespeare’s works airs in Japan, and explicitly points to our
          Shakespeare service as an excellent place to conduct further research. Following the
          broadcast, traffic to our Asian datacenter surges beyond the service’s capacity. This
          capacity problem is further compounded by a major update to the Shakespeare service that
          simultaneously occurs in that datacenter.
        </p>
        <p>
          Fortunately, a number of safeguards are in place that help mitigate the potential for
          failure. The Production Readiness Review process identified some issues that the team
          already addressed. For example, the developers built graceful degradation into the
          service. As capacity becomes scarce, the service no longer returns pictures alongside
          text or small maps illustrating where a story takes place. And depending on its purpose,
          an RPC that times out is either not retried (for example, in the case of the
          aforementioned pictures), or is retried with a randomized exponential backoff. Despite
          these safeguards, the tasks fail one by one and are then restarted by Borg, which drives
          the number of working tasks down even more.
        </p>
        <p>
          As a result, some graphs on the service dashboard turn an alarming shade of red and SRE
          is paged. In response, SREs temporarily add capacity to the Asian datacenter by
          increasing the number of tasks available for the Shakespeare job. By doing so, they’re
          able to restore the Shakespeare service in the Asian cluster.
        </p>
        <p>
          Afterward, the SRE team writes a postmortem detailing the chain of events, what went
          well, what could have gone better, and a number of action items to prevent this scenario
          from occurring again. For example, in the case of a service overload, the GSLB load
          balancer will redirect some traffic to neighboring datacenters. Also, the SRE team turns
          on autoscaling, so that the number of tasks automatically increases with traffic, so they
          don’t have to worry about this type of issue again.
        </p>
        <h1 class="heading">
          Closing Remarks
        </h1>
        <p>
          When systems are overloaded, something needs to give in order to remedy the situation.
          Once a service passes its breaking point, it is better to allow some user-visible errors
          or lower-quality results to slip through than try to fully serve every request.
          Understanding where those breaking points are and how the system behaves beyond them is
          critical for service owners who want to avoid cascading failures.
        </p>
        <p>
          Without proper care, some system changes meant to reduce background errors or otherwise
          improve the steady state can expose the service to greater risk of a full outage.
          Retrying on failures, shifting load around from unhealthy servers, killing unhealthy
          servers, adding caches to improve performance or reduce latency: all of these might be
          implemented to improve the normal case, but can improve the chance of causing a
          large-scale failure. Be careful when evaluating changes to ensure that one outage is not
          being traded for another.
        </p>
        <div class="footnotes">
          <p id="id-GbduZFnh9">
            <sup><a href="addressing-cascading-failures.html#id-GbduZFnh9-marker">107</a></sup>See Wikipedia, “Positive feedback,”
            <a href="https://en.wikipedia.org/wiki/Positive_feedback" target=
            "_blank"><em>https://en.wikipedia.org/wiki/<wbr>Positive_feedback</em></a>.
          </p>
          <p id="id-p8BuzSVSmijIlh0tjiq">
            <sup><a href="addressing-cascading-failures.html#id-p8BuzSVSmijIlh0tjiq-marker">108</a></sup>A watchdog is often
            implemented as a thread that wakes up periodically to see whether work has been done
            since the last time it checked. If not, it assumes that the server is stuck and kills
            it. For instance, requests of a known type can be sent to the server at regular
            intervals; if one hasn’t been received or processed when expected, this may indicate
            failure—of the server, the system sending requests, or the intermediate network.
          </p>
          <p id="id-jE2ujIQSjUEIKuv">
            <sup><a href="addressing-cascading-failures.html#id-jE2ujIQSjUEIKuv-marker">109</a></sup>This is often not a good
            assumption due to geography; see also <a href=
            "production-environment.html#xref_production-environment_job-and-data-organization">
            Job and Data Organization</a>.
          </p>
          <p id="id-OdauNSrT7cvuy">
            <sup><a href="addressing-cascading-failures.html#id-OdauNSrT7cvuy-marker">110</a></sup>An instructive exercise, left for
            the reader: write a simple simulator and see how the amount of useful work the backend
            can do varies with how much it’s overloaded and how many retries are permitted.
          </p>
          <p id="id-wbau7SBiPUZ">
            <sup><a href="addressing-cascading-failures.html#id-wbau7SBiPUZ-marker">111</a></sup>Sometimes you find that a meaningful
            proportion of your actual serving capacity is as a function of serving from a cache,
            and if you lost access to that cache, you wouldn’t actually be able to serve that many
            queries. A similar observation holds for latency: a cache can help you achieve latency
            goals (by lowering the average response time when the query is servable from cache)
            that you possibly couldn’t meet without that cache.
          </p>
        </div>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="handling-overload.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Chapter 21- Handling Overload
          </p></a>
        </div>
        <div class="next">
          <a href="managing-critical-state.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 23- Managing Critical State: Distributed Consensus for Reliability
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
