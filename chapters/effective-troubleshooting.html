<!DOCTYPE html>
<html class="google" lang="en">
  <head>
    <meta charset="utf-8">
    <script>
    (function(H){H.className=H.className.replace(/\bgoogle\b/,'google-js')})(document.documentElement)
    </script>
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">
    <title>
      Google - Site Reliability Engineering
    </title>
    <script src="../js/google.js">
    </script>
    <script>
    new gweb.analytics.AutoTrack({profile:"UA-75468017-1"});
    </script>
    <link href="../css/opensans.css" rel=
    "stylesheet">
    <link href="../css/main.min.css" rel="stylesheet">
    <link href=
    '../css/roboto.css'
    rel='stylesheet' type='text/css'>
    <link href="../../images/favicon.ico" rel="shortcut icon">
  </head>
  <body data-ng-app="myApp">
    <div class="menu-closed" id="curtain"></div>
    <div class="header clearfix">
      <div class="header-wrraper">
        <a class="expand" id="burger-menu"></a>
        <h2 class="chapter-title">
          Chapter 12 - Effective Troubleshooting
        </h2>
      </div>
    </div>
    <div class="expands" id="overlay-element">
      <div class="logo">
        <a href="https://www.google.com"><img alt="Google" src=
        "../../images/googlelogo-grey-color.png"></a>
      </div>
      <ol class="dropdown-content hide" id="drop-down">
        <li>
          <a class="menu-buttons" href="../index.html">Table of Contents</a>
        </li>
        <li>
          <a class="menu-buttons" href="foreword.html">Foreword</a>
        </li>
        <li>
          <a class="menu-buttons" href="preface.html">Preface</a>
        </li>
        <li>
          <a class="menu-buttons" href="part1.html">Part I - Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="introduction.html">1. Introduction</a>
        </li>
        <li>
          <a class="menu-buttons" href="production-environment.html">2. The
          Production Environment at Google, from the Viewpoint of an SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="part2.html">Part II - Principles</a>
        </li>
        <li>
          <a class="menu-buttons" href="embracing-risk.html">3. Embracing
          Risk</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-level-objectives.html">4.
          Service Level Objectives</a>
        </li>
        <li>
          <a class="menu-buttons" href="eliminating-toil.html">5. Eliminating
          Toil</a>
        </li>
        <li>
          <a class="menu-buttons" href="monitoring-distributed-systems.html">6.
          Monitoring Distributed Systems</a>
        </li>
        <li>
          <a class="menu-buttons" href="automation-at-google.html">7. The
          Evolution of Automation at Google</a>
        </li>
        <li>
          <a class="menu-buttons" href="release-engineering.html">8. Release
          Engineering</a>
        </li>
        <li>
          <a class="menu-buttons" href="simplicity.html">9. Simplicity</a>
        </li>
        <li>
          <a class="menu-buttons" href="part3.html">Part III - Practices</a>
        </li>
        <li>
          <a class="menu-buttons" href="practical-alerting.html">10. Practical
          Alerting</a>
        </li>
        <li>
          <a class="menu-buttons" href="being-on-call.html">11. Being
          On-Call</a>
        </li>
        <li class='active'>
          <a class="menu-buttons" href="effective-troubleshooting.html">12.
          Effective Troubleshooting</a>
        </li>
        <li>
          <a class="menu-buttons" href="emergency-response.html">13. Emergency
          Response</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-incidents.html">14. Managing
          Incidents</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem-culture.html">15. Postmortem
          Culture: Learning from Failure</a>
        </li>
        <li>
          <a class="menu-buttons" href="tracking-outages.html">16. Tracking
          Outages</a>
        </li>
        <li>
          <a class="menu-buttons" href="testing-reliability.html">17. Testing
          for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href="software-engineering-in-sre.html">18.
          Software Engineering in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-frontend.html">19. Load
          Balancing at the Frontend</a>
        </li>
        <li>
          <a class="menu-buttons" href="load-balancing-datacenter.html">20. Load
          Balancing in the Datacenter</a>
        </li>
        <li>
          <a class="menu-buttons" href="handling-overload.html">21. Handling
          Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href="addressing-cascading-failures.html">22.
          Addressing Cascading Failures</a>
        </li>
        <li>
          <a class="menu-buttons" href="managing-critical-state.html">23.
          Managing Critical State: Distributed Consensus for Reliability</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "distributed-periodic-scheduling.html">24. Distributed Periodic
          Scheduling with Cron</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-processing-pipelines.html">25. Data
          Processing Pipelines</a>
        </li>
        <li>
          <a class="menu-buttons" href="data-integrity.html">26. Data Integrity:
          What You Read Is What You Wrote</a>
        </li>
        <li>
          <a class="menu-buttons" href="reliable-product-launches.html">27.
          Reliable Product Launches at Scale</a>
        </li>
        <li>
          <a class="menu-buttons" href="part4.html">Part IV - Management</a>
        </li>
        <li>
          <a class="menu-buttons" href="accelerating-sre-on-call.html">28.
          Accelerating SREs to On-Call and Beyond</a>
        </li>
        <li>
          <a class="menu-buttons" href="dealing-with-interrupts.html">29.
          Dealing with Interrupts</a>
        </li>
        <li>
          <a class="menu-buttons" href="operational-overload.html">30. Embedding
          an SRE to Recover from Operational Overload</a>
        </li>
        <li>
          <a class="menu-buttons" href=
          "communication-and-collaboration.html">31. Communication and
          Collaboration in SRE</a>
        </li>
        <li>
          <a class="menu-buttons" href="evolving-sre-engagement-model.html">32.
          The Evolving SRE Engagement Model</a>
        </li>
        <li>
          <a class="menu-buttons" href="part5.html">Part V - Conclusions</a>
        </li>
        <li>
          <a class="menu-buttons" href="lessons-learned.html">33. Lessons
          Learned from Other Industries</a>
        </li>
        <li>
          <a class="menu-buttons" href="conclusion.html">34. Conclusion</a>
        </li>
        <li>
          <a class="menu-buttons" href="availability-table.html">Appendix A.
          Availability Table</a>
        </li>
        <li>
          <a class="menu-buttons" href="service-best-practices.html">Appendix B.
          A Collection of Best Practices for Production Services</a>
        </li>
        <li>
          <a class="menu-buttons" href="incident-document.html">Appendix C.
          Example Incident State Document</a>
        </li>
        <li>
          <a class="menu-buttons" href="postmortem.html">Appendix D. Example
          Postmortem</a>
        </li>
        <li>
          <a class="menu-buttons" href="launch-checklist.html">Appendix E.
          Launch Coordination Checklist</a>
        </li>
        <li>
          <a class="menu-buttons" href="bibliography.html">Appendix F.
          Bibliography</a>
        </li>
      </ol>
    </div>
    <div id="maia-main" role="main">
      <div class="maia-teleport" id="content"></div>
      <div class="content">
        <section data-type="chapter" id="chapter_troubleshooting">
          <h1 class="heading">
            Effective Troubleshooting
          </h1>
          <p class="author byline">
            Written by Chris Jones
          </p>
          <blockquote data-type="epigraph">
            <p class="quote">
              Be warned that being an expert is more than understanding how a system is supposed to
              work. Expertise is gained by investigating why a system doesn't work."
            </p>
            <p class="quote-author" data-type="attribution">
              Brian Redman
            </p>
          </blockquote>
          <blockquote data-type="epigraph">
            <p class="quote">
              Ways in which things go right are special cases of the ways in which things go
              wrong."
            </p>
            <p class="quote-author" data-type="attribution">
              John Allspaw
            </p>
          </blockquote>
          <p>
            <a data-primary="troubleshooting" data-secondary="approaches to" data-type="indexterm"
            id="id-zdCxSyhN"></a>Troubleshooting is a critical skill for anyone who operates
            distributed computing systems—especially SREs—but it’s often viewed as an innate skill
            that some people have and others don’t. One reason for this assumption is that, for
            those who troubleshoot often, it’s an ingrained process; explaining <em>how</em> to
            troubleshoot is difficult, much like explaining how to ride a bike. However, we believe
            that troubleshooting is <em>both</em> learnable and teachable.
          </p>
          <p>
            Novices are often tripped up when troubleshooting because the exercise ideally depends
            upon two factors: an understanding of how to troubleshoot generically (i.e., without
            any particular system knowledge) and a solid knowledge of the system. While you can
            investigate a problem using only the generic process and derivation from first
            principles,<sup><a data-type="noteref" href="effective-troubleshooting.html#id-EYJuMSKTj" id=
            "id-EYJuMSKTj-marker">58</a></sup> we usually find this approach to be less efficient
            and less effective than understanding how things are supposed to work. Knowledge of the
            system typically limits the effectiveness of an SRE new to a system; there’s little
            substitute to learning how the system is designed and built.
          </p>
          <p>
            Let’s look at a general model of the troubleshooting process. Readers with expertise in
            troubleshooting may quibble with our definitions and process; if your method is
            effective for you, there’s no reason not to stick with it.
          </p>
          <section data-type="sect1" id="theory-Nbsviy">
            <h1 class="heading">
              Theory
            </h1>
            <p>
              <a data-primary="troubleshooting" data-secondary="model of" data-type="indexterm" id=
              "id-wqC7SPFziP"></a>Formally, we can think of the troubleshooting process as an
              application of the hypothetico-deductive method:<sup><a data-type="noteref" href=
              "effective-troubleshooting.html#id-a82udFEFzik" id="id-a82udFEFzik-marker">59</a></sup> given a set of observations
              about a system and a theoretical basis for understanding system behavior, we
              iteratively hypothesize potential causes for the failure and try to test those
              hypotheses.
            </p>
            <p>
              In an idealized model such as that in <a data-type="xref" href=
              "effective-troubleshooting.html#xref_troubleshooting_process">Figure 12-1</a>, we’d start with a problem report
              telling us that something is wrong with the system. Then we can look at the system’s
              telemetry<sup><a data-type="noteref" href="effective-troubleshooting.html#id-0vYuEF7IWiA" id=
              "id-0vYuEF7IWiA-marker">60</a></sup> and logs to understand its current state. This
              information, combined with our knowledge of how the system is built, how it should
              operate, and its failure modes, enables us to identify some possible causes.
            </p>
            <figure class="horizontal vertical" id="xref_troubleshooting_process">
              <img alt="A process for troubleshooting." src="../images/srle-1201.jpg">
              <figcaption>
                <span class="label">Figure 12-1.</span> A process for troubleshooting
              </figcaption>
            </figure>
            <p class="pagebreak-before">
              We can then test our hypotheses in one of two ways. We can compare the observed state
              of the system against our theories to find confirming or disconfirming evidence. Or,
              in some cases, we can actively “treat” the system—that is, change the system in a
              controlled way—and observe the results. This second approach refines our
              understanding of the system’s state and possible cause(s) of the reported problems.
              Using either of these strategies, we repeatedly test until a root cause is
              identified, at which point we can then take corrective action to prevent a recurrence
              and write a postmortem. Of course, fixing the proximate cause(s) needn’t always wait
              for root-causing or postmortem writing.
            </p>
            <aside class="highlight" data-type="sidebar" id="common-pitfalls-QASNTOi0">
              <h5 class="heading">
                Common Pitfalls
              </h5>
              <p>
                <a data-primary="troubleshooting" data-secondary="process diagram" data-type=
                "indexterm" id="id-jyCxSoFET0iw"></a><a data-primary="troubleshooting"
                data-secondary="common pitfalls" data-type="indexterm" id=
                "id-2nClFjFGTJiD"></a><a data-primary="troubleshooting" data-secondary="pitfalls"
                data-type="indexterm" id="Tpit12"></a>Ineffective troubleshooting sessions are
                plagued by problems at the Triage, Examine, and Diagnose steps, often because of a
                lack of deep system understanding. The following are common pitfalls to avoid:
              </p>
              <ul>
                <li>Looking at symptoms that aren’t relevant or misunderstanding the meaning of
                system metrics. Wild goose chases often result.
                </li>
                <li>Misunderstanding how to change the system, its inputs, or its environment, so
                as to safely and effectively test hypotheses.
                </li>
                <li>Coming up with wildly improbable theories about what’s wrong, or latching on to
                causes of past problems, reasoning that since it happened once, it must be
                happening again.
                </li>
                <li>Hunting down spurious correlations that are actually coincidences or are
                correlated with shared causes.
                </li>
              </ul>
              <p>
                Fixing the first and second common pitfalls is a matter of learning the system in
                question and becoming experienced with the common patterns used in distributed
                systems. The third trap is a set of logical fallacies that can be avoided by
                remembering that not all failures are equally probable—as doctors are taught, “when
                you hear hoofbeats, think of horses not zebras.”<sup><a data-type="noteref" href=
                "effective-troubleshooting.html#id-GbduJSWtpTeiW" id="id-GbduJSWtpTeiW-marker">61</a></sup> Also remember that,
                all things being equal, we should prefer simpler explanations.<sup><a data-type=
                "noteref" href="effective-troubleshooting.html#id-D04IaFmtGTziJ" id="id-D04IaFmtGTziJ-marker">62</a></sup>
              </p>
              <p>
                <a data-primary="correlation vs. causation" data-type="indexterm" id=
                "id-oPCAS4hjT0iE"></a>Finally, we should remember that correlation is not
                causation:<sup><a data-type="noteref" href="effective-troubleshooting.html#id-kD2uEF7hnT7io" id=
                "id-kD2uEF7hnT7io-marker">63</a></sup> some correlated events, say packet loss
                within a cluster and failed hard drives in the cluster, share common causes—in this
                case, a power outage, though network failure clearly doesn’t cause the hard drive
                failures nor vice versa. Even worse, as systems grow in size and complexity and as
                more metrics are monitored, it’s inevitable that there will be events that happen
                to correlate well with other events, purely by coincidence.<sup><a data-type=
                "noteref" href="effective-troubleshooting.html#id-4DBukIohGT7ir" id="id-4DBukIohGT7ir-marker">64</a></sup>
              </p>
              <p>
                Understanding failures in our reasoning process is the first step to avoiding them
                and becoming more effective in solving problems. A methodical approach to knowing
                what we do know, what we don’t know, and what we need to know, makes it simpler and
                more straightforward to figure out what’s gone wrong and how to fix
                it.<a data-primary="" data-startref="Tpit12" data-type="indexterm" id=
                "id-g9CnSoTOTPi1"></a>
              </p>
            </aside>
          </section>
          <section data-type="sect1" id="in-practice-8ksluy">
            <h1 class="heading">
              In Practice
            </h1>
            <p>
              In practice, of course, troubleshooting is never as clean as our idealized model
              suggests it should be. There are some steps that can make the process less painful
              and more productive for both those experiencing system problems and those responding
              to them.
            </p>
            <section data-type="sect2" id="problem-report-dbsaI8uP">
              <h2 class="subheaders">
                Problem Report
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="problem reports" data-type=
                "indexterm" id="id-OnCNSmF2IvuM"></a><a data-primary="problem reports" data-type=
                "indexterm" id="id-XmCpFOF8IluL"></a>Every problem starts with a problem report,
                which might be an automated alert or one of your colleagues saying, “The system is
                slow.” An effective report should tell you the <em>expected</em> behavior, the
                <em>actual</em> behavior, and, if possible, how to reproduce the
                behavior.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-GbduEhgFkIQuW" id=
                "id-GbduEhgFkIQuW-marker">65</a></sup> Ideally, the reports should have a
                consistent form and be stored in a searchable location, such as a bug tracking
                system. Here, our teams often have customized forms or small web apps that ask for
                information that’s relevant to diagnosing the particular systems they support,
                which then automatically generate and route a bug. This may also be a good point at
                which to provide tools for problem reporters to try self-diagnosing or
                self-repairing common issues on their own.
              </p>
              <p>
                It’s common practice at Google to open a bug for every issue, even those received
                via email or instant messaging. Doing so creates a log of investigation and
                remediation activities that can be referenced in the future. Many teams discourage
                reporting problems directly to a person for several reasons: this practice
                introduces an additional step of transcribing the report into a bug, produces
                lower-quality reports that aren’t visible to other members of the team, and tends
                to concentrate the problem-solving load on a handful of team members that the
                reporters happen to know, rather than the person currently on duty (see also
                <a data-type="xref" href=
                "dealing-with-interrupts.html">Dealing with
                Interrupts</a>).
              </p>
              <aside class="highlight" data-type="sidebar" id=
              "shakespeare-has-a-problem-bwSnteIPuG">
                <h5 class="heading">
                  Shakespeare Has a Problem
                </h5>
                <p>
                  <a data-primary="Shakespeare search service, example" data-secondary="alert"
                  data-type="indexterm" id="id-2nCzSjFltYI1ug"></a>You’re on-call for the
                  Shakespeare search service and receive an alert,
                  <code>Shakespeare-BlackboxProbe_SearchFailure</code>: your black-box monitoring
                  hasn’t been able to find search results for “the forms of things unknown” for the
                  past five minutes. The alerting system has filed a bug—with links to the
                  black-box prober’s recent results and to the playbook entry for this alert—and
                  assigned it to you. Time to spring into action!
                </p>
              </aside>
            </section>
            <section data-type="sect2" id="triage-BVs0truY">
              <h2 class="subheaders">
                Triage
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="triage" data-type="indexterm" id=
                "id-XmC9SOFytluL"></a><a data-primary="triage process" data-type="indexterm" id=
                "id-jyClFoFbtOuw"></a>Once you receive a problem report, the next step is to figure
                out what to do about it. Problems can vary in severity: an issue might affect only
                one user under very specific circumstances (and might have a workaround), or it
                might entail a complete global outage for a service. Your response should be
                appropriate for the problem’s impact: it’s appropriate to declare an
                all-hands-on-deck emergency for the latter (see <a data-type="xref" href=
                "managing-incidents.html">Managing Incidents</a>), but doing so
                for the former is overkill. Assessing an issue’s severity requires an exercise of
                good engineering judgment and, often, a degree of calm under <span class=
                "keep-together">pressure</span>.
              </p>
              <p>
                Your first response in a major outage may be to start troubleshooting and try to
                find a root cause as quickly as possible. Ignore that instinct!
              </p>
              <p>
                Instead, your course of action should be to <em>make the system work as well as it
                can under the circumstances</em>. This may entail emergency options, such as
                diverting traffic from a broken cluster to others that are still working, dropping
                traffic wholesale to prevent a cascading failure, or disabling subsystems to
                lighten the load. Stopping the bleeding should be your first priority; you aren’t
                helping your users if the system dies while you’re root-causing. Of course, an
                emphasis on rapid triage doesn’t preclude taking steps to preserve evidence of
                what’s going wrong, such as logs, to help with subsequent root-cause analysis.
              </p>
              <p>
                Novice pilots are taught that their first responsibility in an emergency is to fly
                the airplane <a data-type="xref" href="bibliography.html#Gaw09"
                target="_blank">[Gaw09]</a>; troubleshooting is secondary to getting the plane and
                everyone on it <em>safely</em> onto the ground. This approach is also applicable to
                computer systems: for example, if a bug is leading to possibly unrecoverable data
                corruption, freezing the system to prevent further failure may be better than
                letting this behavior continue.
              </p>
              <p>
                This realization is often quite unsettling and counterintuitive for new SREs,
                particularly those whose prior experience was in product development organizations.
              </p>
            </section>
            <section data-type="sect2" id="examine-MQsnhjuB">
              <h2 class="subheaders">
                Examine
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="examining system components"
                data-type="indexterm" id="id-jyCxSoFLhOuw"></a>We need to be able to examine what
                each component in the system is doing in order to understand whether or not it’s
                behaving correctly.
              </p>
              <p>
                Ideally, a monitoring system is recording metrics for your system as discussed in
                <a data-type="xref" href="practical-alerting.html">Practical
                Alerting from Time-Series Data</a>. These metrics are a good place to start
                figuring out what’s wrong. Graphing time-series and operations on time-series can
                be an effective way to understand the behavior of specific pieces of a system and
                find correlations that might suggest where problems began.<sup><a data-type=
                "noteref" href="effective-troubleshooting.html#id-GbduZFjIQhQuW" id="id-GbduZFjIQhQuW-marker">66</a></sup>
              </p>
              <p>
                <a data-primary="troubleshooting" data-secondary="logging" data-type="indexterm"
                id="id-pOCzSWtBhguX"></a><a data-primary="logging" data-type="indexterm" id=
                "id-oPCkF1t2h2uE"></a>Logging is another invaluable tool. Exporting information
                about each operation and about system state makes it possible to understand exactly
                what a process was doing at a given point in time. You may need to analyze system
                logs across one or many processes. Tracing requests through the whole stack using
                tools such as Dapper <a data-type="xref" href=
                "bibliography.html#Sig10" target="_blank">[Sig10]</a> provides a
                very powerful way to understand how a distributed system is working, though varying
                use cases imply significantly different tracing designs <a data-type="xref" href=
                "bibliography.html#Sam14" target="_blank">[Sam14]</a>.
              </p>
              <aside class="highlight" data-type="sidebar" id="logging-0bS8hLhruA">
                <h5 class="heading">
                  Logging
                </h5>
                <p>
                  <a data-primary="text logs" data-type="indexterm" id="id-g9CnS2FPhLhDu8"></a>Text
                  logs are very helpful for reactive debugging in real time, while storing logs in
                  a structured binary format can make it possible to build tools to conduct
                  retrospective analysis with much more information.
                </p>
                <p>
                  It’s really useful to have multiple verbosity levels available, along with a way
                  to increase these levels on the fly. This functionality enables you to examine
                  any or all operations in incredible detail without having to restart your
                  process, while still allowing you to dial back the verbosity levels when your
                  service is operating normally. Depending of the volume of traffic your service
                  receives, it might be better to use statistical sampling; for example, you might
                  show one out of every 1,000 operations.
                </p>
                <p>
                  A next step is to include a selection language so that you can say “show me
                  operations that match X,” for a wide range of X—e.g., <code>Set</code> RPCs with
                  a payload size below 1,024 bytes, or operations that took longer than 10 ms to
                  return, or which called <code>doSomethingInteresting()</code> in
                  <em>rpc_handler.py</em>. You might even want to design your logging
                  infrastructure so that you can turn it on as needed, quickly and selectively.
                </p>
              </aside>
              <p>
                <a data-primary="current state, exposing" data-type="indexterm" id=
                "id-g9CnSoTPhzu1"></a><a data-primary="Remote Procedure Call (RPC)" data-type=
                "indexterm" id="id-qXCAFJTEhauA"></a><a data-primary="endpoints, in debugging"
                data-type="indexterm" id="id-1nCEI1TAheua"></a>Exposing current state is the third
                trick in our toolbox. For example, Google servers have endpoints that show a sample
                of RPCs recently sent or received, so it’s possible to understand how any one
                server is communicating with others without referencing an architecture diagram.
                These endpoints also show histograms of error rates and latency for each type of
                RPC, so that it’s possible to quickly tell what’s unhealthy. Some systems have
                endpoints that show their current configuration or allow examination of their data;
                for instance, Google’s Borgmon servers (<a data-type="xref" href=
                "practical-alerting.html">Practical Alerting from Time-Series
                Data</a>) can show the monitoring rules they’re using, and even allow tracing a
                particular computation step-by-step to the source metrics from which a value is
                derived.
              </p>
              <p>
                Finally, you may even need to instrument a client to experiment with, in order to
                discover what a component is returning in response to requests.
              </p>
              <aside class="highlight" data-type="sidebar" id="debug_shakespeare">
                <h5 class="heading">
                  Debugging Shakespeare
                </h5>
                <p>
                  <a data-primary="Shakespeare search service, example" data-secondary="debugging"
                  data-type="indexterm" id="id-YQCDSYFWi4hlux"></a>Using the link to the black-box
                  monitoring results in the bug, you discover that the prober sends an HTTP GET
                  request to the <code>/api/search</code> endpoint:
                </p>
                <pre data-type="programlisting">
{
      ‘search_text’: ‘the forms of things unknown’
    }
</pre>
                <p>
                  It expects to receive a response with an HTTP 200 response code and a JSON
                  payload exactly matching:
                </p>
                <pre data-type="programlisting">
[{
        "work": "A Midsummer Night's Dream",
        "act": 5,
        "scene": 1,
        "line": 2526,
        "speaker": "Theseus"
    }]
</pre>
                <p>
                  The system is set up to send a probe once a minute; over the past 10 minutes,
                  about half the probes have succeeded, though with no discernible pattern.
                  Unfortunately, the prober doesn’t show you <em>what</em> was returned when it
                  failed; you make a note to fix that for the future.
                </p>
                <p>
                  Using <code>curl</code>, you manually send requests to the search endpoint and
                  get a failed response with HTTP response code 502 (Bad Gateway) and no payload.
                  It has an HTTP header, <code>X-Request-Trace</code>, which lists the addresses of
                  the backend servers responsible for responding to that request. With this
                  information, you can now examine those backends to test whether they’re
                  responding appropriately.
                </p>
              </aside>
            </section>
            <section data-type="sect2" id="diagnose-7ksgTPub">
              <h2 class="subheaders">
                Diagnose
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="diagnosing issues" data-type=
                "indexterm" id="Tdiag12"></a>A thorough understanding of the system’s design is
                decidedly helpful for coming up with plausible hypotheses about what’s gone wrong,
                but there are also some generic practices that will help even without domain
                knowledge.
              </p>
              <section data-type="sect3" id="simplify-and-reduce-Q0sAIJTzuE">
                <h3 class="subheaders">
                  Simplify and reduce
                </h3>
                <p>
                  Ideally, components in a system have well-defined interfaces and perform known
                  transformations from their input to their output (in our example, given an input
                  search text, a component might return output containing possible matches). It’s
                  then possible to look at the connections <em>between</em> components—or,
                  equivalently, at the data flowing between them—to determine whether a given
                  component is working properly. Injecting known test data in order to check that
                  the resulting output is expected (a form of black-box testing) at each step can
                  be especially effective, as can injecting data intended to probe possible causes
                  of errors. Having a solid reproducible test case makes debugging much faster, and
                  it may be possible to use the case in a non-production environment where more
                  invasive or riskier techniques are available than would be possible in
                  production.
                </p>
                <p>
                  Dividing and conquering is a very useful general-purpose solution technique. In a
                  multilayer system where work happens throughout a stack of components, it’s often
                  best to start systematically from one end of the stack and work toward the other
                  end, examining each component in turn. This strategy is also well-suited for use
                  with data processing pipelines. In exceptionally large systems, proceeding
                  linearly may be too slow; an alternative, <em>bisection</em>, splits the system
                  in half and examines the communication paths between components on one side and
                  the other. After determining whether one half seems to be working properly,
                  repeat the process until you’re left with a possibly faulty component.
                </p>
              </section>
              <section data-type="sect3" id="ask-what-where-and-why-bEsntqTPuG">
                <h3 class="subheaders">
                  Ask "what," "where," and "why"
                </h3>
                <p>
                  A malfunctioning system is often still trying to do <em>something</em>—just not
                  the thing you want it to be doing. Finding out <em>what</em> it’s doing, then
                  asking <em>why</em> it’s doing that and <em>where</em> its resources are being
                  used or where its output is going can help you understand how things have gone
                  wrong.<a data-primary="Five Whys" data-type="indexterm" id=
                  "id-vgCkhMFLtOT9uO"></a><a data-primary="emergency response" data-secondary=
                  "Five Whys" data-type="indexterm" id="id-lrCwT8FZt0TduO"></a><sup><a data-type=
                  "noteref" href="effective-troubleshooting.html#id-yJLulcpFvt2TNu2" id="id-yJLulcpFvt2TNu2-marker">67</a></sup>
                </p>
                <aside class="pagebreak-before highlight" data-type="sidebar" id=
                "unpacking-the-causes-of-a-symptom-AlSKImtAT9uL">
                  <h5 class="heading">
                    Unpacking the Causes of a Symptom
                  </h5>
                  <p>
                    <strong>Symptom</strong>: A Spanner cluster has high latency and RPCs to its
                    servers are timing out.
                  </p>
                  <p>
                    <strong>Why</strong>? The Spanner server tasks are using all their CPU time and
                    can’t make progress on all the requests the clients send.
                  </p>
                  <p>
                    <strong>Where</strong> in the server is the CPU time being used? Profiling the
                    server shows it’s sorting entries in logs checkpointed to disk.
                  </p>
                  <p>
                    <strong>Where</strong> in the log-sorting code is it being used? When
                    evaluating a regular expression against paths to log files.
                  </p>
                  <p>
                    <strong>Solutions</strong>: Rewrite the regular expression to avoid
                    backtracking. Look in the codebase for similar patterns. Consider using RE2,
                    which does not backtrack and guarantees linear runtime growth with input
                    size.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-VjDupFDT9IatgTPuB" id=
                    "id-VjDupFDT9IatgTPuB-marker">68</a></sup>
                  </p>
                </aside>
              </section>
              <section data-type="sect3" id="what-touched-it-last-K7sBhnTjuM">
                <h3 class="subheaders">
                  What touched it last
                </h3>
                <p>
                  Systems have inertia: we’ve found that a working computer system tends to remain
                  in motion until acted upon by an external force, such as a configuration change
                  or a shift in the type of load served. Recent changes to a system can be a
                  productive place to start identifying what’s going wrong.<sup><a data-type=
                  "noteref" href="effective-troubleshooting.html#id-4DBuqSYFkhXTxug" id="id-4DBuqSYFkhXTxug-marker">69</a></sup>
                </p>
                <p>
                  Well-designed systems should have extensive production logging to track new
                  version deployments and configuration changes at all layers of the stack, from
                  the server binaries handling user traffic down to the packages installed on
                  individual nodes in the cluster. Correlating changes in a system’s performance
                  and behavior with other events in the system and environment can also be helpful
                  in constructing monitoring dashboards; for example, you might annotate a graph
                  showing the system’s error rates with the start and end times of a deployment of
                  a new version, as seen in <a data-type="xref" href=
                  "effective-troubleshooting.html#fig_troubleshooting_annotated-graph">Figure 12-2</a>.
                </p>
                <figure class="horizontal vertical" id="fig_troubleshooting_annotated-graph">
                  <img alt="Error rates graphed against deployment start and end times." src=
                  "../images/srle-1202.jpg">
                  <figcaption>
                    <span class="label">Figure 12-2.</span> Error rates graphed against deployment
                    start and end times
                  </figcaption>
                </figure>
                <aside class="highlight" data-type="sidebar" id="-GBSEhnhpTQuW">
                  <p>
                    Manually sending a request to the <code>/api/search</code> endpoint (see
                    <a data-type="xref" href="effective-troubleshooting.html#debug_shakespeare">Debugging Shakespeare</a>) and
                    seeing the failure listing backend servers that handled the response lets you
                    discount the likelihood that the problem is with the API frontend server and
                    with the load balancers: the response probably wouldn’t have included that
                    information if the request hadn’t at least made it to the search backends and
                    failed there. Now you can focus your efforts on the backends—analyzing their
                    logs, sending test queries to see what responses they return, and examining
                    their exported metrics.
                  </p>
                </aside>
              </section>
              <section data-type="sect3" id="specific-diagnoses-a7swTnTpuk">
                <h3 class="subheaders">
                  Specific diagnoses
                </h3>
                <p>
                  While the generic tools described previously are helpful across a broad range of
                  problem domains, you will likely find it helpful to build tools and systems to
                  help with diagnosing your particular services. Google SREs spend much of their
                  time building such tools. While many of these tools are necessarily specific to a
                  given system, be sure to look for commonalities between services and teams to
                  avoid duplicating effort.<a data-primary="" data-startref="Tdiag12" data-type=
                  "indexterm" id="id-1nCxSqFBTjT7uJ"></a>
                </p>
              </section>
            </section>
            <section data-type="sect2" id="test-and-treat-EvsWceuj">
              <h2 class="subheaders">
                Test and Treat
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="testing and treating issues"
                data-type="indexterm" id="Ttest12"></a>Once you’ve come up with a short list of
                possible causes, it’s time to try to find <em>which</em> factor is at the root of
                the actual problem. Using the experimental method, we can try to rule in or rule
                out our hypotheses. For instance, suppose we think a problem is caused by either a
                network failure between an application logic server and a database server, or by
                the database refusing connections. Trying to connect to the database with the same
                credentials the application logic server uses can refute the second hypothesis,
                while pinging the database server may be able to refute the first, depending on
                network topology, firewall rules, and other factors. Following the code and trying
                to imitate the code flow, step-by-step, may point to exactly what’s going wrong.
              </p>
              <p>
                There are a number of considerations to keep in mind when designing tests (which
                may be as simple as sending a ping or as complicated as removing traffic from a
                cluster and injecting specially formed requests to find a race condition):
              </p>
              <ul>
                <li>
                  <p>
                    An ideal test should have mutually exclusive alternatives, so that it can rule
                    one group of hypotheses in and rule another set out. In practice, this may be
                    difficult to achieve.
                  </p>
                </li>
                <li>
                  <p>
                    Consider the obvious first: perform the tests in decreasing order of
                    likelihood, considering possible risks to the system from the test. It probably
                    makes more sense to test for network connectivity problems between two machines
                    before looking into whether a recent configuration change removed a user’s
                    access to the second machine.
                  </p>
                </li>
                <li>
                  <p>
                    An experiment may provide misleading results due to confounding factors. For
                    example, a firewall rule might permit access only from a specific IP address,
                    which might make pinging the database from your workstation fail, even if
                    pinging from the application logic server’s machine would have succeeded.
                  </p>
                </li>
                <li>
                  <p>
                    Active tests may have side effects that change future test results. For
                    instance, allowing a process to use more CPUs may make operations faster, but
                    might increase the likelihood of encountering data races. Similarly, turning on
                    verbose logging might make a latency problem even worse and confuse your
                    results: is the problem getting worse on its own, or because of the logging?
                  </p>
                </li>
                <li>
                  <p>
                    Some tests may not be definitive, only suggestive. It can be very difficult to
                    make race conditions or deadlocks happen in a timely and reproducible manner,
                    so you may have to settle for less certain evidence that these are the causes.
                  </p>
                </li>
              </ul>
              <p>
                Take clear notes of what ideas you had, which tests you ran, and the results you
                saw.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-4DBuqSohQc0ur" id=
                "id-4DBuqSohQc0ur-marker">70</a></sup> Particularly when you are dealing with more
                complicated and drawn-out cases, this documentation may be crucial in helping you
                remember exactly what happened and prevent having to repeat these
                steps.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-JJ7uzFLhVcyud" id=
                "id-JJ7uzFLhVcyud-marker">71</a></sup> If you performed active testing by changing
                a system—for instance by giving more resources to a process—making changes in a
                systematic and documented fashion will help you return the system to its pre-test
                setup, rather than running in an unknown hodge-podge configuration.
              </p>
            </section>
          </section>
          <section data-type="sect1" id="xref_troubleshooting_negative-results">
            <h1 class="heading">
              Negative Results Are Magic
            </h1>
            <p class="authot byline">
              Written by Randall Bosetti<br>
              Edited by Joan Wendt
            </p>
            <p>
              <a data-primary="negative results" data-type="indexterm" id="id-XmC9SLteUJ"></a>A
              "negative" result is an experimental outcome in which the expected effect is
              absent—that is, any experiment that doesn’t work out as planned. This includes new
              designs, heuristics, or human processes that fail to improve upon the systems they
              replace.
            </p>
            <p>
              <strong>Negative results should not be ignored or discounted.</strong> Realizing
              you’re wrong has much value: a clear negative result can resolve some of the hardest
              design questions. Often a team has two seemingly reasonable designs but progress in
              one direction has to address vague and speculative questions about whether the other
              direction might be better.
            </p>
            <p>
              <strong>Experiments with negative results are conclusive.</strong> They tell us
              something certain about production, or the design space, or the performance limits of
              an existing system. They can help others determine whether their own experiments or
              designs are worthwhile. For example, a given development team might decide against
              using a particular web server because it can handle only ~800 connections out of the
              needed 8,000 connections before failing due to lock contention. When a subsequent
              development team decides to evaluate web servers, instead of starting from scratch,
              they can use this already well-documented negative result as a starting point to
              decide quickly whether (a) they need fewer than 800 connections or (b) the lock
              contention problems have been resolved.
            </p>
            <p>
              Even when negative results do not apply directly to someone else’s experiment, the
              supplementary data gathered can help others choose new experiments or avoid pitfalls
              in previous designs. Microbenchmarks, documented antipatterns, and project
              postmortems all fit this category. You should consider the scope of the negative
              result when designing an experiment, because a broad or especially robust negative
              result will help your peers even more.
            </p>
            <p>
              <strong>Tools and methods can outlive the experiment and inform future work.</strong>
              As an example, benchmarking tools and load generators can result just as easily from
              a disconfirming experiment as a supporting one. Many webmasters have benefited from
              the difficult, detail-oriented work that produced Apache Bench, a web server
              loadtest, even though its first results were likely disappointing.
            </p>
            <p>
              Building tools for repeatable experiments can have indirect benefits as well:
              although one application you build may not benefit from having its database on SSDs
              or from creating indices for dense keys, the next one just might. Writing a script
              that allows you to easily try out these configuration changes ensures you don’t
              forget or miss optimizations in your next project.
            </p>
            <p>
              <strong>Publishing negative results improves our industry’s data-driven
              culture.</strong> Accounting for negative results and statistical insignificance
              reduces the bias in our metrics and provides an example to others of how to maturely
              accept uncertainty. By publishing everything, you encourage others to do the same,
              and everyone in the industry collectively learns much more quickly. SRE has already
              learned this lesson with high-quality postmortems, which have had a large positive
              effect on production stability.
            </p>
            <p>
              <strong>Publish your results.</strong> If you are interested in an experiment’s
              results, there’s a good chance that other people are as well. When you publish the
              results, those people do not have to design and run a similar experiment themselves.
              It’s tempting and common to avoid reporting negative results because it’s easy to
              perceive that the experiment "failed." Some experiments are doomed, and they tend to
              be caught by review. Many more experiments are simply unreported because people
              mistakenly believe that negative results are not progress.
            </p>
            <p>
              Do your part by telling everyone about the designs, algorithms, and team workflows
              you’ve ruled out. Encourage your peers by recognizing that negative results are part
              of thoughtful risk taking and that every well-designed experiment has merit. Be
              skeptical of any design document, performance review, or essay that doesn’t mention
              failure. Such a document is potentially either too heavily filtered, or the author
              was not rigorous in his or her methods.
            </p>
            <p>
              Above all, publish the results you find surprising so that others—including your
              future self—aren’t surprised.<a data-primary="" data-startref="Ttest12" data-type=
              "indexterm" id="id-vgCPSeHBUo"></a>
            </p>
            <section data-type="sect2" id="cure-WEs2fAUm">
              <h2 class="subheaders">
                Cure
              </h2>
              <p>
                <a data-primary="troubleshooting" data-secondary="curing issues" data-type=
                "indexterm" id="id-nxC1SLFLfpU2"></a>Ideally, you’ve now narrowed the set of
                possible causes to one. Next, we’d like to prove that it’s the actual cause.
                Definitively proving that a given factor <em>caused</em> a problem—by reproducing
                it at will—can be difficult to do in production systems; often, we can only find
                <em>probable</em> causal factors, for the following reasons:
              </p>
              <ul>
                <li>
                  <p>
                    <em>Systems are complex</em>. It’s quite likely that there are multiple
                    factors, each of which individually is not the cause, but which taken jointly
                    are causes.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-rq7upFOSNSDIvf9UB" id=
                    "id-rq7upFOSNSDIvf9UB-marker">72</a></sup> Real systems are also often
                    path-dependent, so that they must be in a specific state before a failure
                    occurs.
                  </p>
                </li>
                <li>
                  <p>
                    <em>Reproducing the problem in a live production system may not be an
                    option</em>, either because of the complexity of getting the system into a
                    state where the failure can be triggered, or because further downtime may be
                    unacceptable. Having a nonproduction environment can mitigate these challenges,
                    though at the cost of having another copy of the system to run.
                  </p>
                </li>
              </ul>
              <p>
                Once you’ve found the factors that caused the problem, it’s time to write up notes
                on what went wrong with the system, how you tracked down the problem, how you fixed
                the problem, and how to prevent it from happening again. In other words, you need
                to write a postmortem (although ideally, the system is <em>alive</em> at this
                point!).
              </p>
            </section>
          </section>
          <section data-type="sect1" id="case-study-dbsmCB">
            <h1 class="heading">
              Case Study
            </h1>
            <p>
              <a data-primary="App Engine" data-type="indexterm" id=
              "id-OnCNSmFJCp"></a><a data-primary="troubleshooting" data-secondary=
              "App Engine case study" data-type="indexterm" id="Tappeng12"></a>App
              Engine,<sup><a data-type="noteref" href="effective-troubleshooting.html#id-WakuQILFxCp" id=
              "id-WakuQILFxCp-marker">73</a></sup> part of Google’s Cloud Platform, is a
              platform-as-a-service product that allows developers to build services atop Google’s
              infrastructure. One of our internal customers filed a problem report indicating that
              they’d recently seen a dramatic increase in latency, CPU usage, and number of running
              processes needed to serve traffic for their app, a content-management system used to
              build documentation for developers.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-ep2uLtJFaCq"
              id="id-ep2uLtJFaCq-marker">74</a></sup> The customer couldn’t find any recent changes
              to their code that correlated with the increase in resources, and there hadn’t been
              an increase in traffic to their app (see <a data-type="xref" href=
              "effective-troubleshooting.html#fig_troubleshooting_qps">Figure 12-3</a>), so they were wondering if a change in
              the App Engine service was responsible.
            </p>
            <p>
              Our investigation discovered that latency had indeed increased by nearly an order of
              magnitude (as shown in <a data-type="xref" href=
              "effective-troubleshooting.html#fig_troubleshooting_latency-heatmap">Figure 12-4</a>). Simultaneously, the amount
              of CPU time (<a data-type="xref" href="effective-troubleshooting.html#fig_troubleshooting_cpu">Figure 12-5</a>) and
              number of serving processes (<a data-type="xref" href=
              "effective-troubleshooting.html#fig_troubleshooting_instances">Figure 12-6</a>) had nearly quadrupled. Clearly
              something was wrong. It was time to start troubleshooting.
            </p>
            <figure class="horizontal vertical" id="fig_troubleshooting_qps">
              <img alt=
              "Application’s requests received per second, showing a brief spike and return to normal."
              src="../images/srle-1203.jpg">
              <figcaption>
                <span class="label">Figure 12-3.</span> Application’s requests received per second,
                showing a brief spike and return to normal
              </figcaption>
            </figure>
            <figure class="horizontal vertical" id="fig_troubleshooting_latency-heatmap">
              <img alt=
              "Application’s latency, showing 50th, 95th, and 99th percentiles (lines) with a heatmap showing how many requests fell into a given latency bucket at any point in time (shade)."
              src="../images/srle-1204.jpg">
              <figcaption>
                <span class="label">Figure 12-4.</span> Application’s latency, showing 50th, 95th,
                and 99th percentiles (lines) with a heatmap showing how many requests fell into a
                given latency bucket at any point in time (shade)
              </figcaption>
            </figure>
            <figure class="horizontal vertical" id="fig_troubleshooting_cpu">
              <img alt="Aggregate CPU usage for the application." src="../images/srle-1205.jpg">
              <figcaption>
                <span class="label">Figure 12-5.</span> Aggregate CPU usage for the application
              </figcaption>
            </figure>
            <figure class="horizontal vertical" id="fig_troubleshooting_instances">
              <img alt="Number of instances for the application." src="../images/srle-1206.jpg">
              <figcaption>
                <span class="label">Figure 12-6.</span> Number of instances for the application
              </figcaption>
            </figure>
            <p>
              Typically a sudden increase in latency and resource usage indicates either an
              increase in traffic sent to the system or a change in system configuration. However,
              we could easily rule out both of these possible causes: while a spike in traffic to
              the app around 20:45 could explain a brief surge in resource usage, we’d expect
              traffic to return to baseline fairly soon after request volume normalized. This spike
              certainly shouldn’t have continued for multiple days, beginning when the app’s
              developers filed the report and we started looking into the problem. Second, the
              change in performance happened on Saturday, when neither changes to the app nor the
              production environment were in flight. The service’s most recent code pushes and
              configuration pushes had completed days before. Furthermore, if the problem
              originated with the service, we’d expect to see similar effects on other apps using
              the same infrastructure. However, no other apps were experiencing similar effects.
            </p>
            <p>
              We referred the problem report to our counterparts, App Engine’s developers, to
              investigate whether the customer was encountering any idiosyncrasies in the serving
              infrastructure. The developers weren’t able to find any oddities, either. However, a
              developer did notice a correlation between the latency increase and the increase of a
              specific data storage API call, <code>merge_join</code>, which often indicates
              suboptimal indexing when reading from the datastore. Adding a composite index on the
              properties the app uses to select objects from the datastore would speed those
              requests, and in principle, speed the application as a whole—but we’d need to figure
              out <em>which</em> properties needed indexing. A quick look at the application’s code
              didn’t reveal any obvious suspects.
            </p>
            <p>
              It was time to pull out the heavy machinery in our toolkit: using Dapper
              <a data-type="xref" href="bibliography.html#Sig10" target=
              "_blank">[Sig10]</a>, we traced the steps individual HTTP requests took—from their
              receipt by a frontend reverse proxy through to the point where the app’s code
              returned a response—and looked at the RPCs issued by each server involved in handling
              that request. Doing so would allow us to see which properties were included in
              requests to the datastore, then create the appropriate indices.
            </p>
            <p>
              While investigating, we discovered that requests for static content such as images,
              which weren’t served from the datastore, were also much slower than expected. Looking
              at graphs with file-level granularity, we saw their responses had been much faster
              only a few days before. This implied that the observed correlation between
              <code>merge_join</code> and the latency increase was spurious and that our
              suboptimal-indexing theory was fatally flawed.
            </p>
            <p>
              Examining the unexpectedly slow requests for static content, most of the RPCs sent
              from the application were to a memcache service, so the requests should have been
              very fast—on the order of a few milliseconds. These requests did turn out to be very
              fast, so the problem didn’t seem to originate there. However, between the time the
              app started working on a request and when it made the first RPCs, there was about a
              250 ms period where the app was doing…well, <em>something</em>. Because App Engine
              runs code provided by users, its SRE team does not profile or inspect app code, so we
              couldn’t tell what the app was doing in that interval; similarly, Dapper couldn’t
              help track down what was going on since it can only trace RPC calls, and none were
              made during that period.
            </p>
            <p>
              Faced with what was, by this point, quite a mystery, we decided not to solve
              it…<em>yet</em>. The customer had a public launch scheduled for the following week,
              and we weren’t sure how soon we’d be able to identify the problem and fix it.
              Instead, we recommended that the customer increase the resources allocated to their
              app to the most CPU-rich instance type available. Doing so reduced the app’s latency
              to acceptable levels, though not as low as we’d prefer. We concluded that the latency
              mitigation was good enough that the team could conduct their launch successfully,
              then investigate at leisure.<sup><a data-type="noteref" href="effective-troubleshooting.html#id-yJLuyFgHdCL" id=
              "id-yJLuyFgHdCL-marker">75</a></sup>
            </p>
            <p>
              At this point, we suspected that the app was a victim of yet another common cause of
              sudden increases in latency and resource usage: a change in the type of work. We’d
              seen an increase in writes to the datastore from the app, just before its latency
              increased, but because this increase wasn’t very large—nor was it sustained—we’d
              written it off as coincidental. However, this behavior did resemble a common pattern:
              an instance of the app is initialized by reading objects from the datastore, then
              storing them in the instance’s memory. By doing so, the instance avoids reading
              rarely changing configuration from the datastore on each request, and instead checks
              the in-memory objects. Then, the time it takes to handle requests will often scale
              with the amount of configuration data.<sup><a data-type="noteref" href=
              "effective-troubleshooting.html#id-yJLuASefdCL" id="id-yJLuASefdCL-marker">76</a></sup> We couldn’t prove that this
              behavior was the root of the problem, but it’s a common antipattern.
            </p>
            <p>
              The app developers added instrumentation to understand where the app was spending its
              time. They identified a method that was called on every request, that checked whether
              a user had whitelisted access to a given path. The method used a caching layer that
              sought to minimize accesses to both the datastore and the memcache service, by
              holding whitelist objects in instances’ memory. As one of the app’s developers noted
              in the investigation, “I don’t know where the fire is yet, but I’m blinded by smoke
              coming from this whitelist cache.”
            </p>
            <p>
              Some time later, the root cause was found: due to a long-standing bug in the app’s
              access control system, whenever one specific path was accessed, a whitelist object
              would be created and stored in the datastore. In the run-up to launch, an automated
              security scanner had been testing the app for vulnerabilities, and as a side effect,
              its scan produced thousands of whitelist objects over the course of half an hour.
              These superfluous whitelist objects then had to be checked on every request to the
              app, which led to pathologically slow responses—without causing any RPC calls from
              the app to other services. Fixing the bug and removing those objects returned the
              app’s performance to expected levels.<a data-primary="" data-startref="Tappeng12"
              data-type="indexterm" id="id-8nCmSZFXCj"></a>
            </p>
          </section>
          <section data-type="sect1" id="making-troubleshooting-easier-BVsbsE">
            <h1 class="heading">
              Making Troubleshooting Easier
            </h1>
            <p>
              <a data-primary="troubleshooting" data-secondary="simplifying" data-type="indexterm"
              id="id-XmC9SOFqsJ"></a>There are many ways to simplify and speed troubleshooting.
              Perhaps the most fundamental are:
            </p>
            <ul>
              <li>Building observability—with both white-box metrics and structured logs—into each
              component from the ground up
              </li>
              <li>Designing systems with well-understood and observable interfaces between
              <span class="keep-together">components</span>.
              </li>
            </ul>
            <p>
              Ensuring that information is available in a consistent way throughout a system—for
              instance, using a unique request identifier throughout the span of RPCs generated by
              various components—reduces the need to figure out <em>which</em> log entry on an
              upstream component matches a log entry on a downstream component, speeding the time
              to diagnosis and recovery.
            </p>
            <p>
              Problems in correctly representing the state of reality in a code change or an
              environment change often lead to a need to troubleshoot. Simplifying, controlling,
              and logging such changes can reduce the need for troubleshooting, and make it easier
              when it happens.
            </p>
          </section>
          <section data-type="sect1" id="conclusion-MQsEHV">
            <h1 class="heading">
              Conclusion
            </h1>
            <p>
              <a data-primary="troubleshooting" data-secondary="systematic approach to" data-type=
              "indexterm" id="id-jyCxSoFOHp"></a>We’ve looked at some steps you can take to make
              the troubleshooting process clear and understandable to novices, so that they, too,
              can become effective at solving problems. Adopting a systematic approach to
              troubleshooting—as opposed to relying on luck or experience—can help bound your
              services’ time to recovery, leading to a better experience for your users.
            </p>
          </section>
          <div class="footnotes" data-type="footnotes">
            <p data-type="footnote" id="id-EYJuMSKTj">
              <sup><a href="effective-troubleshooting.html#id-EYJuMSKTj-marker">58</a></sup>Indeed, using only first principles
              and troubleshooting skills is often an effective way to learn how a system works; see
              <a data-type="xref" href=
              "accelerating-sre-on-call.html">Accelerating SREs to On-Call and
              Beyond</a>.
            </p>
            <p data-type="footnote" id="id-a82udFEFzik">
              <sup><a href="effective-troubleshooting.html#id-a82udFEFzik-marker">59</a></sup>See <a href=
              "https://en.wikipedia.org/wiki/Hypothetico-deductive_model" target=
              "_blank"><em class="hyperlink">https://en.wikipedia.org/wiki/Hypothetico-deductive_model</em></a>.
            </p>
            <p data-type="footnote" id="id-0vYuEF7IWiA">
              <sup><a href="effective-troubleshooting.html#id-0vYuEF7IWiA-marker">60</a></sup>For instance, exported variables as
              described in <a data-type="xref" href=
              "practical-alerting.html">Practical Alerting from Time-Series
              Data</a>.
            </p>
            <p data-type="footnote" id="id-GbduJSWtpTeiW">
              <sup><a href="effective-troubleshooting.html#id-GbduJSWtpTeiW-marker">61</a></sup>Attributed to Theodore Woodward,
              of the University of Maryland School of Medicine, in the 1940s. See <a href=
              "https://en.wikipedia.org/wiki/Zebra_(medicine)" target="_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Zebra_(medicine)</em></a>. This works in
              some domains, but in some systems, entire classes of failures may be eliminable: for
              instance, using a well-designed cluster filesystem means that a latency problem is
              unlikely to be due to a single dead disk.
            </p>
            <p data-type="footnote" id="id-D04IaFmtGTziJ">
              <sup><a href="effective-troubleshooting.html#id-D04IaFmtGTziJ-marker">62</a></sup>Occam’s Razor; see <a href=
              "https://en.wikipedia.org/wiki/Occam's_razor" target="_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Occam%27s_razor</em></a>. But remember that
              it may still be the case that there are multiple problems; in particular, it may be
              more likely that a system has a number of common low-grade problems that, taken
              together, explain all the symptoms rather than a single rare problem that causes them
              all. Cf <a href="https://en.wikipedia.org/wiki/Hickam's_dictum" target=
              "_blank"><em class=
              "hyperlink">https://en.wikipedia.org/wiki/Hickam%27s_dictum</em></a>.
            </p>
            <p data-type="footnote" id="id-kD2uEF7hnT7io">
              <sup><a href="effective-troubleshooting.html#id-kD2uEF7hnT7io-marker">63</a></sup>Of course, see <a href=
              "https://xkcd.com/552" target="_blank"><em class=
              "hyperlink">https://xkcd.com/552</em></a>.
            </p>
            <p data-type="footnote" id="id-4DBukIohGT7ir">
              <sup><a href="effective-troubleshooting.html#id-4DBukIohGT7ir-marker">64</a></sup>At least, we have no plausible
              theory to explain why the number of PhDs awarded in Computer Science in the US should
              be extremely well correlated (r<sup>2</sup> = 0.9416) with the per capita consumption
              of cheese, between 2000 and 2009: <a href=
              "http://tylervigen.com/view_correlation?id=1099" target="_blank"><em class=
              "hyperlink">http://tylervigen.com/view_correlation?id=1099</em></a>.
            </p>
            <p data-type="footnote" id="id-GbduEhgFkIQuW">
              <sup><a href="effective-troubleshooting.html#id-GbduEhgFkIQuW-marker">65</a></sup>It may be useful to refer
              prospective bug reporters to <a data-type="xref" href=
              "bibliography.html#Tat99" target="_blank">[Tat99]</a> to help them
              provide high-quality problem reports.
            </p>
            <p data-type="footnote" id="id-GbduZFjIQhQuW">
              <sup><a href="effective-troubleshooting.html#id-GbduZFjIQhQuW-marker">66</a></sup>But beware false correlations
              that can lead you down wrong paths!
            </p>
            <p data-type="footnote" id="id-yJLulcpFvt2TNu2">
              <sup><a href="effective-troubleshooting.html#id-yJLulcpFvt2TNu2-marker">67</a></sup>In many respects, this is
              similar to the “Five Whys” technique <a data-type="xref" href=
              "bibliography.html#Ohn88" target="_blank">[Ohn88]</a> introduced
              by Taiichi Ohno to understand the root causes of manufacturing errors.
            </p>
            <p data-type="footnote" id="id-VjDupFDT9IatgTPuB">
              <sup><a href="effective-troubleshooting.html#id-VjDupFDT9IatgTPuB-marker">68</a></sup>In contrast to RE2, PCRE can
              require exponential time to evaluate some regular expressions. RE2 is available at
              <a href="https://github.com/google/re2" target="_blank"><em class=
              "hyperlink">https://github.com/google/re2</em></a>.
            </p>
            <p data-type="footnote" id="id-4DBuqSYFkhXTxug">
              <sup><a href="effective-troubleshooting.html#id-4DBuqSYFkhXTxug-marker">69</a></sup><a data-type="xref" href=
              "bibliography.html#All15" target="_blank">[All15]</a> observes
              this is a frequently used heuristic in resolving outages.
            </p>
            <p data-type="footnote" id="id-4DBuqSohQc0ur">
              <sup><a href="effective-troubleshooting.html#id-4DBuqSohQc0ur-marker">70</a></sup>Using a shared document or
              real-time chat for notes provides a timestamp of <em>when</em> you did something,
              which is helpful for postmortems. It also shares that information with others, so
              they’re up to speed with the current state of the world and don’t need to interrupt
              your troubleshooting.
            </p>
            <p data-type="footnote" id="id-JJ7uzFLhVcyud">
              <sup><a href="effective-troubleshooting.html#id-JJ7uzFLhVcyud-marker">71</a></sup>See also <a data-type="xref"
              href="effective-troubleshooting.html#xref_troubleshooting_negative-results">Negative Results Are Magic</a> for more
              on this point.
            </p>
            <p data-type="footnote" id="id-rq7upFOSNSDIvf9UB">
              <sup><a href="effective-troubleshooting.html#id-rq7upFOSNSDIvf9UB-marker">72</a></sup>See <a data-type="xref" href=
              "bibliography.html#Mea08" target="_blank">[Mea08]</a> on how to
              think about systems, and also <a data-type="xref" href=
              "bibliography.html#Coo00" target="_blank">[Coo00]</a> and
              <a data-type="xref" href="bibliography.html#Dek14" target=
              "_blank">[Dek14]</a> on the limitations of finding a single root cause instead of
              examining the system and its environment for causative factors.
            </p>
            <p data-type="footnote" id="id-WakuQILFxCp">
              <sup><a href="effective-troubleshooting.html#id-WakuQILFxCp-marker">73</a></sup>See <a href=
              "https://cloud.google.com/appengine" target="_blank"><em class=
              "hyperlink">https://cloud.google.com/appengine</em></a>.
            </p>
            <p data-type="footnote" id="id-ep2uLtJFaCq">
              <sup><a href="effective-troubleshooting.html#id-ep2uLtJFaCq-marker">74</a></sup>We have compressed and simplified
              this case study to aid understanding.
            </p>
            <p data-type="footnote" id="id-yJLuyFgHdCL">
              <sup><a href="effective-troubleshooting.html#id-yJLuyFgHdCL-marker">75</a></sup>While launching with an
              unidentified bug isn’t ideal, it’s often impractical to eliminate all known bugs.
              Instead, sometimes we have make do with second-best measures and mitigate risk as
              best we can, using good engineering judgment.
            </p>
            <p data-type="footnote" id="id-yJLuASefdCL">
              <sup><a href="effective-troubleshooting.html#id-yJLuASefdCL-marker">76</a></sup>The datastore lookup can use an
              index to speed the comparison, but a frequent in-memory implementation is a simple
              <code>for</code> loop comparison across all the cached objects. If there are only a
              few objects, it won’t matter that this takes linear time—but this can cause a
              significant increase in latency and resource usage as the number of cached objects
              grows.
            </p>
          </div>
        </section>
      </div>
    </div>
    <div class="footer">
      <div class="maia-aux">
        <div class="previous">
          <a href="being-on-call.html">
          <p class="footer-caption">
            previous
          </p>
          <p class="chapter-link">
            Chapter 11- Being On-Call
          </p></a>
        </div>
        <div class="next">
          <a href="emergency-response.html">
          <p class="footer-caption">
            next
          </p>
          <p class="chapter-link">
            Chapter 13- Emergency Response
          </p></a>
        </div>
        <p class="footer-link">
          Copyright © 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under <a href=
          "https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>
        </p>
      </div>
    </div>
    <script src="../js/main.min.js">
    </script> 
    <script src="../js/maia.js">
    </script>
  </body>
</html>
